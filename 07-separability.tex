\chapter{Separability}\label{ch:separability}

\cmnt{%

  I need to be clearer about the connection between separability and localism:
  /If/ the individual elements of a stream capture all I care about, then
  separability is highly plausible. If I prefer an even over an uneven
  distribution of ``outcomes'' then I don't just care about these ``outcomes''.
  The problem is that if we enrich the outcomes by the condition of evenness
  then we can't even consider permutations of the outcomes any more.
  
  Maybe I should call the basic desires ``reasons'', a la Dietrich and List?

  Read Brown, Composition of reasons

  Read Weirich 2001 and 2015 on ``intrinsic utility'' (the two books)

  Should I explain that in ordinary FP explanations we only give relevant
  ``basic desires'', as Pettit mentioned?

  mcclennen1990rationality pp.47f. mentions the connection between additive
  representations of preferences over commodities and the independence axiom for
  preferences over gambles. He points at krantz1971foundations as a classical
  text on multi-attribute utility theory.
  
} %

\section{The construction of value}\label{sec:construction-value}

\cmnt{%

  One way to distinguish instrumental from non-instrumental desires:
  you non-instrumentally desire $P$ if you desire $P$ all else
  equal. I.e., if $P$ is a good-making feature of worlds.

} %

At the end of chapter \ref{ch:utility} we saw that the utility of a proposition
for an agent is determined by two factors: the agent's credences, and the
agent's basic desires, which are reflected in the utility assigned to the
agent's ``concerns''. In this chapter, we will explore how the utility of a
concern might be determined.

Suppose you have two basic desires: being rich and being famous. All worlds in
which you are both rich and famous are then equally desirable for you, no matter
what else happens in them. The set of these worlds has ``uniform utility''. The
same is true for the worlds in which you are neither rich nor famous, and for
the worlds in which you are rich but not famous, and for the worlds in which you
are not rich but famous. The four sets of worlds are your ``concerns''.

In general, a concern is a proposition that settles everything an agent
ultimately cares about. If there are $n$ propositions $A_1,A_2,\ldots,A_n$ that the
agent ultimately cares about, then any conjunction that can be formed from these
propositions and their negations (such as
$A_1 \land \neg A_2 \land A_3 \land \ldots \land \neg A_n$) has uniform utility,
and is one of the agent's concerns.

It will be useful to have a label for an agent's utility function restricted to
concerns. I will call it the agent's \textbf{value function}. So an agent's
value function specifies to what extent the agent desires all the combinations
of propositions she ultimately cares about. The value function represents the
agent's belief-independent desires.

In the above example, your value function might assign 10 to the proposition
that you are rich and famous ($R \land F$), 5 to $R \land \neg F$, 5 to
$\neg R \land F$, and 0 to $\neg R \land \neg F$. Intuitively, we can explain
where these numbers come from by assuming that being rich and being famous both
have an intrinsic value of 5 for you; $R \land F$ has value 10 because it has
both of these features; $R \land \neg F$ has only one and so has value 5.

The aim of the present chapter is to spell out this kind of explanation, and
to investigate under what conditions it is possible.

\section{Additivity}\label{sec:additivity}

Let's start with a toy example. You are looking for a flat to rent.
You care about various aspects of a flat such as size, location, and
price. We'll call these aspects \textbf{attributes}. If a set of
attributes comprises all the features (of a flat) that matter to you,
then your preferences between possible flats are determined by your
preferences between combinations of these attributes: if you prefer
one flat to another, that's because you prefer the combined attributes
of the first to those of the second.

So the desirability of any possible flat is determined by the
desirability of every possible combination of attributes. We'll write
these combinations as lists enclosed in angular brackets. For example,
`$\t{40 \m^2, \text{central}, \text{£500}}$' stands for any flat with
a size of 40 $\m^2$, central location, and monthly costs of £500. If
these are all the attributes you care about, then your utility
function will assign the same value to all such flats.

\cmnt{%
  An \textbf{attribute}, on this usage, is not a particular property
  of any particular flat. Rather, an aspect is a set of related
  properties -- a set that divides all possible flats into groups. For
  example, monthly cost of rent (an aspect) divides all possible flats
  into those that cost £300, those that cost £310, those that cost
  £320, and so on.%
} %

It's the same with possible worlds. If all you care about is the
degree of pleasure of you and your three best friends, then we can
represent your basic desires by a value function that assigns numbers
to lists like $\t{10, 1, 2, 3}$, specifying degrees of pleasure for
you and your friends (in some fixed order). Each such list effectively
specifies one of your concerns: a maximal conjunction of propositions
you care about.

Return to the flats. Assuming you only care about size, location, and
price, there will be some value function that assigns a desirability
score to possible combinations of size, location, and price. If you're
like most people, we can we say more about how these scores are
determined on the basis of the individual attributes. For example, you
probably prefer cheaper flats to more expensive flats, and larger ones
to smaller ones.

A natural method for determining the overall score for a given flat is to first
assign a score to each of the flat's attributes; adding these scores yields the
overall score for the flat. A cheap but small flat in a good location, for
example, gets a high score for price, a low score for size, and a high score for
location, which amounts to a medium overall score.

More formally, the present idea is to define the value of any given
attribute list as the sum of \textbf{subvalues} assigned to individual
attributes in the list. That is, if $V_{S}(40 \m^2)$ is the score you
assign to a size of 40 m$^2$, $V_{L}(\text{central})$ is the score for
central location, and $V_{P}(\text{£500})$ is the score for monthly
costs of £500, then
\[
V(\t{40 \m^2, \text{central}, \text{£500}}) = V_{S}(40 \m^2) +
V_{L}(\text{central}) + V_{P}(\text{£500}).
\]
%
If a value function $V$ is determined by adding up subvalues
in this manner, then $V$ is called \textbf{additive} relative to the
attributes in question.

Additivity may seem to imply that you assign the same weight to
all the attributes: that size, location, and price are equally
important to you. To allow for different weights, you might think, we
should include scaling factors $w_S, w_L, w_P$, like so:
\[
V(\t{40 \m^2, \text{central}, \text{£500}}) = w_s \cdot V_{S}(40 \m^2) +
w_L \cdot V_{L}(\text{central}) + w_P \cdot V_{P}(\text{£500}).
\]

However, we can omit the weights by folding them into the
subvalues. We will let $V_S(200 \m^2)$ measure not just how
awesome it would be to have a 200 $\m^2$ flat, but also how important
this feature is compared to price and location.

\cmnt{%
  The weight representation is simpler insofar as it fixes the weights
  independently of the attribute values. With our representation, size
  could have a large weight for large flats and a low weight for small
  flats.%
} %

\begin{exercise}{2}\label{e:subv-not-u}
  Like utility functions, subvalue functions assign numbers to sets of
  possible worlds that may vary in desirability. But unlike utility
  functions, subvalue functions are insensitive to belief. This
  explains why, if you can afford to pay £600 in monthly
  rent, then $V_{P}(\text{£300})$ is plausibly high, even though the
  \emph{utility} you assign to renting a flat for £300 is low. Can you
  spell out the explanation?
\end{exercise}

\cmnt{%
  If we want to decompose the overall desirability of a given flat
  into the desirability of the flat's individual aspects, we need to
  assume that the desirability for the aspects has more than just an
  ordinal scale. Consider number of rooms. Perhaps you'd really like
  to have more than one room, but you don't care much about whether
  you have three rooms or four. Your ranking of the possibilities is 4
  rooms $\succ_r$ 3 rooms $\succ_r$ 2 rooms $\succ_r$ 1 room, but the
  difference in desirability between 4 rooms and 3 rooms is smaller
  than that between 2 rooms and 1 room. (`$\succ_r$' is supposed to
  represent your basic preferences over room number, setting aside
  your knowledge that more rooms typically cost more, etc.) To make
  such comparisons between differences meaningful, we need an interval
  scale.

  So let's assume that if we arbitrarily measure the desirability of 1
  room as 0 and the desirability of 2 rooms as 10, then your basic
  desires fix the number assigned to 3 rooms and 4 rooms. Let $V_r$ be
  the resulting value function. It is a value function not a utility
  function because we take it to be defined just over room numbers. We
  don't ``look inside'' the room number possibilities, taking into
  account what else is likely to be the case if a flat has four rooms.
} %

\begin{exercise}{3}
  Additivity greatly simplifies an agent's psychology. Suppose an
  agent's basic desires pertain to 10 propositions
  $A_1,A_2,\ldots,A_{10}$. There are $2^{10} = 1024$ conjunctions of
  these propositions and their negations (such as $A_1 \land A_2 \land
  \neg A_3 \land \neg A_4 \land A_5 \land A_6 \land \neg A_7 \land A_8
  \land A_9 \land \neg A_{10}$). To store the agent's value function
  in a database, we would therefore need to store up to 1024
  numbers. By contrast, how many numbers would we need to store if the
  value function is additive?
\end{exercise}


\section{Separability}

Under what conditions is value determined by adding subvalues? How are
different subvalue functions related to one another? What do subvalue
functions represent anyway? We can get some insight into these
questions by following an idea from the previous chapter and study how
an agent's value function might be derived from her preferences.

For the moment, we want to set aside the influence of the agent's
beliefs, so we are not interested in an agent's preferences between
lotteries or conditional prospects. Rather, we will look at an agent's
preferences between complete attribute lists, assuming the relevant
attributes comprise everything the agent cares about. 

The main motivation for starting with preferences is, as always, the
problem of measurement. We need to explain what it means that your
subvalue for a given attribute is 5 or 29. Since the numbers are
supposed to reflect, among other things, the importance (or weight) of
the relevant attribute in comparison to other attributes, it makes
sense to determine the subvalues from their effect on overall
rankings.
 
So assume we have preference relations $\succ$, $\succsim$, $\sim$
between some lists of attributes. To continue the illustration in
terms of flats, if you prefer a central \mbox{40 $\m^2$} flat for £500 to a
central 60 $\m^2$ for £800, then
\[
 \t{40 \m^2, \text{central}, \text{£500}} \succ
 \t{60 \m^2, \text{central}, \text{£800}}.
\]

Above we've assumed that you prefer cheaper flats to more expensive
flats, so that $V_P$ is a decreasing function of the monthly costs:
the higher the costs $c$, the lower $V_p(c)$. But of course you don't
prefer \emph{any} cheaper flat to \emph{any} more expensive flat. You
probably don't prefer a 5 $\m^2$ flat for £499 to a 60 $\m^2$ flat 
for £500. The other attributes also matter.

In what sense, then, do you prefer cheaper flats to more expensive
flats? We can cash this out as follows: whenever two flats agree in
terms of size and location, and one is cheaper than the other, then
you prefer the cheaper one. 

Let's generalize this concept. Suppose $A_1$ and $A_1'$ are two
attributes that can occur in the first position of an attribute list
-- for example 40 $\m^2$ and 60 $\m^2$ if the first position
represents the size of a flat, or £499 and £500 if it represents
price, etc. For any way of filling in all other
positions in the list, your preferences between attribute lists
determine a ranking of $A_1$ and $A_1'$. Call these your preferences
between $A_1$ and $A_1'$ \emph{conditional on} the attributes in the
other positions. That is, if
\[
   \t{A_1,A_2,\ldots,A_n} > \t{A_1',A_2,\ldots,A_n},
\]
then you prefer $A_1$ to $A_1'$ conditional on $A_2,\ldots,A_n$. Now suppose
your preferences between $A_1$ and $A_1'$ are the same conditional on any way of
filling in the other attributes. In other words, if
$\t{A_1,A_2,\ldots,A_n} > \t{A_1',A_2,\ldots,A_n}$, and we replace
$A_2,\ldots,A_n$ by arbitrary alternatives $A_2',\ldots,A_n'$, we still get
$\t{A_1,A_2',\ldots,A_n'} > \t{A_1',A_2',\ldots,A_n'}$. In that case, let's say
that your preferences between $A_1$ and $A_1'$ are \emph{independent} of the
other attributes.

In the example of the flats, your preference of £400 over £500 is plausibly
independent of the other attributes, for whenever two possible flats
agree in size and location, but one costs £400 and the other £500, you
prefer the one for £400.

Now suppose your preferences between any two attributes in the first
position (not just $A_1$ and $A_1'$) are independent of the other
attributes. Moreover, suppose your preferences between any attributes
in the second position are independent of the other attributes. And so
on for all positions. Then your preferences between attribute lists
are called \textbf{weakly separable}.

Weak separability means that your preference between two attribute lists that
differ only in one position does not depend on the attributes in the other
positions.

Consider the following preferences between four possible flats.
\begin{gather*}
\t{50 \m^2, \text{central}, \text{£500}} \succ \t{40 \m^2, \text{beach}, \text{£500}}\\
\t{40 \m^2, \text{beach}, \text{£400}} \succ \t{50 \m^2, \text{central}, \text{£400}}
\end{gather*}
Among flats that cost £500, you prefer central 50 \m$^2$ flats to 40
$\m^2$ flats at the beach. But among flats that cost £400, your
preferences are reversed: you prefer 40 $\m^2$ beach flats to 50
$\m^2$ central flats. In a sense, your preferences for size and
location depend on price. Nonetheless, your preferences may well be
weakly separable.

That's why weak separability is called `weak'. To rule out the present
kind of dependence, we need to strengthen the concept of
separability. Your preferences are \textbf{strongly separable} if your
ranking of lists that differ in \emph{one or more positions} does not
depend on the attributes in the remaining positions. In the example,
your ranking of $\t{50 \m^2, \text{central}, -}$ and $\t{40 \m^2,
  \text{beach}, -}$ depends on how the blank (`$-$') is filled in, and
so your preferences aren't strongly separable.

\begin{exercise}{2}
  Suppose all you care about is the degree of pleasure of you and your
  three friends. And suppose you prefer states in which you all
  experience equal pleasure to states in which your degrees of
  pleasure are very different. For example, you prefer $\t{2,2,2,2}$
  to $\t{2,2,2,8}$, and you prefer $\t{8,8,8,8}$ to $\t{8,8,8,2}$.
  Are your preferences weakly separable? Are they strongly separable?
\end{exercise}

\cmnt{%
  Explain why strong entails weak?%
} %

\begin{exercise}{2}
  Which of the following preferences violate weak separability or
  strong separability, based on the information provided?

  \medskip
  
  \noindent\hspace{-2mm}\begin{tabular}{lll}
    a. & b. & c.\\
    $\t{A_1,B_1,C_3} \!\succ\! \t{A_3,B_1,C_1}$ & $\t{A_1,B_3,C_1} \!\succ\! \t{A_1,B_3,C_2}$  & $\t{A_1,B_3,C_2} \!\succ\! \t{A_1,B_1,C_2}$ \\ 
    $\t{A_3,B_2,C_1} \!\succ\! \t{A_1,B_2,C_3}$ &  $\t{A_1,B_2,C_2} \!\succ\! \t{A_1,B_2,C_3}$ &  $\t{A_2,B_3,C_2} \!\succ\! \t{A_2,B_1,C_2}$ \\
    $\t{A_3,B_2,C_3} \!\succ\! \t{A_3,B_2,C_1}$ &  $\t{A_3,B_2,C_3} \!\succ\! \t{A_3,B_1,C_3}$ &  $\t{A_1,B_1,C_1} \!\succ\! \t{A_1,B_3,C_1}$ 
 \end{tabular}  
 \cmnt{%
   Answer: 

   In a., there's no counterex to w.s., but there is one to s.s.: hold fixed B in row 1 and 2.

   In b., there's no counterex to either.

   In c., there's a counterex to w.s.: hold fixed A,C in rows 1 and 3.

 } %
\end{exercise}

In 1960, G\'erard Debreu proved that strong separability is exactly what is
needed to ensure additivity.

To state Debreu's result, let's say that an agent's preferences over
attribute lists have an \textbf{additive representation} if there is a
value function $V$, assigning numbers to the lists, and there are
subvalue functions $V_1, V_2, \ldots, V_n$, assigning numbers to
attributes in the individual positions of the lists, such that the
following two conditions are satisfied. First, the preferences are
represented by $V$. That is, for any two lists $A$ and $B$,
\[
  A \pref B \text{ iff } V(A) > V(B), \text{ and }A \sim B \text{ iff }V(A) = V(B).
\]
Second, the value assigned to any list $\t{A_1,A_2,\ldots,A_n}$ equals
the sum of the subvalues assigned to the items on the list:
\[
V(\t{A_1,A_2,\ldots,A_n}) = V_1(A_1) + V_2(A_2) + \ldots + V_n(A_n).
\]

Now, in essence, Debreu's theorem states that if preferences over
attribute lists are complete and transitive, then they have an
additive representation if and only if they are strongly separable.

A technical further condition is needed if the number of attribute
combinations is uncountably infinite; we'll ignore that. Curiously,
the result also requires that there are at least three attributes that
matter to the agent. For two attributes, a different condition called
`double-cancellation' is required instead of
separability. Double-cancellation says that if $\t{A_1,B_1} \succsim
\t{A_2,B_2}$ and $\t{A_2,B_3} \succsim \t{A_3,B_1}$ then $\t{A_2,B_3}
\succsim \t{A_3,B_2}$. But let's just focus on cases with at least
three relevant attributes.

One corollary of Debreu's theorem may also be worth noting: if the agent's
preferences are defined over a sufficiently rich set of possibilities, then the
value function $V$ that additively represents her preferences is unique except
for the choice of unit and zero. Additivity therefore opens the way to another
potential response to the ordinalist challenge. The ordinalists claimed that
utility assignments are arbitrary as long as they respect the agent's preference
order. In response, one might argue that strongly separable preferences should
be represented by an additive utility (or value) function. The utilities
representing strongly separable preferences would then have an interval scale.

\begin{exercise}{2}
  Show that whenever $V$ additively represents an agent's preferences,
  then so does any function $V'$ that differs from $V$ only by the
  choice of zero and unit. That is, assume that $V$ additively
  represents an agent's preferences, so that for some subvalue
  functions $V_1,V_2,\ldots,V_n$,
  \[
  V(\t{A_1,A_2,\ldots,A_n}) = V_1(A_1) + V_2(A_2) + \ldots + V_n(A_n).
  \]
  Assume $V'$ differs from $V$ only by a different choice of unit and
  zero, which means that there are numbers $x>0$ and $y$ such that
  $V'(\t{A_1,A_2,\ldots,A_n}) = x\cdot V(\t{A_1,A_2,\ldots,A_n}) +
  y$. From these assumptions, show that there are subvalue functions
  $V_1',V_2',\ldots,V_n'$ such that
  \[
  V'(\t{A_1,A_2,\ldots,A_n}) = V_1'(A_1) + V_2'(A_2) + \ldots +
  V_n'(A_n).
  \]
  \cmnt{%
    (This proves that whenever $V$ additively represents an agent's
    preferences, then so does any function $V'$ that differs only by
    the choice of zero and unit. The converse can also be shown, but
    it's a little harder: if there are \emph{no} numbers $x>0$ and $y$
    for which $V'(\t{A_1,A_2,\ldots,A_n}) = x\cdot
    V(\t{A_1,A_2,\ldots,A_n}) + y$, then $V'$ does \emph{not}
    additively represent the agent's preferences.) $\star \star \star$
  }%
  \cmnt{%
    Additive separability with richness of state implies
    cardinality. Note that additive separability is not preserved
    under arbitrary positive transformations of $U$. For example, if
    $U(X_1,X_2,X_3) = \log(X_1) + \log(X_2) + \log(X_3)$, and we
    transform $U$ by the exponential function, then $U'(X_1,X_2,X_3) =
    e^{U(X_1,X_2,X_3)} = e^{\log(X_1) + \log(X_2) + \log(X_3)} =
    e^{\log(X_1)} e^{\log(X_2)} e^{\log(X_3)} = X_1 X_2 X_3$, and the
    product of three numbers cannot be represented as a sum of some
    function of the three numbers. By contrast, if we take any
    positive linear transform of $U$, then additivity is preserved:
    \begin{align*}
      a U(X_1,X_2,X_3) +b &= a(u_1(X_1) + u_2(X_2) + u_3(X_3)) + b
      &= a u_1(X_1)+b + a u_2(X_2)+b + a u_3(X_3)b.
    \end{align*}
    Indeed, the only transformation that preserve additive representation
    are increasing linear transformations. Hence additive separability
    implies cardinality.
  } %
\end{exercise}



\cmnt{%

\cmnt{%
  From Decision Analysis, pp.35f.
} %


The addition rule doesn't always work. But here's a condition under
which they are: call two outcomes \textbf{aspect equivalent} if the
agent is indifferent between them in every aspect; that is, if $a(w)
\sim a(w')$ for all aspects $a$. Now suppose an agent is indifferent
between any two outcomes that are aspect equivalent. In that case
there are scaling coefficients with which the addition rule correctly
represents her preferences over the outcomes.

How do we construct the utility function?

Let's pretend there are two aspects $a$ and $b$, both of which have a
minimum and a maximum value (in terms of preference). Let $V_a$ and
$V_b$ range between 0 and 1 accordingly. Now compare three outcomes
that agree in terms of $b$, but differ in $a$: in $o_0$, $a$ takes its
minimum value, in $o_1$ it takes its maximum value, in $o$ it takes
some intermediate value. For some value $x$, you should be indifferent
between $o$ and a lottery that yields $o_1$ with probability $x$ else
$o_0$. So 
\[
U(o) = x U(o_1) + (1-x)U(o_0).
\]
If you are indifferent between aspect equivalent outcomes, 
\begin{gather*}
U(o_0) = s_a V_a(a(o_0)) + s_b V_b(b(o_0)) = 0 + s_b V_b(b(o_0)).\\
U(o_1) = s_a V_a(a(o_1)) + s_b V_b(b(o_1)) = s_a + s_b V_b(b(o_1)).\\
U(o) = s_a V_a(a(o)) + s_b V_b(b(o)).
\end{gather*}
Substituting these in the previous equation, we get
\[
s_a V_a(a(o)) + s_b V_b(b(o)) = x(s_a + s_b V_b(b(o_1))) + (1-x) s_b V_b(b(o_0))
\]
Since $o_0, o_1, o$ agree in aspect $b$, this simplifies:
\begin{align*}
s_a V_a(a(o)) + k &= x(s_a + k) + (1-x) k\\
s_a V_a(a(o)) + k &= xs_a + xk + k-xk\\
s_a V_a(a(o)) + k &= xs_a + k\\
s_a V_a(a(o)) &= xs_a \\
V_a(a(o)) &= x.
\end{align*}

So we can determine the individual value functions. How do we
determine the scaling factors? Define $o_{00}$ to be the outcome in
which $a$ and $b$ both take minimum value; similarly for $o_{11}$ and
maximum value. Let $o_{01}$ have minimum value for $a$ and maximum for
$b$, conversely for $o_{10}$. For some value $x_a$, you should be
indifferent between $o_{10}$ and a lottery $x_a o_{11} + (1-x_a)
o_{00}$. So
\[
U(o_{10}) = x_a U(o_{11}) + (1-x_a)U(o_{00}).
\]
If you are indifferent between aspect equivalent outcomes, 
\begin{gather*}
U(o_{00}) = 0\\
U(o_{11}) = 1 \text{ provided $s_a +s_b = 1$}\\
U(o_{10}) = s_a.
\end{gather*}
Plugging these into the previous equation, we get
\[
s_a = x_a.
\]


Suppose for each $x_1,y_1$ and $z$, if $\t{x_1,x_2,\ldots,x_n}
\succsim z \succsim \t{y_1,x_2,\ldots,x_n}$ then there is some $t_1$
such that $z \sim \t{t_1, x_2,\ldots, x_n}$. Then $\succsim$ is said
to have \textbf{restricted solvability} w.r.t. the first
attribute. (Similarly for the other attributes.) Restricted
solvability is not quite enough for additive respresentation. We need
to add non-triviality of each position: that for each $i$ there are
$x_i, y_i$ such that $\t{x_1,\ldots,x_i\ldots,x_n} \succ
\t{x_1,\ldots,y_i\ldots,x_n}$. For the infinite case, we also need an
archimedian condition blocking lexical orderings. We also need to
assume weak separability.


} %

\cmnt{%

Often, different kinds or sources of motivation track different
aspects of possible worlds, and so we can check whether the relevant
motives combine additively or in some more complicated manner.

Here's a toy example. Suppose you have a basic, unreflective desire
for your own pleasure. You also think it is good to increase the
pleasure of others. Nothing else matters to you. Your two motive can
pull in opposite directions -- for example, when you consider donating
to charity. How do they combine to determine your overall preferences?

Suppose for simplicity that there is a fixed number $n$ of other
people. Since all you care about is the pleasure of yourself and
everyone else, your preferences between any two worlds is determined
by your preferences between the corresponding list $\t{P^w_{you},
  P^w_1,\ldots, P^w_n}$, where $P^w_{\text{you}}$ is your degree of pleasure
in the relevant world, and $P^w_1,\ldots,P^w_n$ is everyone else's degree
of pleasure. It is then not unreasonable to assume that the pleasure
of the individual people are separable. For example, whether some
amount of pleasure for Fred is better than some other amount does not
depend on the amount of pleasure in the others. Debreu's theorem then
implies that your preferences are represented by an additive utility
function
\[
U(w) = V_a(P_\text{you}^w) + V_1(P_1^w) + V_2(P_2^w) + \ldots.
\]
Here $V_1$ measures how much you care about $P_1$'s degree of pleasure
etc. his means that you
} %

\cmnt{%

  Dual-ranking?

  Harder if morality imposes fixed side constraints.

} %


\begin{exercise}{2}
  Imagine you can freely choose four courses for next semester. You
  assess each course by a range of criteria (such as whether the
  course will teach you anything useful). On this basis, you determine
  an overall ranking of the courses and sign up for the top four. Why
  might this not be a good idea?
\end{exercise}

\cmnt{%

\section{Kinds of value}

The concept of separability is quite versatile. Let's turn to a
different way to decompose a value function. I emphasised that utility
(and therefore value) comprises a wide range of factors, from urges to
commitments to reflective judgments about what would be best. We can
understand each of these sources as a subvalue function. If the
overall preferences are separable, overall utility is determined by
adding up all the subvalues. 

Here's a toy example. Suppose you are motivated by hunger, commitment
to a certain diet, and a desire for social conformity . These can
easily pull in different directions. If all your friends are ordering
pizza, then the first and third might speak in favour of joining in,
while the second speaks against. How do you balance these motives?

To apply Debreu's theorem, we need to map the different motives onto
attributes of possible worlds. The basic thought is to treat the
extent to which a world satisfies the individual motives as attributes
of the world. 

It's easiest if we already assume this is captured by some value
functions $V_h$, $V_s$, etc. Then all you need to know about a world
to determine an overall ranking is how it is ranked by $V_h$, $V_s$,
etc. So we can represent the preferences between worlds by preferences
between these attribute values. Quite plausibly, there's a Pareto
principle: if two worlds are equally good in all respects except one,
then the world in which the one respect is better is better overall. xxx



Suppose we can say, for any worlds $w,w'$ how they compare with
respect to the individual (sub)values. So we have $w \succ_h w'$, $w'
\succ_s w$, etc. Now consider how $h$ ranks two worlds that are equal
with respect to all other subvalues. 


Separability: $\t{A_1, A_2, A_3} \succsim \t{A_1', A_2, A_3}$ fixed
for all $A_2,A_3$. But are worlds collections of motive values?
Otherwise $\succsim$ is not defined over these lists. It looks like we
have to assume a cardinal scale for the motives. 

 we can For example, your hunger ranks favours
states in which you are eating, but it is indifferent between other

To see how you balance the different
motives, we can check if your all-things-considered preferences are
separable. For example, if you could choose between eating something
and not eating something

There is no a priori argument for separability across motives, and it
is easy to think of counterexamples. For example, you may prefer xxx
if your basic needs (shelter, food) are satisfied. ?


Here's a toy example that is easy to model. Suppose all you care about
is your own pleasure as well as the pleasure of certain other people
-- the members of your family, say. It is then not unreasonable to
assume that the pleasure of the individual people are separable. For
example, whether some amount of pleasure for Fred is better than some
other amount does not depend on the amount of pleasure in the
others. Debreu's theorem then shows that your preferences are
represented by an additive utility function
\[
U(w) = V_a(\emph{Pleasure of $A$}(w)) + V_b(\emph{Pleasure of $B$}(w))
+ \ldots.
\]
Moreover, the representation is unique up to an affine transformation.

Dual-ranking?

Harder if morality imposes fixed side constraints.

\cmnt{%

  Sen on commitment vs preference?

  See
  http://www.tnr.com/article/books-and-arts/true-lies on the book
  ``Private Truths, Public Lies: The Social Consequences of Preference
  Falsification'' 1995, By Timur Kuran.

  Kuran's real interest is not in moral evaluation, but in explaining
  individual and collective choices. To this end, he offers a simple
  economic framework based on three factors, which he describes
  (somewhat awkwardly) as intrinsic utility, reputational utility and
  expressive utility. A person's purely private preference is based on
  the intrinsic utility, to him, of the options under
  consideration. Some people really want to get rid of affirmative
  action or welfare programs, because they think that these are bad
  things, but their private preference may not be expressed publicly,
  because of the loss of reputational utility that would come from
  expressing it. The importance of reputational utility in a
  particular case depends on the extent of the risk to your
  reputation, and also on how much you care about your reputation. And
  people get what Kuran calls expressive utility from bringing their
  public statements into alignment with their private judgments. We
  all know people who hate to bow before social pressures; such people
  are willing to risk their reputation because what they especially
  hate is to speak or act in a way that does not reflect their true
  beliefs.
} %

} %

\section{Separability across time}\label{sec:separability-time}

According to psychological hedonism, the only thing people ultimately
care about is their personal pleasure. But pleasure isn't constant. So
the hedonist conjecture leaves open how people rank different ways
pleasure can be distributed over a lifetime. Unless an agent just
cares about her pleasure at a single point in time, a basic desire for
pleasure is really a concern for a lot of things: pleasure now,
pleasure tomorrow, pleasure the day after, and so on. We can think of
these as the ``attributes'' in the agent's value function. The
hedonist's value function somehow aggregates the value of pleasure
experienced at different times.

To keep things simple, let's pretend that pleasure does not vary
within any given day. We might then model a hedonist value function as
a function that assigns numbers to lists like $\t{1,10,-1,2,\ldots}$,
where the elements in the list specify the agent's degree of pleasure
today (1), tomorrow (10), the day after (-1), and so on. Such
attribute lists in which successive positions correspond to successive
points in time are called \textbf{time streams}.

A hedonist agent would plausibly prefer more pleasure to less at any
point in time, no matter how much pleasure there is before or
afterwards. If so, their preferences between time streams are weakly
separable. Strong separability is also plausible: whether the agent
prefers a certain amount of pleasure on some days to a different
amount of pleasure on these days should not depend on how much
pleasure the agent has on other days. It follows by Debreu's theorem
that the value the agent assigns to a time stream can be determined as
the sum of the subvalues she assign to the individual parts of the
stream. That is, if $p_1$, $p_2$, \ldots, $p_n$ are the agent's
degrees of pleasure on days $1, 2, \ldots, n$ respectively, then there
are subvalue functions $V_1,V_2,\ldots,V_n$ such that
\[
V(\t{p_1,p_2,\ldots,p_n}) = V_1(p_1) + V_2(p_2) + \ldots + V_n(p_n).
\]

We can say more if we make one further assumption. Suppose an agent
prefers stream $\t{p_1,p_2,\ldots,p_n}$ to an alternative
$\t{p_1',p_2',\ldots,p_n'}$. Now consider the same streams with all
entries pushed one day into the future, and prefixed with the same
degree of pleasure $p_0$. So the first stream turns into $\t{p_0,
  p_1,p_2,\ldots,p_n}$ and the second into $\t{p_0,
  p_1',p_2',\ldots,p_n'}$. Will the agent prefer the modified first
stream to the modified second stream, given that she preferred the
original first stream? If the answer is yes, then her preferences are
called \textbf{stationary}. From a hedonist perspective, stationarity
seems plausible: if there's more aggregated pleasure in
$\t{p_1,p_2,\ldots,p_n}$ than in $\t{p_1',p_2',\ldots,p_n'}$, then
there is also more pleasure in $\t{p_0,p_1,p_2,\ldots,p_n}$ than in
$\t{p_0,p_1',p_2',\ldots,p_n'}$.

It is not hard to show that if preferences over time streams are
separable and stationary (as well as transitive and complete), then
they can be represented by a value function of the form
\[
V(\t{A_1,\ldots,A_n}) = V_1(A_1) + \delta \cdot V_1(A_2) +
\delta^2 \cdot V_1(A_3) \ldots + \delta^{n-1} \cdot V_1(A_n),
\]
where $\delta$ is a fixed number. The interesting thing here is that the
subvalue function for all times equals the subvalue function $V_1$ for the first
time, scaled by the exponential \textbf{discounting factor} $\delta^i$.

\cmnt{%

  (The argument is actually quite simple. By separability,
  $U(p_1,\ldots,p_n) = u_1(p_1) + \ldots$. By stationarity, $u_1(p_1)
  + \ldots \geq u_1(p_1') + \ldots$ iff $u_2(p_1) + \ldots \geq
  u_2(p_1') + \ldots$. By cardinal uniqueness there exist $\delta > 0$
  and $b_t$ such that $u_{i+1} = \delta u_i + b_i$, which by cardinal
  uniqueness again means we can find another representation with
  $u_{i+1} = \delta u_i$.)

} %

So if a hedonist agent has strongly separable and stationary preferences, then
the only remaining question is to what extent she discounts future pleasure. If
$\delta = 1$, she values pleasure equally no matter when it occurs. If
$\delta = \nicefrac{1}{2}$, then one unit of pleasure tomorrow is worth half as
much as one unit of pleasure today; the day after tomorrow it is worth a
quarter, and so on.

\begin{exercise}{1}
  Consider the following streams of pleasure:
  \begin{enumerate}
    \itemsep-0.3em 
  \item[S1:] $\t{1,2,3,4,5,6,7,8,9}$ 
  \item[S2:] $\t{9,8,7,6,5,4,3,2,1}$
  \item[S3:] $\t{1,9,2,8,3,7,4,6,5}$ 
  \item[S4:] $\t{9,1,8,2,7,3,6,4,5}$ 
  \item[S5:] $\t{5,5,5,5,5,5,5,5,5}$
  \end{enumerate}
  Assuming present pleasure is valued in proportion to its degree, so
  that $V_1(p) = p$ for all degrees of pleasure $p$, how would a
  hedonist agent with separable and stationary preferences rank these
  streams, provided that (a) $\delta = 1$, (b)
  $\delta < 1$, (c) $\delta > 1$? (You need to give three answers.)
\end{exercise}

Even if you're not a hedonist, you probably care about some things
that can occur (and re-occur) at different times: talking to friends,
going to concerts, having a glass of wine, etc. The formal results
still apply. If your preferences over the relevant time streams are
separable and stationary, then they are fixed by your subvalue
function for having the relevant events (talking to friends, etc.)
right now and a discounting parameter $\delta$.

Some have argued that stationarity and separability across times are
requirements of rationality. Some have even suggested that the only
rationally defensible discounting factor is 1, on the ground that we
should be impartial with respect to different parts of our life.

One argument in favour of stationarity is that -- under certain
modelling assumptions -- it is required to protect the agent from a
kind of disagreement with her future self. To illustrate, suppose you
prefer getting £100 now to getting £105 tomorrow, but you also prefer
£105 in 11 days over £100 in 10 days. These preferences violate
stationarity. For if you prefer $\t{\text{£100}, \text{£0}, \ldots}$
to $\t{\text{£0}, \text{£105}, \ldots}$ (the entries in the positions
specifying how much money you get on successive days), then by
stationarity you also prefer $\t{\text{£0}, \text{£100}, \text{£0},
  \ldots}$ to $\t{\text{£0}, \text{£0}, \text{£105}, \ldots}$, and
$\t{\text{£0}, \text{£0}, \text{£100}, \text{£0}, \ldots}$ to
$\t{\text{£0}, \text{£0}, \text{£0}, \text{£105}, \ldots}$, and so on;
so £100 in 10 days should be preferred to £105 in 11 days. Now suppose
your (non-stationary) preferences remain the same for the next 10
days. At the end of this time, you then still prefer £100 now over
£105 tomorrow. But your new ``now'' is your old ``in 10 days''. So
your new preferences disagree with those of your earlier self in the
sense that what you now regard as better is what your earlier self
regarded as worse. That kind of disagreement is called \textbf{time
  inconsistency}.

Empirical studies show that time inconsistency is fairly common. People often
prefer their future selves to study, eat well, and exercise, but choose burgers
and TV for today. These preference do look problematic.

On the other hand, other violations of stationarity and even separability across
time look perfectly sensible. For example, suppose you value having a glass of
wine every now and then. But only now and then; you don't want to have wine
every day. It follows that your preferences violate both separability and
stationarity. You violate stationarity because even though you might prefer a
stream $\t{\text{wine}, \text{no wine}, \text{no wine}, \ldots}$ to $\t{\text{no
    wine}, \text{no wine}, \text{no wine}, \ldots}$, your preference reverses if
both streams are prefixed with wine (or many instances of wine). You violate
separability because whether you regard having wine in $n$ days as desirable
depends on whether you will have wine right before or after these days.

Even if an agent only cares about pleasure, it is not obvious why a
rational agent might not (say) prefer relatively constant levels or
pleasure over wildly fluctuating levels, or the other way round.
Either preference would violate both stationarity and separability.

\cmnt{%
  What's going on in these examples? Do we have time inconsistency? We
  must have if the underlying preferences stay the same. (See
  e.g. Halevy, ``Time Consistency: Stationarity and time invariance'',
  2015.) I guess that is what fails. In the wine case, you prefer wine
  today over no wine today, but in 10 days, after lots of wine, your
  preference is reversed. The problem is that we're ignoring
  pre-histories. Preferences are defined only over future streams.%
} %

On the standard way of modelling preferences over time streams (which ignores
what happened in the past), these preferences are time inconsistent. But that
kind of time inconsistency does not look problematic.

What shows up in the more problematic kind of time inconsistency
concerning studying, food, or exercise, is that preferences have a
range of different sources, as I emphasized in chapter
\ref{ch:utility}. When we reflect on having fries or salad now, we are
more influenced by spontaneous cravings than when we consider the same
options in the distant future.

If different sources or kinds of preference pertain to different
aspects of the world, then the results we have reviewed can also clarify
how these sources are aggregated into the agent's
all-things-considered preference. In particular, if the agent's
preferences are separable across the relevant aspects, then (and only
then) the all-things-considered preferences can be understood to
result by adding up (and possibly scaling) independent scores assigned
by the different sources.

\cmnt{%
  This can be observed in other cases as well where motives of
  different kinds pertain to different aspects of the world, for
  example, a desire to help strangers vs to help people you're
  acquainted with.
}%

\section{Separability across states}

Let's briefly return to decision problems. In a decision problem,
every available act leads to a particular outcome in each of the
relevant states. Standard decision theory assumes that a rational
agent prefers an act $A$ to an act $B$ in a given decision problem
just in case $A$ has greater expected utility, defined as
\[
EU(A) = U(O_1)\Cr(S_1) + U(O_2)\Cr(S_2) + \ldots + U(O_n)\Cr(S_n),
\]
where $S_1,S_2,\ldots,S_n$ are the states and $O_1,O_2,\ldots,O_n$ are
the various outcomes of act $A$ in those states.

Standard decision theory therefore assumes that the only thing that
matters to the agent's preferences between acts, in any fixed decision
problem, are the possible outcomes. If two acts lead to the same
outcomes in all states, the agent will be indifferent between them. We
can therefore model the agent's preferences between acts as 
preferences between lists of outcomes, one for each state.
For example, in the mushroom problem from chapter \ref{ch:overview},
eating the mushroom can be modelled as $\t{\text{satisfied},
  \text{dead}}$, and not eating as $\t{\text{hungry}, \text{hungry}}$.

Now, if an agent ranks acts by their expected utility, then her
preferences between acts have an additive representation, since they
are represented by a function $V$ whose values are determined by
adding up subvalues assigned to the individual outcomes: the function
$V$ is the $EU$ function; the subvalue assigned to outcome $O_1$ is
$U(O_1)\Cr(S_1)$, and so on. 

By Debreu's theorem, rational preferences have an additive
representation if and only if they are strongly separable. So standard
decision theory implies that preferences between acts are (strongly)
separable across states, meaning that the desirability of an act's outcome
in one state does not depend on the outcomes in other states.

Admittedly, this is an elaborate path to a fairly obvious result. I mention it
for two reasons. First, it shows that the two responses to the ordinalist
challenge are actually closely related. In effect, Ramsey, Savage, and von
Neumann assumed that rational preferences are separable across states, and that
they should have an additive representation in terms of expected utility.

Second, a general consequence of separability is that the relevant
preferences are insensitive to certain ``shapes'' in the distribution
of subvalues. In particular, separable preferences cannot prefer even
distributions to uneven distributions. This may seem to raise a potential
problem with the MEU Principle (and any other decision rule that
implies separability across states). For example, consider the
following schematic decision problem:
%
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr State 1 (\nicefrac{1}{2}) & \gr State 2 (\nicefrac{1}{2}) \\\hline
    \gr $A$ & Outcome 1 (+10) & Outcome 1 (+10) \\\hline
    \gr $B$ & Outcome 2 (-10) & Outcome 3 (+30) \\\hline
  \end{tabular}
\end{center}
%
Option $A$ leads to a guaranteed outcome with utility 10, while option
$B$ leads either to a much better outcome or to a much worse one. The
expected utilities are the same, but one might think an agent might
rationally prefer the safe option $A$ just because it is safe --
because the utility distribution $\t{10,10}$ is more even than
$\t{\text{-10},30}$. Much more on that in the next chapter.

\begin{exercise}{2}
  Where in their axioms do Savage and von Neumann and Morgenstern
  postulate a kind of separability across states?
\end{exercise}

\section{Harsanyi's ``proof of utilitarianism''}

The ordinalist movement, which rejected the quantitative concept of utility,
posed a challenge not only to the MEU Principle, but also to utilitarianism in
ethics. According to utilitarianism, an act is right just in case it brings
about the best available state of the world; a state of the world is assumed to
be better than an alternative just in case the sum of the utility of all people
in that state is greater. Without a numerical (and not just ordinal) measure of
utility, the second of these claims becomes meaningless. We would need a new
criterion for ranking states of the world.

One such criterion was proposed by Pareto. Recall that Pareto did not
deny that people have preferences. So if we want to rank two states of
the world, we can meaningfully ask which of them people prefer. And
that allows us to define at least a partial order on the possible
states:
%
\begin{genericthm}{The Pareto Condition}
  If everyone is indifferent between  $A$ and $B$, then $A$ and $B$
  are equally good; if at least one person prefers $A$ to $B$ and no
  one prefers $B$ to $A$, then $A$ is better than $B$.
\end{genericthm}
%

Unlike classical utilitarianism, however, the Pareto Condition offers
little moral guidance. For instance, while classical utilitarianism
suggests that one should harvest the organs of an innocent person in
order to save ten others, the Pareto Condition does not settle whether
it would be better or worse to harvest the organs, given that the
person to be sacrificed ranks the options differently than those who
would be saved.

\cmnt{%

  When Ramsey, Savage, and von Neumann and Morgenstern showed how a
  meaningful numerical quantity of utility could be derived from an
  agent's preferences, they helped to rescue the MEU Principle, but
  they did not help classical utilitarianism. For remember that the
  utility functions derived from personal preferences have arbitrary
  zero and unit. Thus if according to one adequate representation of
  our preferences, my utility for a given state is 10 and yours is 0,
  then on another equally adequate representation, my utility for the
  state will be -100 and yours 20.

} %

\begin{exercise}{1}[The Condorcet Paradox]
  A ``democratic'' strengthening of the Pareto condition might say that whenever
  \emph{a majority} of people prefer $A$ to $B$, then $A$ is better than $B$.
  But consider the following scenario. There are three relevant states: $A,B,C$,
  and three people. Person 1 prefers $A$ to $B$ to $C$. Person 2 prefers $B$ to
  $C$ to $A$. Person 3 prefers $C$ to $A$ to $B$. If betterness is decided by
  majority vote, which of $A$ and $B$ is better? How about $A$ and $C$, and $B$
  and $C$?
\end{exercise}

In 1955, John Harsanyi proved a remarkable theorem that seemed to
rescue, and indeed vindicate, classical utilitarianism. 

To begin, Harsanyi assumes that there is a betterness order
between states of the world which is also defined for lotteries between
such states. That is not yet a substantive premise, as we have not yet
made any substantive assumptions about the order. 

Harsanyi's first premise is that the order satisfies the axioms of von
Neumann and Morgenstern. By von Neumann and Morgenstern's
representation theorem, it follows that the betterness order is
represented by a (``social'') utility function that is unique except
for the choice of unit and zero.

Second, Harsanyi assumes that the betterness order satisfies the
Pareto condition (both for states and for lotteries).

Finally, Harsanyi assumes that each person -- of which he assumes for simplicity
that there is a fixed number $n$ -- has personal preferences between the
relevant states and lotteries, and that these preferences also satisfy the von
Neumann and Morgenstern axioms. So the personal preferences  are represented
by $n$ personal utility functions.

Note that the Pareto condition states a simple kind of separability
across people. The assumption that social and personal utility rank
lotteries by their expected utility, which follows from the von
Neumann and Morgenstern construction, amounts to separability in
another dimension, across states. As it turns out, Debreu's results
can be strengthened for cases in which the attributes are separable
across two independent dimensions (here, people and states). Drawing
on this result, Harsanyi showed that it follows from the above three
assumptions that the individual and social preferences are represented
by utility functions $U_s$ and $U_1,U_2,\ldots,U_n$ such that the
social utility function is simply the sum of the individual utility
functions: for any state or lottery $A$,
\[
  U_s(A) = U_1(A) + U_2(A) + \ldots + U_n(A).
\]
And that looks just like classical utilitarianism.

On closer inspection, things are less clear-cut. For a start, recall
that the utility functions established by von Neumann and
Morgenstern's representation theorem have arbitrary units and
zeroes. So if according to one adequate representation of our
preferences, my utility for a given state is 10 and yours is 0, then
according to another, equally adequate representation, my utility for
the state is 10000 and yours -3. All Harsanyi's theorem tells us is
that there is \emph{some} utility representation of our individual
preferences relative to which our utilities add up to social
utility. This is compatible with the assumption that social utility is
almost entirely determined by the preferences of a single person,
because her utilities are scaled so as to dwarf all the others. That
does not look like classical utilitarianism.

Also, anyone who is not already a utilitarian should probably reject
the Pareto Condition. After all, the condition implies that the only
thing that matters, from a moral perspective, is the satisfaction of
people's preferences. If anything else had any moral weight -- whether
people's rights are respected, whether animals suffer, whether God's
commands are obeyed, or whatever -- then it could happen that everyone
is indifferent between $A$ and $B$, and yet $A$ is actually better.

\cmnt{%
  The assumption that social preferences are a complete order rules
  out interpersonal incommensurabilities.
} %

In general, if someone seems to offer a mathematical proof
of a substantive normative principle, you can be sure that either the
principle isn't really established or it has been smuggled in through
the premises. 


\section{Further reading}

The topic of this chapter is known as ``multi-attribute utility theory''. For an
overview with further references and applications, see
\begin{itemize}
  \item Paul Weirich:
  \href{https://doi.org/10.1007/978-94-007-1433-5_20}{``''Multi-Attribute
    Approaches to Risk''} (2012).
\end{itemize}

On separability across time, have a look at
\begin{itemize}
  \item Johanna Thoma: ``Temptation and preference-based instrumental rationality'' (2018)
\end{itemize}

% For an opinionated review of various positions on time (in)consistency, see
% \begin{itemize}
% \item Tomasz Żuradzki: \href{https://philpapers.org/archive/URATAR.pdf}{``Time-biases  and  Rationality:  The Philosophical  Perspectives  on  Empirical Research  about  Time  Preference''} (2016)
% \end{itemize}

A lucid introduction to Harsanyi's argument for utilitarianism is
\begin{itemize}
  \item John Broome: ``General and Personal Good: Harsanyi's Contribution to the Theory of Value'' (2015)
\end{itemize}


\begin{essay}
  Is time inconsistency always irrational? Can you explain why, or why not? 
\end{essay}

\begin{essay}
  Following up on a thought at the end of section
  \ref{sec:separability-time}: Can you think of a way to define
  separability directly for sources of utility, without assuming that
  different kinds of motives pertain to different aspects of the
  world?
\end{essay}

% The importance of subvalues is emphasised in Pettit 1991.
   

\cmnt{%
An interesting observation in behavioural economics concerns the
difference between single and repeated offers of gambles. Many people
would reject a gamble $[0.5? \$200 : -\$100]$; but hardly anyone would
reject a long sequence of such gambles. It is clear why: if losses
hurt more than gains are pleasurable, then a single gamble looks much
less attractive than a sequence of gambles. With a hundred instances
of the above gamble, the chance of a net loss is down from $1/2$ to
$1/2300$. This is interesting because it shows that we mustn't assume
that if an agent prefers $A$ over $B$ in a single choice, she also
prefers $A$ over $B$ when the choice is repeated, or known to be part
of a sequence. As Kahneman \citey[338f.]{kahneman11thinking} points
out, it also suggests that we are often irrational when we evaluate
choices by themselves, not regarding them in a wider context. After
all, every choice is in a sense part of a long sequence. If you reject
gambles with positive expected payoff out of loss aversion, you do
worse in the long run. 
} %


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
