\chapter{Probabilism}\label{ch:probabilism}

\section{Justifying the probability axioms}

The hypothesis that rational degrees of belief satisfy the
mathematical conditions on a probability measure is known as
\textbf{probabilism}. In this chapter, we will look at some arguments
for probabilism. We do so not because the hypothesis is especially
controversial (by philosophy standards, it is not), but because it is
instructive to reflect on how one could argue for an assumption like
this, and also because the task will bring us back to a more
fundamental question: what it means to say that an agent has
such-and-such degrees of belief in the first place.

We will assume without argument that if a rational agent has degrees
of belief in some propositions $A$ and $B$, then she also has degrees
of belief in their conjunction, disjunction, and negation. Probabilism
then reduces to the hypothesis that rational degrees of belief satisfy
the probability axioms -- specifically, Kolmogorov's axioms (i)--(iii):
\begin{itemize}
\itemsep0em 
\item[(i)] For any proposition $A$, $0 \leq \Cr(A) \leq 1$.
\item[(ii)] If $A$ is logically necessary, then $\Cr(A) = 1$.
\item[(iii)] If $A$ and $B$ are logically incompatible, then $\Cr(A \lor B) = \Cr(A) + \Cr(B)$.
\end{itemize}

Consider axiom (i). Why should rational degrees of belief always fall
in the range between 0 and 1? Why would it be irrational to believe
some proposition to degree 7? The question is hard to answer unless we
have some idea of what it would mean to believe a proposition to
degree 7.

It is tempting to think that axiom (i) does not express a substantive
norm of rationality, but a convention of representation. We have
decided to represent strength of belief by numbers between 0 and 1,
where 1 means absolute certainty. We could just as well have decided
to use numbers between 0 and 100, or between -100 and +100. Having set
up the convention to put the upper limit at 1, it doesn't make any
sense to assume that an agent believes something to degree 7.

Axioms (ii) and (iii) look more substantive. It seems that we can at
least imagine an agent who assigns degree of belief less than 1 to a
logically necessary proposition or whose credence in a disjunction of
incompatible propositions is not the sum of her credence in the
individual disjuncts. Still, we need to clarify what exactly it is
that we're imagining if we want to discuss whether the imagined states
are rational or irrational.

For example, suppose we understand strength of belief as a certain
introspectible quantity: a basic feeling of conviction people have
when entertaining propositions. Axiom (ii) would then say that when
agents entertain logically necessary propositions, they ought to
experience this sensation with maximal intensity. It is hard to see
why this should be norm of rationality. It is also hard to see why
the sensation should guide an agent's choices in line with the MEU
Principle, or why it should be sensitive to the agent's evidence.

So if we understand degrees of belief as measuring the intensity of a
certain feeling, the norms of Bayesian decision theory and Bayesian
epistemology look implausible and inexplicable. The same is true if we
understand degrees of belief as measuring some other basic
psychological quantity: why should that quantity satisfy the
probability axioms, guide behaviour, respond to evidence, etc.?

A more promising line of thought assumes that strength of belief is
defined (perhaps in part) by the MEU Principle. On that approach, what
we mean when we say that an agent has such-and-such degrees of belief
is that she is (or ought to be) disposed to make certain choices. We
can then assess the rationality of the agent's beliefs by looking at
the corresponding choice dispositions.

Unfortunately, beliefs alone do not settle rational choices: the
agent's desires or goals also play a role. The argument we are now
going to look at therefore fixes an agent's goals by assuming that in
certain choices, utility equals monetary payoff. In chapter
\ref{ch:preference}, we will encounter a more sophisticated relative
of the following argument that does not fix the utilities.

\section{The betting interpretation}

Before we return to degrees of belief, let's briefly look at other
numerical quantities in science. Mass, for example. What do we mean
when we say that an object -- a chunk of iron perhaps -- has a mass of
2 kg? There are no little numbers written in chunks of iron, just as
there are no little numbers written in the head. As with degrees of
belief, there is an element of conventionality in the way we represent
masses by numbers: instead of representing the chunk's mass by the
number 2, we could just as well have used a different scale on which
the mass would be 2000 or 4.40925. (Appending `kg' to the number, as
opposed to `g' or `lb', hints at the conventional scale.)

I am not suggesting that mass itself is conventional. I take it that whether a chunk
of iron has a mass of 2 kg is a completely objective, mind-independent
matter. If there were no humans, the chunk would still have that
mass. What's conventional is only the representation of masses (which
are not intrinsically numerical) by numbers.

The reason why we can measure mass in numbers -- and the reason why we
know anything at all about mass -- is that things tend to behave
differently depending on their mass. The greater an object's mass, the
harder the object is to lift up or accelerate. Numerical measures of
mass reflect these dispositions, and can be standardized by reference
to particular manifestations. For example, if we put two objects on
opposite ends of a balance, the one with greater mass will go down. So
we could choose a random chunk of iron, call it the ``standard
kilogram'', and stipulate that something has a mass of $n$ kg just in
case it balances against $n$ copies of the standard kilogram (or
against $n$ objects each of which balances against the standard
kilogram).

Can we take a similar approach to degrees of belief? The idea would be
to find a characteristic way in which degrees of belief manifest
themselves in behaviour and use that to define a numerical scale for
degrees of belief.

So how do you measure someone's degrees of belief? The classical
answer is: by offering them a bet. 

Consider a bet that pays £1 if it will rain tomorrow at noon, and
nothing if it won't rain. How much would you be willing to pay for
this bet?

We can calculate the expected payoff, i.e.\ the average of the
possible payoffs, weighted by their subjective probability.  Suppose
your degree of belief in rain tomorrow is $x$, and your degree of
belief in not-rain is $1-x$. Then the bet would give you £1 with
probability $x$ and £0 with probability $1-x$. So the expected payoff
is $x \times \text{£1} + (1-x) \times \text{£0} = \text{£}x$. This
suggests that the bet is worth £$x$. That is, £$x$ is the most you
should pay for the bet.

\begin{exercise}
  Suppose your degree of belief in rain is $0.8$ (and your degree of
  belief in not-rain 0.2). For a price of £0.70 you can buy a bet that
  pays £1 if it rains and £0 if it doesn't rain. Draw a decision
  matrix for your decision problem and compute the expected utility of
  the acts, assuming your subjective utilities equal the net amount of
  money you have gained in the end. $\star$
\end{exercise}

If we're looking for a way to measure your degrees of belief, we can
turn this line of reasoning around: if £$x$ is the most you're willing
to pay for the bet, then $x$ is your degree of belief in the
proposition that it will rain. This leads to the
following suggestion.

\begin{genericthm}{The betting interpretation}
  An agent believes a proposition $A$ to degree $x$ just in case she
  would pay up to £$x$ for a bet that pays £1 if $A$ is true and £0
  otherwise.
\end{genericthm}

The betting interpretation is meant to have the same status as the
above (hypothetical) stipulation that an object has a mass of $n$ kg
just in case it balances against $n$ copies of the standard kilogram.
On the betting interpretation, offering people bets is like putting
objects on a balance scale. For some prices, the test subject will
prefer to buy the bet, for others she will be prefer to sell the bet
to others; in between there is a point at which the price of the bet
is in balance with the expected payoff, so the subject will be
indifferent about the bet. The price at the point of balance reveals
the subject's degree of belief. The stake of £1 is a unit of
measurement, much like the standard kilogram in the measurement of
mass.

\cmnt{%

 xxx This exercise was too imprecise. Improve!!

\begin{exercise}
  Show that if an agent's degree of belief in $A$ is $x$, then the
  a bet that pays £100 if $A$ and £0 if $\neg A$ has expected
  payoff £$100 \times x$. (Thus if we had used £100 instead of £1 in
  the betting interpretation, then degrees of belief would range
  from 0 to 100 rather than from 0 to 1.) 
\end{exercise}

} %

The betting interpretation gives us a clear grip on what it means to
believe a proposition to a particular degree. It also points towards
an argument for probabilism. For we can show that if an agent's
degrees of belief do not satisfy the probability axioms (for short, if
her beliefs are not \textbf{probabilistic}), then the agent is
disposed to enter bets that amount to a guaranteed loss.

\section{The Dutch Book theorem}

In what follows, we are going to assume that if an agent is not
willing to buy a bet for £$x$, then she would be be willing to sell
the bet for that price. Selling a bet means offering it to somebody
else. The idea is that if you judge a bet to worth less than £$x$,
then you should be happy to offer someone the bet for a price of £$x$.

\begin{exercise}
  Suppose your degree of belief in rain is $0.8$ (and in not-rain 0.2)
  and someone offers you £0.90 for a bet that pays £1 if it rains and
  £0 if it doesn't rain. Should you sell (i.e.\ offer) them the bet?
  Draw a decision matrix for your decision problem and compute the
  expected utility of the acts, assuming your subjective utilities
  equal the net amount of money you get in the end. $\star$
\end{exercise}

In betting jargon, a combination of bets (bought or sold) is called a
`book'. A combination of bets that amounts to a guaranteed loss is a
\textbf{Dutch book}. We will now prove that if an agent's degrees of
belief violate one or more of the Kolmogorov axioms, and she values
bets in accordance with their monetary payoff, then she is prepared to
accept a Dutch Book.

We begin with axiom (i). Suppose your credence in some proposition $A$
is greater than 1. For concreteness, let's say $\Cr(A)=2$. By the
betting interpretation, this means you'd be willing to pay up to £2
for a deal that pays you back either £0 or £1, depending on whether
$A$ is true. You're guaranteed to lose at least £1. More generally, if
your degree of belief in $A$ is greater than 1, then you are
guaranteed to lose at least the difference between your degree of
belief and 1.

Similarly, suppose your credence in $A$ is below 0. Let's say it's
-1. By the betting interpretation, this means you would pay no more
than £-1 for a bet on $A$ and you'd be willing to sell the bet for any
price above £-1. What does it mean to sell a bet for £-1?  It means to
pay someone £1 to take the bet. So you would be willing to pay up to
£1 for me to take the bet from you, with no chance of getting any
money back. You're guaranteed to lose at least £1. Again, the argument
generalizes to any degree of belief below 0.

I leave the case of axiom (ii) as an exercise.

\begin{exercise}
  Suppose your degrees of belief violate axiom (ii). Describe a bet
  you should be willing to sell for a price less than what you could
  possibly get back. $\star\star$
\end{exercise}

Now for axiom (iii). Suppose your credence in the disjunction of two
logically incompatible propositions $A$ and $B$ is not the sum of your
credence in the individual propositions. For concreteness, let's
assume $\Cr(A) = 0.4$, $\Cr(B) = 0.2$, and $\Cr(A \lor B) = 0.5$. By
the betting interpretation, you'll then be willing to sell a bet on $A
\lor B$ for at least £0.50, and you'll be willing to buy a bet on $A$
for up to £0.40 and a bet on $B$ for up to £0.20. Notice that if you
buy these two bets you have in effect bought a bet on $A \lor B$, for
you will get £1 if either $A$ or $B$ is true, and £0 otherwise. So you
are (in effect) willing to sell this bet for £0.50 and buy it back for
£0.60. No matter how the bets turn out, you will therefore lose £0.10
(as you can easily check).

The reasoning generalizes to any other case where $\Cr(A \lor B) <
\Cr(A) + \Cr(B)$. For cases where $\Cr(A \lor B) > \Cr(A) + \Cr(B)$,
simply reverse all occurrences of `buy' and `sell'.

We have thereby proved the \emph{Dutch Book Theorem}.

\begin{genericthm}{Dutch Book Theorem}
  If an agent values bets by their expected monetary payoff and her
  degrees of belief don't conform to the Kolmogorov axioms, then she
  is prepared to accept combinations of bets that amount to a
  guaranteed loss. 
\end{genericthm}

Note that the Dutch Book Theorem is a conditional: \emph{if} an agent
has non-probabilistic beliefs and values bets by their expected
monetary payoff, \emph{then} she is vulnerable to Dutch Books. We have
not shown that agents with probabilistic beliefs are immune to Dutch
Books. But that can also be shown (with some further
restrictions on the relevant agents); the result is known as the
\textbf{Converse Dutch Book Theorem}. We won't go through the proof.

In chapter \ref{ch:probability} I mentioned that some authors treat
the Ratio Formula for conditional probability as a definition while
others treat it fourth axiom of probability. On the second
perspective, we might want to show that violations of that fourth
axiom also make an agent vulnerable to Dutch Books.

To this end, we would first have to extend the betting interpretation,
in order to clarify how conditional credences manifest themselves in
betting behaviour. The standard approach is to introduce the idea of a
\emph{conditional bet}. A unit bet on $A$ conditional on $B$ is a bet
that only comes into effect if $B$ is true. In that case it pays £1 if
$A$ is true and £0 if $A$ is false. If $B$ is not true, whoever bought
the bet gets a refund for the price they paid. Now we can extend the
betting interpretation to say that your conditional credence in $A$
given $B$ is the maximal price at which you would be willing to buy the
corresponding conditional bet. And then it is not hard to show that a
Dutch Book can be made against you unless your conditional credences
satisfy the Ratio Formula.

\cmnt{%
 show?
} %

\begin{exercise}
  Suppose I believe that it is raining to degree 0.6 and that it is
  not raining also to degree 0.6. Describe a Dutch Book you could make
  against me, assuming I value bets in accordance with their expected
  monetary payoff. $\star\star\star$
\end{exercise}

\section{Problems with the betting interpretation}\label{sec:problem-betting}

The Dutch Book Theorem is a mathematical result. It does not show that
rational degrees of belief satisfy the probability axioms. To reach
that conclusion, and thereby an argument for probabilism, we need to
add some philosophical premises about rational belief.

On the face of it, the Dutch Book Theorem seems to warn us that if our
degrees of belief do not satisfy the probability axioms, then a
cunning Dutchman might come along and trick us out of money. But why
does that show that non-probabilistic beliefs are irrational?  Two
problems immediately stand out.

First, why should the mere possibility of financial loss be a sign of
irrational beliefs? True, there might be a Dutchman going around
exploiting people with non-probabilistic beliefs. But there might also
be someone (a Frenchman, say) going around richly rewarding people
with non-probabilistic beliefs. We don't think the latter possibility
shows that people ought to have non-probabilistic beliefs. Even if
there is such a Frenchman, we can at most conclude that it would be
\emph{practically useful} to have non-probabilistic beliefs. Arguably
those beliefs would still not be \emph{epistemically
  rational}. (Compare: if someone offers you a million pounds if you
believe that the moon is made of cheese, then the belief would be
practically useful, but it would not be epistemically justified;
it would not be grounded in reason and evidence.) Why should we think
differently about the hypothetical Dutchman? 

Second, the threat of financial exploitation only awaits
non-probabilistic agents who value bets by their expected monetary
payoff, i.e.\ who are disposed to buy or sell any bet if the expected
monetary payoff of the transaction is positive. This is entailed by
the betting interpretation, but on reflection it is untenable.

Consider the following gamble.
\begin{example}[The St.\ Petersburg Paradox]
  I will toss a fair coin until it lands tails. If the coin lands tails on
  the first toss, you get £2. If it lands heads on the first
  toss and tails on the second, you get £4. If it lands heads on the
  first two tosses and tails on the third, you get £8. And so on: if
  the coin first lands tails on the $n$th toss, you get £$2^n$.
\end{example}
How much would you pay for this gamble? 

We can compute the expected payoff. With probability \nicefrac{1}{2}
you'll get £2; with probability \nicefrac{1}{4} you get £4; with
probability \nicefrac{1}{8} you get £8; and so on. The expected payoff
is therefore
\[
  \nicefrac{1}{2} \times \text{£2} + 
  \nicefrac{1}{4} \times \text{£4} + 
  \nicefrac{1}{8} \times \text{£8} + 
  \ldots = \text{£1} +  \text{£1} +  \text{£1} + \ldots. 
\]
The sum of this series is infinite. That is, if you value bets in
accordance with their monetary payoff, you should sacrifice everything
you have for an opportunity to play the gamble. In reality, few
people would do that, seeing as the payoff is almost certain to be
quite low.

\begin{exercise}
  What is the probability that you will get £16 or less when playing
  the St.\ Petersburg gamble? $\star\star$
\end{exercise}

The St.\ Petersburg Paradox was first described by the Swiss
mathematician Nicolas Bernoulli in 1713 and motivated his cousin
Daniel Bernoulli to introduce the theoretical concept of utility as
distinct from monetary payoff. As (Daniel) Bernoulli realised, ``a
gain of one thousand ducats is more significant to the pauper than to
a rich man though both gain the same amount''. In other words, most
people don't regard having two million pounds as twice as good as
having one million pounds: the first million would make a much greater
difference to our lives than the second.

In the economics terminology, money has \textbf{declining marginal
  utility}. The `marginal utility' of a good for an agent is how much
she desires an extra unit of the good. To say that the marginal
utility of money is declining therefore means that the more money you
have, the less you value an additional pound.

Concretely, Daniel Bernoulli suggested that $n$ units of money provide
not $n$ but $\log(n)$ units of utility, so that doubling your wealth
from £1000 to £2000 would provide the same boost in utility than
doubling your wealth from £1 million to £2 million (even though the
second change is much larger in absolute terms). On Bernoulli's model,
the expected utility of the St.\ Petersburg gamble for a person with a
wealth of £1000 is not infinite, but £10.95: that is the most she
ought to be willing to pay.

The declining marginal utility of money is empirically well
established and makes intuitive sense. It is certainly not a sign of
irrationality. But it means that rational agents for the most part do
not value bets by their monetary payoff. 

\begin{exercise}
  Suppose owning £$n$ gives you a utility of $\log(n)$. You currently
  have £1. For a price of £0.40 you are offered a bet that pays £1 if
  it will rain tomorrow (and £0 otherwise). Your degree of belief in
  rain tomorrow is \nicefrac{1}{2}. Should you accept the bet?  Draw
  the decision matrix and compute the expected utilities. [You'll need
  to know that $\log(1) = 0$, $\log(1.6) \approx 0.47$, and $\log(0.6)
  \approx -0.51$. Besides that you don't need to understand what `$\log$'
  means.] $\star\star$
\end{exercise}

\begin{exercise}
  Bernoulli's logarithmic model is obviously a simplification. Suppose
  you want to take a bus home, but you only have £1.40 whereas the
  fare is £1.60. If you can't take the bus, you'll have to walk for 50
  minutes through the rain. A stranger at the bus stop offers you a
  deal: if you give her your £1.40, she will toss a fair coin and pay
  you back £1.60 on heads or £0 on tails. Explain (briefly and
  informally) why it would be rational for you to accept the deal. $\star$
\end{exercise}

There's another reason why rational agents don't always value bets by
their expected payoff, even if their subjective utility is adequately
measured by monetary payoff. The reason is that buying or selling bets
can alter the relevant beliefs. 

For example, I am quite confident I will not buy any bets
today. Should I therefore be prepared to pay close to £1 for a bet on
the proposition that I don't buy any bets today? Clearly not. By
buying the bet, I would render the proposition false. Given my current
state of belief, the (imaginary) bet has an expected payoff close to
£1; nonetheless, it would be irrational for me to buy it.

So rational agents don't always value bets by their expected
payoff. The betting interpretation is untenable. An agent's betting
dispositions may often give a good hint about their degrees of belief,
but we can't simply read off degrees of belief from dispositions to
buy and sell bets.

\section{A Dutch Book argument}

Given the issues raised in the previous section, can we learn anything
about rational belief from the Dutch Book Theorems? Some philosophers
have argued that we can't. I am a little more optimistic. But clearly
the argument will have to be more complicated than one might initially
have thought. Here is a sketch of one possible approach.

Consider an arbitrary agent with non-probabilistic beliefs. Call her
$\alpha$. Our aim is to show that $\alpha$'s beliefs are
epistemically irrational. 

Arguably, whether $\alpha$'s beliefs are epistemically rational does
not depend on her goals: if we want to know whether it is
epistemically rational (as opposed to practically useful) for an agent
to have such-and-such beliefs, we don't need to know anything about
her goals or desires.  So let's imagine a counterpart $\beta$ of
$\alpha$ who has the same beliefs as $\alpha$ but possibly different
basic desires. For $\beta$, the utility of any bet she might buy or
sell is adequately measured by monetary payoff. We'll also assume that
$\beta$ is practically rational insofar as she obeys the MEU
Principle, and that this alone does not make her beliefs epistemically
irrational. So our first philosophical premise states that if
$\alpha$'s beliefs are epistemically rational, then so are $\beta$'s.

As we saw at the end of the previous section, the assumption that for
$\beta$, the utility of a bet is measured by monetary payoff does not
guarantee that she values bets by their expected monetary payoff in
the sense that she will be prepared to buy or sell bets whenever the
expected monetary payoff of the transaction is positive, since the act
of buying or selling a bet can affect her credence in relevant
propositions. But this problem seems to arise only for a small and
special class of propositions (e.g., the proposition that the agent
won't buy any bets today). Let's call those propositions `unstable'
and the others `stable'. Stable propositions are those $\beta$ values
by their expected monetary payoff.

\cmnt{%
  A complication: what if $\alpha$ already made a high-stakes bet that
  she would not buy any more bets today? Then buying any more bets has
  negative monetary payoff for $\beta$!
} %

\cmnt{%
  Shouldn't I also restrict the betting propositions to those that can
  credibly be verified?%
} %

Now the probability axioms are supposed to be general consistency
requirements on rational belief. Such requirements plausibly hold for
beliefs or every kind, not just for beliefs with a specific
content. In particular, if the probability axioms are requirements for
beliefs in stable propositions then they are also requirements for
beliefs in unstable propositions. (This is our second premise.) To
show that non-probabilistic beliefs are irrational, it is therefore
enough to show that non-probabilistic beliefs towards stable
propositions are irrational. So we can assume without loss of
generality that $\alpha$'s (and therefore $\beta$'s) beliefs towards
stable propositions are non-probabilistic.

It follows by the Dutch Book Theorem that $\beta$ is prepared to
knowingly buy and sell bets in such a way that she is guaranteed to
lose money. Our next premise states that it would be irrational for
$\beta$ to make these transactions. That is, it is irrational for an
agent whose sole aim is to maximize profits to knowingly and avoidably
make transactions that are logically guaranteed to cost her
money. (The plausibility of this premise turns on the Converse Dutch
Book Theorem: on the fact that it is possible to avoid making such
transactions.)

So our hypothetical agent $\beta$ is disposed to make irrational
choices. Let's have a closer look at these choices. Arguably (premise
4), if a choice is irrational, then the choice is either based on
irrational beliefs, or on irrational desires, or it wrongly evaluates the
agent's options on the basis of her beliefs and desires. In the case
of $\beta$, we can rule out the third possibility, if we assume the
MEU Principle (premise 5), for by hypothesis $\beta$'s choices are in
line with that principle. As I said above, $\beta$ is practically
rational. The fault lies either in her beliefs or in her desires.

Now $\beta$'s desires are certainly peculiar. Since $\beta$ values
bets by their expected monetary payoff, she would be prepared to give
all she has for an opportunity to play the St.\ Petersburg gamble. But
from a thoroughly subjective point of view, there is nothing
incoherent or inconsistent about these desires. (Premise 6.) So the
irrationality lies in $\beta$'s belief. Intuitively, $\beta$ misjudges the
profitability of the relevant bets.

So $\beta$  is epistemically irrational. By our very first premise, it
follows that $\alpha$ is epistemically irrational.

The argument has a lot of premises, and many of them could be
challenged. Can you think of a better argument?



\section{Comparative credence}\label{sec:comparative-credence}

Many philosophers reject the idea of defining credences in terms of an
agent's behaviour, or in terms of anything else. Instead, they hold
that the concept of credence should be treated as basic and
unanalysable. Even on that view, however, more must be said about the
numerical representation of credence. That we represent degrees of
belief by numbers between 0 and 1 is clearly a matter of convention:
whatever is represented by these numbers could just as well be
represented by numbers between 0 and 100, by the rotation of a line,
or in various other ways. So we need to explain the convention of
assigning numbers to whatever an agent's credence function represents.

One approach towards such an explanation was outlined by the Italian
mathematician and philosopher Bruno de Finetti in the 1930s. De
Finetti suggested that degrees of belief could be defined in terms of
the comparative attitude of being more confident in one proposition
than in another. While any numerical representation of beliefs is
partly conventional, this comparative attitude can sensibly be taken
as basic. 

I will write `$A \succ B$' to express that the agent in question is
more confident in $A$ than in $B$. For example, if you are more
confident that it is sunny than that it is raining, then $\emph{Sunny}
\succ \emph{Rainy}$. I'll write `$A \sim B$' if the agent is equally
confident in $A$ and in $B$. From these, we can define a third
relation `$\succsim$' by stipulating that $A \succsim B
\Leftrightarrow (A \succ B) \lor (A \sim B)$.

Let's collect a few assumptions about these relations. First of all,
if you're more confident in $A$ than $B$, then you can't at the same
time be more confident in $B$ than $A$ or equally confident in the two
propositions. Moreover, if you're neither more confident in $A$ than
$B$, nor in $B$ than $A$, then you're plausibly equally confident in
the two. In general, we may assume that an agent's comparative credence
relations are ``complete'' in the following sense:

\begin{genericthm}{Completeness}
  For any $A$ and $B$, exactly one of $A \succ B$, $B\succ A$, or $A
  \sim B$ is the case.
\end{genericthm}

Next, suppose you are at least as confident in $A$ than in $B$, and at
least as confident in $B$ than in $C$. Then you should arguably be at
least as confident in $A$ then in $C$. In other words, $\succsim$ should
be ``transitive'':
\begin{genericthm}{Transitivity}
  If $A \succsim B$ and $B \succsim C$ then $A \succsim C$.
\end{genericthm}

\cmnt{%
  A \textbf{strict partial order} is a relation $\succ$ that is
  asymmetric and transitive. Asymmetry is entailed by Completeness.
  
  A \textbf{non-strict partial order} is a relation $\succsim$ that is
  reflexive, weakly asymmetric ($A \succsim B \land B \succsim A
  \Rightarrow A = B$) and transitive. We don't have weak asymmetry.

  A \textbf{preorder} is a reflexive and transitive relation
  $\succsim$. We do have that.

  A \textbf{strict weak order} is a relation $\succ$ that is
  asymmetric, transitive, and negatively transitive: if $A \not\succ
  B$ and $B \not\succ C$ then $A \not\succ C$. (By contraposition, if
  $A \succ C$ then either $A \succ B$ or $B \succ C$.)

  Completeness and transitivity for $\succsim$ (not just for $\succ$)
  entail negative transitivity for $\succ$, so $\succ$ is a strict
  weak order: Suppose for reductio that (1) $A \succ C$, (2)
  $A\not\succ B$, and (3) $B\not\succ C$. By completeness from (2),
  either (4) $B \succ A$ or (5) $A \sim B$. By transitivity, (4) and
  (1) entail $B \succ C$, contradicting (3). So (4) is impossible,
  leaving (5). By completeness from (3), either (6) $C \succ B$ or (7)
  $B \sim C$. By transitivity, (1) and (6) entail $A \succ B$,
  contradicting (2). So (6) is impossible, leaving (7). Now we have
  (1) $A \succ C$ and (5) $A \sim B$ and (7) $B \sim C$, but I don't
  think that's a contradiction if we just have transitivity for
  $\succ$. It is however a contradiction if we have transitivity for
  $\succsim$.

  Negative transitivity and asymmetry for $\succ$ obviously don't
  entail completeness, unless we say more about $\sim$. But if we
  define $A \sim B$ as $A \not\succ B \land B \not\succ A$, then
  asymmetry for $\succ$ alone entails completeness. To begin, the
  definition of $\sim$ ensures that $A \sim B$ is symmetrical. Given
  symmetry, completeness amounts to the following claims: (1) $A \succ
  B \Rightarrow B \not\succ A \land A \not\sim B$; (2) $A \sim B
  \Rightarrow A \not\succ B$; (3) $A \not\succ B \land B \not\succ A
  \Rightarrow A \sim B$; (4) $A \not\succ B \land A \not\sim B
  \Rightarrow B \succ A$. The first part of (1) follows from asymmetry
  of $\succ$, the second from the definition of $\sim$. (2)--(4) all
  follow from the definition of $\sim$. 

  So if one defines $\sim$ in terms of $\succ$, and stipulates that
  $\succ$ is a weak order, one gets the same formal properties we
  have assumed. 

  (Do we have symmetry of $\sim$? Yes: Assume $A \sim B$ but $B
  \not\sim A$. By completeness, then $B \succ A$ or $A \succ B$. But
  if $A \sim B$ then neither $A \succ B$ nor $B \succ A$ by
  completeness.)

  A \textbf{non-strict weak order} = \textbf{total pre-order} is a
  pre-order that is total (aka complete), meaning that either $A
  \succsim B$ or $B \succsim A$. That is obviously entailed by
  completeness and transitivity for $\succsim$.

} %

\begin{exercise}
  Show that Transitivity and Completeness entail that (a) if $A \sim
  B$ then $B \sim A$, and (b) if $A \sim B$ and $B \sim C$, then $A
  \sim C$. $\star\star\star$
\end{exercise}

\cmnt{%

Figuratively speaking, imagine we wanted to order all propositions on
a line in such a way that a proposition $A$ is to the right of a
proposition $B$ just in case $A \succ B$. (Several propositions can be
at the same point on the line: we arrange them on top of each other.)
It can be shown that this is possible if and only if $\succ$ is a weak
order. (Assuming we have at most countably many propositions to order;
otherwise a further condition is needed.)

If we have things arranged on a line, we can represent their relative
position by a number.

} %

For the next assumptions, I use `$\top$' to stand for the logically
necessary proposition and `$\bot$' for the logically impossible
proposition.
%
\begin{genericthm}{Normalization}
  $\top \succ \bot$.
\end{genericthm}
\vspace{-2mm}
\begin{genericthm}{Boundedness}
  There is no proposition $A$ such that $\bot \succ A$.
\end{genericthm}
These are fairly plausible as demands of rationality. 

The next assumption is best illustrated by an example. Suppose you are
more confident that Bob is German than that he is French.  Then you
should also be more confident that Bob is \emph{either German or
  Russian} than that he is \emph{either French or
  Russian}. Conversely, if you are more confident that he is German or
Russian than that he is French or Russian, then you should be more
confident that he is German than that he is French. In general:

\begin{genericthm}{Quasi-Additivity}
  If $A$ and $B$ are both logically incompatible with $C$, then $A
  \succsim B$ iff $(A \lor C) \succsim (B \lor C)$.
\end{genericthm}

De Finetti conjectured that whenever an agent's comparative credence
relations satisfy the above five assumptions, then there is a unique
probability measure $\Cr$ such that $A \succ B$ iff $\Cr(A) > \Cr(B)$
and $A \sim B$ iff $\Cr(A) = \Cr(B)$. The conjecture turned out to be
false, because a sixth assumption is required. But the following can
be shown:
%
\begin{genericthm}{Probability Representation Theorem}
  If an agent's comparative credence relations satisfy Completeness,
  Transitivity, Normalization, Boundedness, Quasi-Additivity, and the
  Sixth Assumption, then there is a unique probability measure $\Cr$
  such that $A \succsim B$ iff $\Cr(A) \geq \Cr(B)$.
\end{genericthm}
%
Before I describe the Sixth Assumption, let me explain what the
Probability Representation Theorem might do for us.

I have argued that we can't take numerical credences as unanalysed
primitives. There must be an answer to the question why an agent's
degree of belief in rain is correctly represented by the number 0.2
rather than, say, 0.3. De Finetti's idea was to derive numerical
representations of belief from comparative attitudes towards
propositions.

Imagine we order all propositions on a line, in accordance with the
agent's comparative judgements: whenever the agent is more confident
in $A$ then in $B$, $A$ goes to the right of $B$. The impossible
proposition $\bot$ will then be at the left end, the necessary
proposition $\top$ at the right end. If the agent is equally confident
in two propositions, they are stacked on top of each other at the same
point on the line. 

Now imagine we hold a ruler under this line in such a way that $\bot$
lies at 0 and $\top$ at 1. Every other proposition will then have a
number between 0 and 1, given by its position along the line. If
that's how we understand degrees of belief, to say that an agent's
degree of belief in rain is 0.2 is to identify the relative position
of rain in an agent's confidence ordering.

The Probability Representation Theorem tells us that if the confidence
ordering satisfies certain conditions, then there will be exactly one
way of assigning numbers to the propositions that respects the
probability axioms: there is a unique probability measure $\Cr$ that
\emph{represents} the confidence ordering, meaning that $\Cr(A) >
\Cr(B)$ whenever $A \succ B$, and $\Cr(A) = \Cr(B)$ whenever $A \sim
B$. Assuming that an agent's degrees of belief satisfy the probability
axioms therefore amounts to choosing a particular kind of ruler for
measuring degrees of belief.

On this approach, any agent whose attitudes of comparative credence
satisfy the six assumptions is guaranteed to have probabilistic
credences, because the agent's credence function is \emph{defined} as
the unique probability measure $\Cr$ that represents her confidence
ordering.

As you may imagine, this approach has also not gone unchallenged. One
obvious question is whether we can take comparative confidence as
primitive. If we can, a further question is whether the six
assumptions are plausible as general norms of
rationality. Transitivity, Normalization, and Boundedness look fairly
safe, but the others have been questioned. 

The missing sixth assumption is especially troublesome in this
regard. As it turns out, the form of that assumption depends on
whether the number of propositions ranked by $\succ$ is finite or
infinite. In either case the condition is complicated -- which makes
it especially hard to treat it as a basic norm of rationality. Just to
prove the point, here is the condition for the slightly simpler case
of finitely many propositions:

\begin{genericthm}{The Sixth Assumption (finite version)}
  For any two sequences of propositions $A_1,\ldots,A_n$ and
  $B_1,\ldots,B_n$ such that for every possible world $w$, the number of
  propositions in the first sequence that contain $w$ equals the
  number of propositions in the second sequence that contain $w$, if
  $B_i \not\succ A_i$ for all $i < n$, then $A_n \not\succ B_n$.
\end{genericthm}



\section{Further reading}

A thorough critique of Dutch Book arguments can be found in
%
\begin{itemize}
\item Alan H\'ajek: \href{http://philrsss.anu.edu.au/people-defaults/alanh/papers/DBA.pdf}{``Dutch Book Arguments''} (2008).
\end{itemize}
%
For even more details and background information, have a look at the
Stanford Encyclopedia entry
%
\begin{itemize}
\item Susan Vineberg: \href{https://plato.stanford.edu/entries/dutch-book/}{``Dutch Book Arguments''} (2016).
\end{itemize}

If you're interested in the approach based on comparative credence,
a good (though mathematically non-trivial) introduction is
%
\begin{itemize}
\item Peter Fishburn: \href{https://projecteuclid.org/download/pdf_1/euclid.ss/1177013611}{``The Axioms of Subjective Probability''} (1986).
\end{itemize}

\begin{essay}
  Do you think the Dutch Book Theorems can teach us anything about
  epistemic rationality? If so, can you spell out how? If not, can you
  explain why not?
\end{essay}

\cmnt{%
  Using MEU in the definition may seem odd, but remember we're not
  interested in the ordinary sense of ``belief'' and ``desire'', nor
  in what people say about their beliefs and desires. We're defining a
  pragmatic notion of belief and desire. On that usage, it makes no
  sense to suggest that somebody whose choices reveal certain values
  and beliefs really has quite different values and beliefs that don't
  match those choices.

  To illustrate, imagine we find an alien creature on an island that
  displays sophisticated, intelligent behaviour. Suppose we have
  repeatedly observed it sneaking chocolate out of handbags and hiding
  it on a tree. A sensible explanation would be that it likes to eat
  chocolate and thinks it is safe on the tree. But suppose I suggest
  that in fact, this creature doesn't like chocolate at all. We would
  then ask for another explanation of the behaviour: does it think it
  can use it to attract prey on the tree? Or does it like to tease
  humans?  Suppose I say that all of this is false: from the
  creature's point of view, there is no benefit whatsoever in taking
  the chocolate; it takes it \emph{despite the fact} that it realises
  this to be no good. This is an unintelligible suggestion.

  A minor complication is that the theory that introduces theoretical
  terms can be false. Ramsey is well aware of this. It's like defining
  distance by presupposing Newtonian mechanics. In fact, nothing quite
  plays the role of distance, and the notion is indeterminate between
  several candidates.
} %


\cmnt{%

Lecture:

\begin{itemize}
\item Emphasize again that even if we call credences
  ``probabilities'', it is not trivial that they are probabilities,
  and indeed what the numbers are supposed to stand for: not for
  intensity of feeling, nor for a special kind of judgement. (Compare
  ungraded concept of belief, which is often understood in terms of
  assertions.)
\item A good idea is to define credence in terms of effect on
  behaviour; problem: entanglement with desires. 
\item Betting interpretation
\item Go through case (iii) of DBT in detail.
\item First problems with betting interpretation: utility
  curves. Clarify how that affects the argument: Bernoulli agents
  still don't want sure losses. Point is, they wouldn't buy/sell the
  relevant bets.
\item On representation theorems: what it means in general that < is
  represented by a number f; example; conditions that are required:
  weak order; representation is never unique, but here the result is
  that there's a unique probabilistic representation.
\end{itemize}



} %

 	
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
