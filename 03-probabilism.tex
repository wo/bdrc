\chapter{Probabilism}\label{ch:probabilism}

% re-read refereed/Erkenntnis 2019 Comparativism about credence for literature
% and presentation.

\section{Justifying the probability axioms}

The hypothesis that rational degrees of belief satisfy the
mathematical conditions on a probability measure is known as
\textbf{probabilism}. In this chapter, we will look at some arguments
for probabilism. We do so not because the hypothesis is especially
controversial (by philosophy standards, it is not), but because it is
instructive to reflect on how one could argue for an assumption like
this, and also because the task will bring us back to a more
fundamental question: what it means to say that an agent has
such-and-such degrees of belief in the first place.

We will assume without argument that if a rational agent has degrees
of belief in some propositions $A$ and $B$, then she also has degrees
of belief in their conjunction, disjunction, and negation. Probabilism
then reduces to the hypothesis that rational degrees of belief satisfy
the probability axioms -- specifically, Kolmogorov's axioms (i)--(iii):
\begin{itemize}
\itemsep0em 
\item[(i)] For any proposition $A$, $0 \leq \Cr(A) \leq 1$.
\item[(ii)] If $A$ is logically necessary, then $\Cr(A) = 1$.
\item[(iii)] If $A$ and $B$ are logically incompatible, then $\Cr(A \lor B) = \Cr(A) + \Cr(B)$.
\end{itemize}

Consider axiom (i). Why should rational degrees of belief always fall
in the range between 0 and 1? Why would it be irrational to believe
some proposition to degree 7? The question is hard to answer unless we
have some idea of what it would mean to believe a proposition to
degree 7.

It is tempting to think that axiom (i) does not express a substantive
norm of rationality, but a convention of representation. We have
decided to represent strength of belief by numbers between 0 and 1,
where 1 means absolute certainty. We could just as well have decided
to use numbers between 0 and 100, or between -100 and +100. Having set
up the convention to put the upper limit at 1, it doesn't make any
sense to assume that an agent believes something to degree 7.

Axioms (ii) and (iii) look more substantive. It seems that we can at
least imagine an agent who assigns degree of belief less than 1 to a
logically necessary proposition or whose credence in a disjunction of
incompatible propositions is not the sum of her credence in the
individual disjuncts. Still, we need to clarify what exactly it is
that we're imagining if we want to discuss whether the imagined states
are rational or irrational.

For example, suppose we understand strength of belief as a certain
introspectible quantity: a basic feeling of conviction people have
when entertaining propositions. Axiom (ii) would then say that when
agents entertain logically necessary propositions, they ought to
experience this sensation with maximal intensity. It is hard to see
why this should be norm of rationality. It is also hard to see why
the sensation should guide an agent's choices in line with the MEU
Principle, or why it should be sensitive to the agent's evidence.

So if we understand degrees of belief as measuring the intensity of a
certain feeling, then the norms of Bayesian decision theory and Bayesian
epistemology look implausible and inexplicable. The same is true if we
understand degrees of belief as measuring some other basic
psychological quantity: why should that quantity satisfy the
probability axioms, guide behaviour, respond to evidence, etc.?

A more promising line of thought assumes that strength of belief is
defined (perhaps in part) by the MEU Principle. On that approach, what
we mean when we say that an agent has such-and-such degrees of belief
is that she is (or ought to be) disposed to make certain choices. We
can then assess the rationality of the agent's beliefs by looking at
the corresponding choice dispositions.

Unfortunately, beliefs alone do not settle rational choices: the
agent's desires or goals also play a role. The argument we are now
going to look at therefore fixes an agent's goals by assuming that
utility equals monetary payoff. Afterwards we will consider how that
assumption could be relaxed.

%In chapter
%\ref{ch:preference}, we will encounter a more sophisticated relative
%argument that does not fix the utilities.

\section{The betting interpretation}

It is instructive to compare degrees of belief with other numerical
quantities in science. Take mass. What do we mean when we say
that an object -- a chunk of iron perhaps -- has a mass of 2 kg? There
are no little numbers written in chunks of iron, just as there are no
little numbers written in the head. As with degrees of belief, there
is an element of conventionality in the way we represent masses by
numbers: instead of representing the chunk's mass by the number 2, we
could just as well have used a different scale on which the mass would
be 2000 or 4.40925. (Appending `kg' to the number, as opposed to `g'
or `lb', hints at the conventional scale.)

I am not suggesting that mass itself is conventional. Whether a chunk
of iron has a mass of 2 kg is, I believe, a completely objective, mind-independent
matter. If there were no humans, the chunk would still have that
mass. What's conventional is only the representation of masses (which
are not intrinsically numerical) by numbers.

The reason why we can measure mass in numbers -- and the reason why we
know anything at all about mass -- is that things tend to behave
differently depending on their mass. The greater an object's mass, the
harder the object is to lift up or accelerate. Numerical measures of
mass reflect these dispositions, and can be standardized by reference
to particular manifestations. For example, if we put two objects on
opposite ends of a balance, the one with greater mass will go down. So
we could choose a random chunk of iron, call it the ``standard
kilogram'', and stipulate that something has a mass of $n$ kg just in
case it balances against $n$ copies of the standard kilogram (or
against $n$ objects each of which balances against the standard
kilogram). 

Can we take a similar approach to degrees of belief? The idea would be
to find a characteristic way in which degrees of belief manifest
themselves in behaviour and use that to define a numerical scale for
degrees of belief.

So how do you measure someone's degrees of belief? The classical
answer is: by offering them a bet. 

Consider a bet that pays £1 if it will rain at noon tomorrow, and
nothing if it won't rain. How much would you be willing to pay for
this bet?

We can calculate the expected payoff -- that is, the average of the
possible payoffs, weighted by their subjective probability.  Suppose
your degree of belief in rain tomorrow is $x$, and your degree of
belief in not-rain is $1-x$. Then the bet would give you £1 with
probability $x$ and £0 with probability $1-x$. So the expected payoff
is $x \cdot \text{£1} + (1-x) \cdot \text{£0} = \text{£}x$. This
suggests that the bet is worth £$x$. That is, £$x$ is the most you
should pay for the bet.

\begin{exercise1}
  Suppose your degree of belief in rain is $0.8$ (and your degree of
  belief in not-rain 0.2). For a price of £0.70 you can buy a bet that
  pays £1 if it rains and £0 if it doesn't rain. Draw a decision
  matrix for your decision problem and compute the expected utility of
  the acts, assuming your subjective utilities equal the net amount of
  money you have gained in the end.
\end{exercise1}

If we're looking for a way to measure your degrees of belief, we can
turn this line of reasoning around: if £$x$ is the most you're willing
to pay for the bet, then $x$ is your degree of belief in the
proposition that it will rain. This leads to the
following suggestion.

\begin{genericthm}{The betting interpretation}
  An agent believes a proposition $A$ to degree $x$ just in case she
  would pay up to £$x$ for a bet that pays £1 if $A$ is true and £0
  otherwise.
\end{genericthm}

The betting interpretation is meant to have the same status as the
above (hypothetical) stipulation that an object has a mass of $n$ kg
just in case it balances against $n$ copies of the standard kilogram.
On the betting interpretation, offering people bets is like putting
objects on a balance scale. For some prices, the test person will
prefer to buy the bet, for others she will be prefer to sell the bet;
in between there is a point at which the price of the bet is in
balance with the expected payoff, so the test person will be
indifferent between buying, selling, and doing neither. The price at
the point of balance reveals the subject's degree of belief. The stake
of £1 is a unit of measurement, much like the standard kilogram in the
measurement of mass.

\cmnt{%

 xxx This exercise was too imprecise. Improve!!

\begin{exercise}
  Show that if an agent's degree of belief in $A$ is $x$, then the
  a bet that pays £100 if $A$ and £0 if $\neg A$ has expected
  payoff £$100 \cdot x$. (Thus if we had used £100 instead of £1 in
  the betting interpretation, then degrees of belief would range
  from 0 to 100 rather than from 0 to 1.) 
\end{exercise}

} %

The betting interpretation gives us a clear grip on what it means to
believe a proposition to a particular degree. It also points towards
an argument for probabilism. For we can show that if an agent's
degrees of belief do not satisfy the probability axioms (for short, if
her beliefs are not \textbf{probabilistic}), then the agent is
disposed to enter bets that amount to a guaranteed loss.

\section{The Dutch Book theorem}

In what follows, we are going to assume a slightly strengthened form of the
betting interpretation: we will assume that if an agent is not willing to buy a
bet for £$x$, then she would be be willing to sell the bet for that
price. Selling a bet means offering it to somebody else. The idea is that if you
judge a bet to worth less than £$x$, then you should be happy to offer someone
the bet for a price of £$x$.

\begin{exercise1}
  Suppose your degree of belief in rain is $0.8$ (and in not-rain 0.2)
  and someone offers you £0.90 for a bet that pays £1 if it rains and
  £0 if it doesn't rain. Should you sell (i.e.\ offer) them the bet?
  Draw a decision matrix for your decision problem and compute the
  expected utility of the acts, assuming your subjective utilities
  equal the net amount of money you get in the end.
\end{exercise1}

In betting jargon, a combination of bets (bought or sold) is called a
`book'. A combination of bets that amounts to a guaranteed loss is a
\textbf{Dutch book}. We will now prove that if an agent's degrees of
belief violate one or more of the Kolmogorov axioms, and she values
bets in accordance with their expected payoff, then she is prepared to
accept a Dutch Book.

We begin with axiom (i). Suppose your credence in some proposition $A$
is greater than 1. For concreteness, let's say $\Cr(A)=2$. By the
betting interpretation, this means you'd be willing to pay up to £2
for a deal that pays you back either £0 or £1, depending on whether
$A$ is true. You're guaranteed to lose at least £1. More generally, if
your degree of belief in $A$ is greater than 1, then you are
guaranteed to lose at least the difference between your degree of
belief and 1.

Similarly, suppose your credence in $A$ is below 0. Let's say it's
-1. By the betting interpretation, this means you would pay no more
than £-1 for a bet on $A$ and you'd be willing to sell the bet for any
price above £-1. What does it mean to sell a bet for £-1?  It means to
pay someone £1 to take the bet. So you would be willing to pay up to
£1 for me to take the bet from you, with no chance of getting any
money back. You're guaranteed to lose at least £1. Again, the argument
generalizes to any degree of belief below 0.

I leave the case of axiom (ii) as an exercise.

\begin{exercise2}
  Suppose your degrees of belief violate axiom (ii). Assuming the
  betting interpretation, describe a bet you are willing to sell for a
  price less than what you could possibly get back.
\end{exercise2}

Now for axiom (iii). Suppose your credence in the disjunction of two
logically incompatible propositions $A$ and $B$ is not the sum of your
credence in the individual propositions. For concreteness, let's
assume $\Cr(A) = 0.4$, $\Cr(B) = 0.2$, and $\Cr(A \lor B) = 0.5$. By
the betting interpretation, you'll then be willing to sell a bet on $A
\lor B$ for at least £0.50, and you'll be willing to buy a bet on $A$
for up to £0.40 and a bet on $B$ for up to £0.20. If you
buy these two bets you have in effect bought a bet on $A \lor B$, for
you will get £1 if either $A$ or $B$ is true, and £0 otherwise. So you
are (in effect) willing to sell this bet for £0.50 and buy it back for
£0.60. No matter how the bets turn out, you will lose £0.10
(as you can check).

The reasoning generalizes to any other case where
$\Cr(A \lor B) < \Cr(A) + \Cr(B)$. For cases where
$\Cr(A \lor B) > \Cr(A) + \Cr(B)$, simply swap all occurrences of
`buy' and `sell' in the previous paragraph.

We have thereby proved the \emph{Dutch Book Theorem}.

\begin{genericthm}{Dutch Book Theorem}
  If an agent values bets by their expected monetary payoff and her
  degrees of belief don't conform to the Kolmogorov axioms, then she
  is prepared to accept combinations of bets that amount to a
  guaranteed loss. 
\end{genericthm}

Note that the Dutch Book Theorem is a conditional: \emph{if} an agent
has non-probabilistic beliefs and values bets by their expected
monetary payoff, \emph{then} she is vulnerable to Dutch Books. We have
not shown that agents with probabilistic beliefs are immune to Dutch
Books. But that can also be shown (with some further
restrictions on the relevant agents); the result is known as the
\textbf{Converse Dutch Book Theorem}. I won't go through the proof.

In chapter \ref{ch:probability}, I mentioned that some authors treat
the Ratio Formula for conditional probability as a definition while
others treat it fourth axiom of probability. On the second
perspective, we might want to show that violations of that fourth
axiom also make an agent vulnerable to Dutch Books.

To this end, we would first have to extend the betting interpretation,
in order to clarify how conditional credences manifest themselves in
betting behaviour. The standard approach is to introduce the idea of a
conditional bet. A \emph{unit bet on $A$ conditional on $B$} is a bet
that only comes into effect if $B$ is true. In that case it pays £1 if
$A$ is true and £0 if $A$ is false. If $B$ is not true, whoever bought
the bet gets a refund for the price they paid. Now we can extend the
betting interpretation to say that your conditional credence in $A$
given $B$ is the maximal price at which you would be willing to buy the
corresponding conditional bet. And then it is not hard to show that a
Dutch Book can be made against you unless your conditional credences
satisfy the Ratio Formula.

\cmnt{%
 show?
} %

\begin{exercise2}
  Suppose I believe that it is raining to degree 0.6 and that it is
  not raining also to degree 0.6. Describe a Dutch Book you could make
  against me, assuming the betting interpretation.
\end{exercise2}

\section{Problems with the betting interpretation}\label{sec:problem-betting}

The Dutch Book Theorem is a mathematical result. It does not show that
rational degrees of belief satisfy the probability axioms. To reach
that conclusion, and thereby an argument for probabilism, we need to
add some philosophical premises about rational belief.

On a flat-footed interpretation, we might take the theorem as a
warning that if our degrees of belief do not satisfy the probability
axioms, then a cunning Dutchman might come along and trick us out of
money. But does this really show that non-probabilistic beliefs are
irrational? Two problems immediately stand out.

First, why should the mere possibility of financial loss be a sign of
irrational beliefs? True, there might be a Dutchman going around
exploiting people with non-probabilistic beliefs. But there might also
be someone (a Frenchman, say) going around richly rewarding people
with non-probabilistic beliefs. We don't think the latter possibility
shows that people ought to have non-probabilistic beliefs. Even if
there is such a Frenchman, we can at most conclude that it would be
\emph{practically useful} to have non-probabilistic beliefs. Arguably
those beliefs would still not be \emph{epistemically
  rational}. (Compare: if someone offers you a million pounds if you
believe that the moon is made of cheese, then the belief would be
practically useful, but it would not be epistemically justified;
it would not reflect your evidence.) Why should we think
differently about the hypothetical Dutchman? 

Second, the threat of financial exploitation only awaits non-probabilistic
agents who value bets by their expected monetary payoff and so are willing buy
or sell any bet if the expected monetary payoff of the transaction is
positive. This kind of disposition is entailed by the betting interpretation,
but on reflection it is untenable.

Consider the following gamble.
\begin{example}[The St.\ Petersburg Paradox]
  I will toss a fair coin until it lands tails. If the coin lands tails on
  the first toss, you get £2. If it lands heads on the first
  toss and tails on the second, you get £4. If the coin lands heads on the
  first two tosses and tails on the third, you get £8. And so on: if
  the coin first lands tails on the $n$th toss, you get £$2^n$.
\end{example}
How much would you pay for this gamble? 

We can compute the expected payoff. With probability \nicefrac{1}{2}
you'll get £2; with probability \nicefrac{1}{4} you get £4; with
probability \nicefrac{1}{8} you get £8; and so on. The expected payoff
is therefore
\[
  \nicefrac{1}{2} \cdot \text{£2} + 
  \nicefrac{1}{4} \cdot \text{£4} + 
  \nicefrac{1}{8} \cdot \text{£8} + 
  \ldots = \text{£1} +  \text{£1} +  \text{£1} + \ldots. 
\]
The sum of this series is infinite. That is, if you value bets by
their expected monetary payoff, you should sacrifice everything you
have for an opportunity to play the gamble. In reality, few people
would do that, seeing as the payoff is almost certain to be quite low.

\begin{exercise1}
  What is the probability that you will get £16 or less when playing
  the St.\ Petersburg gamble?
\end{exercise1}

When the St.\ Petersburg Paradox was first described by the Swiss
mathematician Nicolas Bernoulli (in 1713), it motivated his cousin
Daniel Bernoulli to introduce the theoretical concept of utility as
distinct from monetary payoff. As (Daniel) Bernoulli realised, ``a
gain of one thousand ducats is more significant to the pauper than to
a rich man though both gain the same amount''. In other words, most
people don't regard having two million pounds as twice as good as
having one million pounds: the first million would make a much greater
difference to our lives than the second.

In economics terminology, what Bernoulli realised is that money has
\textbf{declining marginal utility}. The `marginal utility' of a good
for an agent is how much she desires an extra unit of the good. To say
that the marginal utility of money is declining therefore means that
the more money you have, the less you value an additional ducat or pound.

Concretely, Daniel Bernoulli suggested that $n$ units of money provide
not $n$ but $\log(n)$ units of utility, so that doubling your wealth
from £1000 to £2000 would provide the same boost in utility than
doubling your wealth from £1 million to £2 million (even though the
second change is much larger in absolute terms). On Bernoulli's model,
the expected utility of the St.\ Petersburg gamble for a person with a
wealth of £1000 is not infinite, but £10.95: that is the most she
ought to be willing to pay.

There is clearly nothing irrational about an agent who assigns
declining marginal utility to money. But then we can't assume that
rational agents value bets by their expected monetary payoff.

\begin{exercise2}
  Suppose owning £$n$ gives you a utility of $\log(n)$. You currently
  have £1. For a price of £0.40 you are offered a bet that pays £1 if
  it will rain tomorrow (and £0 otherwise). Your degree of belief in
  rain tomorrow is \nicefrac{1}{2}. Should you accept the bet?  Draw
  the decision matrix and compute the expected utilities. [You'll need
  to know that $\log(1) = 0$, $\log(1.6) \approx 0.47$, and $\log(0.6)
  \approx -0.51$. Apart from that you don't need to know what `$\log$'
  means.] 
\end{exercise2}

\begin{exercise1}
  Bernoulli's logarithmic model is obviously a simplification. Suppose
  you want to take a bus home, but you only have £1.50 whereas the
  fare is £1.70. If you can't take the bus, you'll have to walk for 50
  minutes through the rain. A stranger at the bus stop offers you a
  deal: if you give her your £1.50, she will toss a fair coin and pay
  you back £1.70 on heads or £0 on tails. Explain (briefly and
  informally) why it would be rational for you to accept the deal.
\end{exercise1}

There's another reason why rational agents don't always value bets by
their expected payoff, even if their subjective utility is adequately
measured by monetary payoff. The reason is that buying or selling bets
can alter the relevant beliefs. 

For example, I am quite confident I will not buy any bets
today. Should I therefore be prepared to pay close to £1 for a bet on
the proposition that I don't buy any bets today? Clearly not. By
buying the bet, I would render the proposition false. Given my current
state of belief, the (imaginary) bet has an expected payoff close to
£1; nonetheless, it would be irrational for me to buy it even for £0.10.

So rational agents don't always value bets by their expected
payoff. The betting interpretation is untenable. An agent's betting
dispositions may often give a good hint about their degrees of belief,
but we can't simply read off degrees of belief from dispositions to
buy and sell bets.

\section{A Dutch Book argument}

% read http://m-phi.blogspot.com/2018/08/a-new-sort-of-dutch-book-argument.html

Given the issues raised in the previous section, can we learn anything
about rational belief from the Dutch Book Theorems? Some philosophers
have argued that we can't. I am a little more optimistic. But clearly
the argument will have to be more complicated than one might initially
have thought. Here is a sketch of one possible approach.

Consider an arbitrary agent with non-probabilistic beliefs. Call her
$\alpha$. We want to show that $\alpha$'s beliefs are epistemically
irrational. We can't assume that $\alpha$ values bets by their
expected monetary payoff. Perhaps $\alpha$ hates betting, or doesn't
care about money. But these desires arguably don't affect the
epistemic rationality of $\alpha$'s beliefs. 

So let's imagine a counterpart $\beta$ of $\alpha$ who is in the same
epistemic state as $\alpha$ but doesn't hate betting; $\beta$, I
hereby stipulate, values any bet she might be offered by the bet's
expected monetary payoff. I also stipulate that $\beta$ follows the
MEU Principle.

My first philosophical premise is that \emph{if $\alpha$'s belief state is
  epistemically rational, then so is $\beta$'s}. The idea is that if you want to
know if someone's beliefs are epistemically rational, then you need to know what
her beliefs are and maybe how she acquired those beliefs, but you don't need to
know what this person desires or how she chooses between available acts.
If there's a difference between $\alpha$ and $\beta$, then by assumption it lies
in these irrelevant aspects.

As we saw at the end of the previous section, we can't assume that if
$\beta$'s credence in a proposition is $x$, then she will pay up to
£$x$ for a bet that pays £$1$ if $A$ and £0 if not-$A$, since her
credence in $A$ may be affected by the transaction. But this problem
only seems to arise for a small and special class of propositions.
Let's call a proposition \emph{stable} if it is probabilistically
independent, in $\beta$'s credence function, of the assumption that a
bet on the proposition is bought or sold. Let's call a pair of
propositions $(A,B$) \emph{jointly stable} if both $A$ and $B$ are
stable and $\beta$'s credence in $B$ is unaffected by assumptions
about whether a bet on $A$ has been bought or sold.

Now the probability axioms are supposed to be general consistency
requirements on rational belief. Such requirements should plausibly be
``topic-neutral'': they should hold for beliefs or every kind, not
just for beliefs about a special subject matter. So if the probability
axioms are rational requirements for an agent's credences over stable
propositions, then they should be requirements for an agent's
entire credence function. This is my second premise: \emph{if any
  restriction of an agent's credence function to stable propositions
  should satisfy Kolmogorov's (i), (ii), and (iii), then so should the
  agent's entire credence function}.

To show that non-probabilistic beliefs are irrational, it is therefore
enough to show that non-probabilistic beliefs towards (jointly) stable
propositions are irrational. So we can assume without loss of
generality that $\alpha$'s (and therefore $\beta$'s) beliefs towards
stable propositions are non-probabilistic.

It follows by the Dutch Book Theorem that $\beta$ is prepared to
knowingly buy and sell bets in such a way that she is guaranteed to
lose money. My next premise states that it would be irrational for
$\beta$ to make these transactions. That is, \emph{it is irrational
  for an agent whose sole aim is to maximize her profit in each
  transaction to knowingly and avoidably make transactions whose net
  effect is a guaranteed loss}. This premise relies on the Converse
Dutch Book Theorem, which shows that it is possible to avoid making a
sure loss.

So our hypothetical agent $\beta$ is disposed to make irrational
choices. Arguably (premise 4), \emph{if an agent makes irrational
  choices, then she is epistemically irrational, or her desires are
  irrational, or her acts don't maximize expected utility}. In the
case of $\beta$, we can rule out the third possibility.

Moreover (premise 5), \emph{$\beta$'s desires are not irrational}.
Admittedly, her desires are strange. $\beta$ would be prepared
to give all she has for an opportunity to play the St.\ Petersburg
gamble. But from a thoroughly subjective point of view, there is
nothing incoherent or inconsistent about these desires.

So $\beta$ is epistemically irrational. By the very first premise, it
follows that $\alpha$ is epistemically irrational. And $\alpha$ was an
arbitrary agent whose credences violate Kolmogorov's axioms. So
rational credences are probabilistic.

The argument has a lot of premises, and many of them could be
challenged. Can you think of a better argument?

\begin{exercise3}
  Why do I need the assumption of ``joint stability'' for pairs of
  propositions?
\end{exercise3}



\section{Comparative credence}\label{sec:comparative-credence}

We have seen that the betting interpretation is untenable. Many
philosophers hold that degrees of belief cannot be defined in terms of
an agent's behaviour, but should rather be treated as theoretical
primitives. Even on that view, however, more must be said about the
numerical representation of credence. That we represent degrees of
belief by numbers between 0 and 1 is clearly a matter of convention:
whatever is represented by these numbers could just as well be
represented by numbers between 0 and 100, by the orientation of a line,
or in various other ways. So we need to explain the convention of
assigning numbers to whatever an agent's credence function represents.

One approach towards such an explanation, which does not turn on an
agent's behaviour, was outlined by the Italian mathematician and
philosopher Bruno de Finetti in the 1930s. De Finetti suggested that
degrees of belief could be defined in terms of the comparative
attitude of being more confident in one proposition than in another.
While any numerical representation of beliefs is partly conventional,
this comparative attitude is plausibly objective and might be taken as
primitive.

Let `$A \succ B$' express that a particular (not further
specified) agent is more confident in $A$ than in $B$. For example, if
you are more confident that it is sunny than that it is raining, then
$\emph{Sunny} \succ \emph{Rainy}$. Let `$A \sim B$' mean that the
agent is equally confident in $A$ and in $B$. From these, we can
define a third relation `$\succsim$' by stipulating that
$A \succsim B$ iff $(A \succ B) \lor (A \sim B)$.

What can we assume about the formal structure of these relations?
First of all, if you're more confident in $A$ than $B$, then you can't
at the same time be more confident in $B$ than $A$ or equally
confident in the two propositions. Moreover, if you're neither more
confident in $A$ than $B$, nor in $B$ than $A$, then you're plausibly
equally confident in the two. So we may assume that an agent's
comparative credence relations are ``complete'' in the following
sense:

\begin{genericthm}{Completeness}
  For any $A$ and $B$, exactly one of $A \succ B$, $B\succ A$, or $A
  \sim B$ is the case.
\end{genericthm}

Next, suppose you are at least as confident in $A$ as in $B$, and at
least as confident in $B$ as in $C$. Then you should be at
least as confident in $A$ as in $C$. In other words, $\succsim$ should
be ``transitive'':
\begin{genericthm}{Transitivity}
  If $A \succsim B$ and $B \succsim C$ then $A \succsim C$.
\end{genericthm}

\cmnt{%
  A \textbf{strict partial order} is a relation $\succ$ that is
  asymmetric and transitive. Asymmetry is entailed by Completeness.
  
  A \textbf{non-strict partial order} is a relation $\succsim$ that is
  reflexive, weakly asymmetric ($A \succsim B \land B \succsim A
  \Rightarrow A = B$) and transitive. We don't have weak asymmetry.

  A \textbf{preorder} is a reflexive and transitive relation
  $\succsim$. We do have that.

  A \textbf{strict weak order} is a relation $\succ$ that is
  asymmetric, transitive, and negatively transitive: if $A \not\succ
  B$ and $B \not\succ C$ then $A \not\succ C$. (By contraposition, if
  $A \succ C$ then either $A \succ B$ or $B \succ C$.)

  Completeness and transitivity for $\succsim$ (not just for $\succ$)
  entail negative transitivity for $\succ$, so $\succ$ is a strict
  weak order: Suppose for reductio that (1) $A \succ C$, (2)
  $A\not\succ B$, and (3) $B\not\succ C$. By completeness from (2),
  either (4) $B \succ A$ or (5) $A \sim B$. By transitivity, (4) and
  (1) entail $B \succ C$, contradicting (3). So (4) is impossible,
  leaving (5). By completeness from (3), either (6) $C \succ B$ or (7)
  $B \sim C$. By transitivity, (1) and (6) entail $A \succ B$,
  contradicting (2). So (6) is impossible, leaving (7). Now we have
  (1) $A \succ C$ and (5) $A \sim B$ and (7) $B \sim C$, but I don't
  think that's a contradiction if we just have transitivity for
  $\succ$. It is however a contradiction if we have transitivity for
  $\succsim$.

  Negative transitivity and asymmetry for $\succ$ obviously don't
  entail completeness, unless we say more about $\sim$. But if we
  define $A \sim B$ as $A \not\succ B \land B \not\succ A$, then
  asymmetry for $\succ$ alone entails completeness. To begin, the
  definition of $\sim$ ensures that $A \sim B$ is symmetrical. Given
  symmetry, completeness amounts to the following claims: (1) $A \succ
  B \Rightarrow B \not\succ A \land A \not\sim B$; (2) $A \sim B
  \Rightarrow A \not\succ B$; (3) $A \not\succ B \land B \not\succ A
  \Rightarrow A \sim B$; (4) $A \not\succ B \land A \not\sim B
  \Rightarrow B \succ A$. The first part of (1) follows from asymmetry
  of $\succ$, the second from the definition of $\sim$. (2)--(4) all
  follow from the definition of $\sim$. 

  So if one defines $\sim$ in terms of $\succ$, and stipulates that
  $\succ$ is a weak order, one gets the same formal properties we
  have assumed. 

  (Do we have symmetry of $\sim$? Yes: Assume $A \sim B$ but $B
  \not\sim A$. By completeness, then $B \succ A$ or $A \succ B$. But
  if $A \sim B$ then neither $A \succ B$ nor $B \succ A$ by
  completeness.)

  A \textbf{non-strict weak order} = \textbf{total pre-order} is a
  pre-order that is total (aka complete), meaning that either $A
  \succsim B$ or $B \succsim A$. That is obviously entailed by
  completeness and transitivity for $\succsim$.

} %

\begin{exercise3}
  Show that Transitivity and Completeness together entail that (a) if
  $A \sim B$ then $B \sim A$, and (b) if $A \sim B$ and $B \sim C$,
  then $A \sim C$.
\end{exercise3}

\cmnt{%

Figuratively speaking, imagine we wanted to order all propositions on
a line in such a way that a proposition $A$ is to the right of a
proposition $B$ just in case $A \succ B$. (Several propositions can be
at the same point on the line: we arrange them on top of each other.)
It can be shown that this is possible if and only if $\succ$ is a weak
order. (Assuming we have at most countably many propositions to order;
otherwise a further condition is needed.)

If we have things arranged on a line, we can represent their relative
position by a number.

} %

For the next assumptions, I use `$\top$' to stand for the logically
necessary proposition (the set of all worlds) and `$\bot$' for the
logically impossible proposition (the empty set).
%
\begin{genericthm}{Normalization}
  $\top \succ \bot$.
\end{genericthm}
\vspace{-2mm}
\begin{genericthm}{Boundedness}
  There is no proposition $A$ such that $\bot \succ A$.
\end{genericthm}
These are fairly plausible as demands of rationality. 

The next assumption is best illustrated by an example. Suppose you are
more confident that Bob is German than that he is French.  Then you
should also be more confident that Bob is \emph{either German or
  Russian} than that he is \emph{either French or
  Russian}. Conversely, if you are more confident that he is German or
Russian than that he is French or Russian, then you should be more
confident that he is German than that he is French. In general:

\begin{genericthm}{Quasi-Additivity}
  If $A$ and $B$ are both logically incompatible with $C$, then $A
  \succsim B$ iff $(A \lor C) \succsim (B \lor C)$.
\end{genericthm}

De Finetti conjectured that whenever an agent's comparative credence
relations satisfy the above five assumptions, then there is a unique
probability measure $\Cr$ such that $A \succ B$ iff $\Cr(A) > \Cr(B)$
and $A \sim B$ iff $\Cr(A) = \Cr(B)$. The conjecture turned out to be
false, because a sixth assumption is required. But the following can
be shown:
%
\begin{genericthm}{Probability Representation Theorem}
  If an agent's comparative credence relations satisfy Completeness,
  Transitivity, Normalization, Boundedness, Quasi-Additivity, and the
  Sixth Assumption, then there is a unique probability measure $\Cr$
  such that $A \succsim B$ iff $\Cr(A) \geq \Cr(B)$.
\end{genericthm}
%
Before I describe the Sixth Assumption, let me explain what the
Probability Representation Theorem might do for us.

I have argued that we can't take numerical credences as unanalysed
primitives. There must be an answer to the question why an agent's
degree of belief in rain is correctly represented by the number 0.2
rather than, say, 0.3. De Finetti's idea was to derive numerical
representations of belief from comparative attitudes towards
propositions.

Imagine we order all propositions on a line, in accordance with the
agent's comparative judgements (which we take as primitive): whenever
the agent is more confident in $A$ then in $B$, $A$ goes to the right
of $B$. The impossible proposition $\bot$ will then be at the left
end, the necessary proposition $\top$ at the right end. If the agent
is equally confident in two propositions, they are stacked on top of
each other at the same point on the line.

Now imagine we hold a ruler under this line in such a way that $\bot$
lies at 0 and $\top$ at 1. Every other proposition will then have a
number between 0 and 1, given by its position along the line. If
that's how we understand degrees of belief, to say that an agent's
degree of belief in rain is 0.2 is to identify the relative position
of rain in an agent's confidence ordering.

The Probability Representation Theorem tells us that if the confidence
ordering satisfies the conditions I have described, then there will be
exactly one way of assigning numbers to the propositions that respects
the probability axioms: there is a unique probability measure $\Cr$
that \emph{represents} the confidence ordering, meaning that
$\Cr(A) > \Cr(B)$ whenever $A \succ B$, and $\Cr(A) = \Cr(B)$ whenever
$A \sim B$. Assuming that an agent's degrees of belief satisfy the
probability axioms therefore amounts to choosing a particular kind of
ruler for measuring degrees of belief.

On this approach, any agent whose attitudes of comparative credence
satisfy the six assumptions is guaranteed to have probabilistic
credences, because the agent's credence function is \emph{defined} as
the unique probability measure $\Cr$ that represents her confidence
ordering.

As you may imagine, this approach has also not gone unchallenged. One
obvious question is whether we can take comparative confidence as
primitive. If we can, a further question is whether the six
assumptions are plausible as general norms of
rationality. Transitivity, Normalization, and Boundedness look fairly
safe, but the others have been questioned. 

The missing sixth assumption is especially troublesome in this
regard. As it turns out, the form of that assumption depends on
whether the number of propositions ranked by $\succ$ is finite or
infinite. In either case the condition is complicated -- which makes
it especially hard to treat it as a basic norm of rationality. Just to
prove the point, here is the condition for the slightly simpler case
of finitely many propositions:

\begin{genericthm}{The Sixth Assumption (finite version)}
  For any two sequences of propositions $A_1,\ldots,A_n$ and
  $B_1,\ldots,B_n$ such that for every possible world $w$, the number of
  propositions in the first sequence that contain $w$ equals the
  number of propositions in the second sequence that contain $w$, if
  $A_i \succsim B_i$ for all $i < n$, then $B_n \succsim A_n$.
\end{genericthm}



\section{Further reading}

A thorough critique of Dutch Book arguments can be found in
%
\begin{itemize}
\item Alan H\'ajek: \href{http://philrsss.anu.edu.au/people-defaults/alanh/papers/DBA.pdf}{``Dutch Book Arguments''} (2008).
\end{itemize}
%
For even more details and background information, have a look at the
Stanford Encyclopedia entry
%
\begin{itemize}
\item Susan Vineberg: \href{https://plato.stanford.edu/entries/dutch-book/}{``Dutch Book Arguments''} (2016).
\end{itemize}

If you're interested in the approach based on comparative credence,
a good (though mathematically non-trivial) introduction is
%
\begin{itemize}
\item Peter Fishburn: \href{https://projecteuclid.org/download/pdf_1/euclid.ss/1177013611}{``The Axioms of Subjective Probability''} (1986).
\end{itemize}

\begin{essay}
  Do you think the Dutch Book Theorems can teach us anything about
  epistemic rationality? If so, can you spell out how? If not, can you
  explain why not?
\end{essay}

\cmnt{%
  Using MEU in the definition may seem odd, but remember we're not
  interested in the ordinary sense of ``belief'' and ``desire'', nor
  in what people say about their beliefs and desires. We're defining a
  pragmatic notion of belief and desire. On that usage, it makes no
  sense to suggest that somebody whose choices reveal certain values
  and beliefs really has quite different values and beliefs that don't
  match those choices.

  To illustrate, imagine we find an alien creature on an island that
  displays sophisticated, intelligent behaviour. Suppose we have
  repeatedly observed it sneaking chocolate out of handbags and hiding
  it on a tree. A sensible explanation would be that it likes to eat
  chocolate and thinks it is safe on the tree. But suppose I suggest
  that in fact, this creature doesn't like chocolate at all. We would
  then ask for another explanation of the behaviour: does it think it
  can use it to attract prey on the tree? Or does it like to tease
  humans?  Suppose I say that all of this is false: from the
  creature's point of view, there is no benefit whatsoever in taking
  the chocolate; it takes it \emph{despite the fact} that it realises
  this to be no good. This is an unintelligible suggestion.

  A minor complication is that the theory that introduces theoretical
  terms can be false. Ramsey is well aware of this. It's like defining
  distance by presupposing Newtonian mechanics. In fact, nothing quite
  plays the role of distance, and the notion is indeterminate between
  several candidates.
} %

\cmnt{%
  Add:

One thing this theory helps explain is why and under what conditions
people buy certain kinds of insurance. ``Imagine the utility an agent
gets from an income of x dollars is x 2 . And imagine that right now
their income is \$90,000. But there is a 5\% chance that something
catastrophic will happen, and their income will be just \$14,400.  So
their expected income is 0.95 × 90, 000 + 0.05 × 14, 400 = 86220. But
their expected utility is just 0.95 × 300 + 0.05 × 120 = 291, or the
utility they would have with an income of \$84,861.  Now imagine this
person is offered insurance against the catastrophic scenario. They
can pay, say, \$4,736, and the insurance company will restore the
\$75,600 that they will lose if the catastrophic event takes
place. Their income is now sure to be \$85,264 (after the insurance is
taken out), so they have a utility of 292. That’s higher than what
their utility was, so this is a good deal for them.  But note that it
might also be a good deal for the insurance company. They receive in
premiums \$4,736. And they have a 5\% chance of paying out
\$75,600. So the expected outlay, in dollars, for them, is \$3,780.
So they turn an expected profit on the deal. If they repeat this deal
often enough, the probability that they will make a profit goes very
close to 1.  The point of the example is that people are trying to
maximise expected utility, while insurance companies are trying to
maximise expected profits. Since there are cases where lowering your
expected income can raise your expected utility, there is a chance for
a win-win trade.''

} %


\cmnt{%

Lecture:

\begin{itemize}
\item Emphasize again that even if we call credences
  ``probabilities'', it is not trivial that they are probabilities,
  and indeed what the numbers are supposed to stand for.
\item They don't stand for intensity of feeling, nor for a special
  kind of judgement. Compare ungraded concept of belief, which is
  often understood in terms of assertions.
\item A good idea is to define credence in terms of effect on
  behaviour; this can come apart from overt judgement. 
\item Problem: entanglement with desires. Later we will define Cr and
  U together.
\item Betting interpretation
\item Go through case (iii) of DBT in detail.
\item First problems with betting interpretation: utility curves.
  Clarify how this affects the argument: Bernoulli agents still don't
  want sure losses. Point is, they wouldn't buy/sell the relevant
  bets. Many bets you would neither buy nor sell for a given price.
\item On representation theorems: what it means in general that < is
  represented by a number f; example; conditions that are required:
  weak order; representation is never unique, but here the result is
  that there's a unique probabilistic representation.
\end{itemize}

--- This went by too fast. I need to go more slowly and give students
more opportunity to think. Like so, perhaps:

\begin{itemize}
\item Emphasize again that even if we call credences
  ``probabilities'', it is not trivial that they are probabilities,
  and indeed what the numbers are supposed to stand for.
\item They don't stand for intensity of feeling, nor for graded
  saying. What else? A good way to put is: how would I test your
  degree of belief? -- Collect ideas.
\item Best approach: look at choices.
\item Problem: entanglement with desires. (Later we will define Cr and
  U together.)
\item Let's assume someone cares about money, and wants more of it.
  Carefully go through why you'd pay up to x for a bet if that's your
  credence.
\item No slowly go through DBT. 
\item If time, mention utility curves. Clarify how this affects the
  argument: Bernoulli agents still don't want sure losses. Point is,
  they wouldn't buy/sell the relevant bets. Many bets you would
  neither buy nor sell for a given price.
\end{itemize}




} %

 	
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
