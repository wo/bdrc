\chapter{Risk}\label{ch:risk}

\cmnt{%

I need to clarify that $U$ is a function of outcomes.

Taleb: ```Psychologists determine our “paranoia” or “risk aversion” by
subjecting a person to a single experiment -- then declare that humans
are rationally challenged as there is an innate tendency to
“overestimate” small probabilities. It is as if the person will never
again take any personal tail risk! Recall that academics in social
science are ... dynamically challenged. Nobody could see the
grandmother-obvious inconsistency of such behavior with our ingrained
daily life logic. Smoking a single cigarette is extremely benign, so a
cost-benefit analysis would deem one irrational to give up so much
pleasure for so little risk!  The flaw in psychology papers is to
believe that the subject doesn’t take any other tail risks anywhere
outside the experiment and will never take tail risks again. ... our
wiring might be adapted to “say no” to tail risk.'''

} %


\section{Why maximize expected utility?}\label{sec:why-meu}

So far, we have largely taken for granted that rational agents
maximize expected utility. It is time to put this assumption under
scrutiny.

In chapter \ref{ch:overview}, I motivated the MEU Principle by arguing
that an adequate decision rule should consider all the outcomes an act
might bring about -- not just the best, the worst, or the most likely
-- and that it should weigh outcomes in proportion to their
probability, so that more likely outcomes are given proportionally
greater weight. These are fairly natural assumptions, and they rule
out many alternatives to the MEU Principle.

We encountered another, quite different, argument for the MEU
Principle in chapter \ref{ch:preference}. The argument began with the
assumption that utility can be measured by the methods described by
von Neumann and Morgenstern or Savage. The MEU Principle can then be
shown to reduce to certain ``axioms'' concerning the agent's
preferences. To conclude the argument, we would have to convince
ourselves that these axioms are genuine norms of rationality. We will
look at some apparent counterexamples below.

Yet another argument for the MEU Principle was hiding in chapter
\ref{ch:utility}.  There we saw that the desirability (or utility) of
a proposition can plausibly be understood as a probability-weighted
average of the desirability of its parts, as described by Jeffrey's
axiom. Now consider a schematic decision problem with two acts and two
states.
%
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr $S_1$ & \gr $S_2$ \\\hline
    \gr $A$ & $O_1$ & $O_2$ \\\hline
    \gr $B$ & $O_3$ & $O_4$ \\\hline
  \end{tabular}
\end{center}
%
Choosing $A$ effectively means choosing to bring about $O_1 \lor O_2$;
choosing $B$ means choosing to bring about $O_3 \lor O_4$. Let's
assume the four outcomes are logically incompatible with each other.
(We can always make them incompatible by adding more information.)
By Jeffrey's axiom,
\[
U(O_1 \lor O_2) = U(O_1)\cdot \Cr(O_1/O_1 \lor O_2) + U(O_2)\cdot \Cr(O_2/O_1 \lor O_2).
\]
Since choosing $A$ is certain to lead to either $O_1$ or $O_2$, and
choosing $B$ is certain to lead to $O_3$ or $O_4$, we also have
\[
  \Cr(O_1 \lor O_2) = \Cr(A).
\]
Moreover, on the supposition that $O_1 \lor O_2$ is true, it is
certain that $O_1$ comes about just in case state $S_1$ obtains:
\[
  \Cr(O_1/O_1 \lor O_2) = \Cr(S_1/O_1 \lor O_2).
\]
Together, the previous two observations entail that
\[
  \Cr(O_1/O_1 \lor O_2) = \Cr(S_1/A).
\]
But in a well-defined decision matrix, the states must be independent
of the acts. This suggests that $\Cr(S_{1}/A) = \Cr(S_{1})$. So we have
\[
  \Cr(O_1/O_1 \lor O_2) = \Cr(S_1).
\]
By the same reasoning, $\Cr(O_2/O_1\lor O_2) = \Cr(S_2)$. The above
instance of Jeffrey's axiom can therefore be rewritten as follows:
\[
U(A) = U(O_1)\cdot \Cr(S_1) + U(O_2)\cdot\Cr(S_2).
\]
This says that the \emph{utility} of act $A$ equals the \emph{expected utility}
of $A$! Since utility is defined as a measure of (all-things-considered)
desirability, the MEU principle therefore says that rational agents prefer to
bring about the most desirable propositions among those they can bring about.
(We will reconsider the present argument in chapter \ref{ch:cdt}.)

We have seen three arguments in favour of the MEU Principle. I will
mention two more, before I turn to objections.

The next argument is simply that the MEU Principle seems to deliver
the correct result in many concrete situations. This includes
situations in which it is obvious what one should do (like in the
mushroom problem from page p.\pageref{ex:mushroom}), but also
situations in which many people are tempted to give a different
answer.

% For example, consider a decision problem in
% which there are just two possible outcomes, \emph{Good} and
% \emph{Bad}, and a range of options leading to these outcomes with
% different probabilities. If \emph{Good} has greater utility than
% \emph{Bad}, one should clearly choose whichever option maximizes the
% probability of \emph{Good}. Similarly, in a case where there is no
% relevant uncertainty, one should clearly choose whichever option is
% known to bring about the best outcome. The MEU Principle passes these
% test.

% In real life, people occasionally seem to violate the MEU Principle,
% but in these cases it is often plausible that they are making a
% mistake. For example, people often make suboptimal choices because
% they overlook certain options. You go to the shop, but forget to buy
% soap. You walk along the highway because it doesn't occur to you that
% you could instead take the nicer route through the park. The relevant
% options (buying soap, taking the nicer route) are available to you,
% and they are better by the lights of your beliefs and desires, so it
% is a mistake that you don't choose them. 

For example, people often fail to give appropriate weight to
low-probability outcomes with very high or very low utility. Many
people worry about dying in a plane crash or a terrorist attack, and
take steps to avoid these events, but don't think twice about driving
to the mall, even after being informed that they are more likely to
die on their way to the mall than on a plane trip or in a terror
attack. This violates the norms of Bayesian rationality, and on
reflection it does seem irrational.

\begin{exercise1}
  In the National Lottery, a £2 ticket typically has an expected
  payoff of around £1. Many people are aware of this fact but still
  play the lottery. One explanation is that they are violating the MEU
  Principle. Can you think of an alternative explanation?
\end{exercise1}

Another example. Suppose you try to avoid plane trips not because you
are afraid of a crash, but because you care about your carbon
footprint. Your friend argues that your behaviour is irrational
because avoiding plane trips actually won't affect any carbon
emissions: no plane is going to stay on the ground just because you
don't fly. Is she right?

It is certainly unlikely that fewer flights will be scheduled as a
result of a single person deciding not to go on a plane. On the other
hand, the number of flights is sensitive to demand: if, one by one,
fewer people decide to fly, at some point fewer flights will be
scheduled. So there must be some chance that avoiding a single plane
trip will reduce overall air traffic. To be sure, the chance is low.
On the other hand, the reduction in carbon emissions would be high. One
can estimate the \emph{expected} reduction in carbon emissions: the
probability-weighted average of the reduction in carbon emissions.
Depending on the flight route, it typically works out to be a little
less than the flight's emissions divided by the number of seats on the
plane. This is how much overall carbon emissions are reduced, on
average, as a result of one passenger deciding not to take a flight.
And so it makes perfect sense to avoid plane trips if you want to
reduce your carbon footprint. Your friend is wrong.

Even Nobel-price winning decision theorists are not immune to this
kind of error. In 1980, John Harsanyi published a paper in which he
argued that utilitarian citizens who care a lot about the common good
would still have little incentive to participate in elections, given
that any individual vote is almost certain not to make a difference.
Here is one of Harsanyi's simplified examples.

\begin{example}
  ``1000 voters have to decide the fate of a socially very desirable
  policy measure $M$. All of them favor the measure. Yet it will pass
  only if all 1000 voters actually come to the polls and vote for
  it. But voting entails some minor costs in terms of time and
  inconvenience. The voters cannot communicate and cannot find out how
  many other voters actually voted or will vote.''
\end{example}

Harsanyi claims that even if the 1000 eligible voters are utilitarians
whose aim is to bring about the best overall result for everyone,
defeat of the measure is likely, since ``each voter will vote only if
he is reasonably sure that all other 999 voters will vote''.

Harsanyi is making the same mistake as your hypothetical friend above.
To flesh out the scenario, let's assume that each voter would lose 1
degree of pleasure by voting. For the case to be interesting, we can
assume that the very desirable measure $M$ would more than offset the
cost of voting, so that the group of 1000 voters is better off if
everyone votes than if everyone stays at home. Since the total degree
of pleasure in the society is reduced by 1000 if everyone votes, the
measure $M$ must therefore increase the total degree of pleasure in
the society by more than 1000. Now consider a utilitarian voter who
values outcomes by their effect on the total degree of pleasure in the
society. If the agent is certain that only, say, 900 others will vote,
it would clearly be sensible for her (in line with the MEU Principle)
to stay at home. But she doesn't need to be ``reasonably sure'', as
Harsanyi claims, that all other 999 will vote, to make it worthwhile
for her to go out and vote -- just as you don't need to be reasonably
sure that fewer flights will be scheduled if you don't take a plane.
If you do the math, you can see that voting maximizes expected utility
even if the probability of all others voting is as low as 0.001.

\begin{exercise3}
  Do the math. That is, describe the decision matrix for a voter in
  Harsanyi's scenario, and confirm that voting maximizes expected
  utility if the probability of all others voting is
  0.001.
\end{exercise3}

\section{The long run}

The last argument for the MEU Principle that I want to mention looks
at the long-term consequences of maximizing expected utility.

Suppose you repeatedly toss a fair coin, keeping track of the total
number of heads and tails. You will find that over time, the
proportion of each outcome approaches its objective probability,
\nicefrac{1}{2}. After one toss, you will have 100\% heads or 100\%
tails. After ten tosses, it's very unlikely that you'll still have
100\% heads or 100\% tails. But 60\% heads and 40\% tails wouldn't be
unusual. The probability of getting 40\% tails or less in 10
independent tosses of a coin is 0.377. For 100 tosses, it is 0.028;
for 1000, it is less than 0.000001. After 1000 tosses, the probability
that the proportion of tails lies between 45\% and 55\% is 0.999.

In general, the probability axioms entail that if there is a sequence
of ``trials'' $T_1,T_2,T_3\ldots$ with the same possible outcomes
(like heads and tails) and the same probabilities for the outcomes,
independent of earlier outcomes, then the probability that the
\emph{proportion} of any outcome in the sequence differs from its
\emph{probability} by more than an arbitrarily small amount $\epsilon$
converges to 0 as the number of trials gets larger and larger. This is
known as the \textbf{law of large numbers}. Loosely speaking: in the
long run, probabilities turn into proportions.

How is that relevant to the MEU Principle? Well, consider a bet on a
fair coin flip: if the coin lands heads, you get £1, otherwise you get
£0. The bet costs £0.40. If you are offered this deal again and again,
the law of large numbers entails that the percentage of heads will
(with high probability) converge to 50\%. So if you buy the bet each
time, you can be confident that you will loose £0.40 in about half the
trials and win £0.60 in the other half. The £0.10 \emph{expected
  payoff} turns into the \emph{average payoff}. In this kind of
scenario, the MEU Principle effectively says that you should prefer
acts with greater average utility over acts with lower average
utility. Which seems obviously correct.

In reality, of course, there are limits to how often one can encounter
the very same decision problem. ``In the long run, we are all dead'',
as John Maynard Keynes quipped. The practical relevance of the above
argument therefore derives not just from the law of large numbers, but
from the fact that probabilities can be expected to converge to
proportions \emph{reasonably fast}: it does not take millions of
tosses until the percentage of heads is almost certain to exceed 40\%.

The above argument also assumes that the same decision problem is faced over and
over. In practice, this rarely happens. But the argument can be generalised.
Suppose an agent faces a sequence of decision problems, which may involve
different outcomes, different states, and different probabilities. One can show
that if the states in the various problems are probabilistically independent and
the relevant probabilities and utilities are not too extreme, then over time,
maximizing expected utility is likely to maximize average (or total) utility.

% \begin{exercise}
%   If you play roulette in a casino, the possible outcomes vary wildly:
%   you might lose a lot or you might win even more. Nonetheless,
%   roulette operators have fairly stable income streams and almost
%   never run out of money. How is that possible?  $\star$
% \end{exercise}

% Long-run considerations offer some support to the MEU Principle; but
% they also point towards a possibly serious problem.

From what I said, you might expect that professional gamblers and investors
generally put their money on the options with greatest expected payoff, which
would give them the greatest overall profit in the long run. But they do not. --
Those who do don't remain professional gamblers or investors for long. To see
why, imagine you are offered to invest in a startup company that attempts to
find a cure for snoring. If the company succeeds, your investment will pay back
tenfold. If the startup fails, the investment is lost. The chance of success is
20\%, so the expected return is $0.2 \cdot 1000\% + 0.8 \cdot 0\% = 200\%$. Even
if this exceeds the expected return of all other investment possibilities, you
would be mad to put all your money into the startup. If you repeatedly face that
kind of decision and go all-in each time, then after ten rounds you are bankrupt
with a probability of $1-0.2^{10} = 0.9999998976$.%
\cmnt{%
  (With the remaining 0.0000001024 probability your wealth would have increased
  1,000,000,000,000\%.)%
} %

This does not contradict the law of large numbers. In the startup
example, you are not facing the same decision problem again and
again. If you lose all your money in the first round, you don't have
anything left to invest in later rounds. Still, the example shows that
maximizing expected utility does not always mean maximizing long-term
actual utility. More importantly, it suggests that there is something
wrong with the MEU Principle. Sensible investors balance expected
returns and risks. A safe investment with lower expected returns is
often preferred to a risky investment with greater expected returns.
Shouldn't we adjust the MEU Principle, so that agents can factor in
risk in addition to expected utility?

\begin{exercise2}
  Every year, an investor is given £100,000, which she can either invest
  in a risky startup of the kind described (a different one
  each year), or put in a bank account at 0\% interest. If she always
  chooses the second option, she will have £1,000,000 after ten years.
  \begin{enumerate}
  \item[(a)] What are the chances that she would do at least as well
    (after ten years) if she always chooses the first option, without
    reinvesting previous profits? (Hint: Compute the chance that she
    would do worse.)
  \item[(b)] How does the answer to (a) mesh with my claim in the text
    that an investor who always goes with the risky option is
    virtually guaranteed to go bankrupt?
  \end{enumerate}
\end{exercise2}

\section{Risk aversion}

Aversion to risk is common, and does not seem irrational. Let's see if
it poses a genuine threat to the MEU Principle.

The standard way to measure risk aversion is to offer people gambles.
Consider a lottery with an 80\% chance of £0 and a 20\% chance of
£1000. The lottery's expected payoff is £200. Given a choice between
the lottery and £100 for sure, a risk averse agent might prefer the
£100. Can we account for these preferences, without giving up the MEU
Principle?

Yes. We only need to assume that, for this agent, the difference in
utility between £1000 and £100 is less than five times the difference
in utility between £100 and £0. For example, if $U(\text{£0}) = 0$,
$U(\text{£100}) = 1$, and $U(\text{£1000}) = 4$, then the lottery has
expected utility $0.8 \cdot 0 + 0.2 \cdot 4 = 0.8$, which is less than
the guaranteed utility of the £100.

\cmnt{%
More generally, as long as a risk averse agent doesn't violate the
axioms of von Neumann and Morgenstern or Savage, we know that there is
a utility function relative to which she ranks all options by their
expected utility.%
} %

Along these lines, the standard model of risk aversion in economics is
to assume that utility is a ``concave function of money'', meaning
that the amount of utility an extra £100 would add to an outcome of
£1000 is less than the amount of utility the same £100 would add to a
lesser outcome of, say, £100. We have already encountered this
phenomenon in chapter \ref{ch:utility}, where we saw that for most
people, money has declining marginal utility: the more you have, the
less utility you get from an extra £100. According to standard
economics, risk aversion is the flip side of declining marginal
utility.

This should seem strange. Intuitively, the fact that the same amount of
money becomes less useful the more money you already have has nothing
to do with risk. Money could have declining marginal utility even for
an agent who loves risk. Conversely, an agent might value every penny
as much as the previous one, but shy away from risks.

The problem is that risk neutrality combined with declining marginal
utility seems to support the exact same choices as risk aversion
combined with non-declining marginal utility. So if an agent's utility
function is defined through her preferences or choice dispositions
over monetary gambles -- by the von Neumann and Morgenstern method,
perhaps -- then the intuitive difference between risk aversion and
declining marginal utility collapses.

You can spin this both ways. You can conclude that we should reject
the intuitive difference between risk aversion and declining marginal
utility. Or you can hold on to the intuitive difference and conclude that
utilities cannot be defined through preferences over monetary gambles.

In any case, things are not so simple. The following scenario, due to
Maurice Allais, shows that risk aversion is not equivalent to
declining marginal utility after all. It also appears to show that
risk aversion is incompatible with the MEU Principle.

\begin{example}[Allais's Paradox]\label{ex:allais}
  A ball is drawn from an urn containing 80 red balls, 19 green balls,
  and 1 blue ball. Consider first a choice between the following two
  lotteries. Which do you prefer?

  \begin{center}
  \begin{tabular}{|r|c|c|c|}\hline
    \gr & \gr Red & \gr Green & \gr Blue \\\hline
    \gr $A$ & £0 & £1000 & £1000 \\\hline
    \gr $B$ & £0 & £1200 & £0  \\\hline
  \end{tabular}
  \end{center}
  %
  Next, consider the alternative lotteries $C$ and $D$, based on the
  same draw from the urn. Which of these do you prefer?
 
  \begin{center}
  \begin{tabular}{|r|c|c|c|}\hline
    \gr & \gr Red & \gr Green & \gr Blue \\\hline
    \gr $C$ & £1000 & £1000 & £1000 \\\hline
    \gr $D$ & £1000 & £1200 & £0 \\\hline
  \end{tabular}
  \end{center}

\end{example}
%
The MEU Principle does not settle the answer to either question. But
it seems to rule out a combination of preferences many people in fact
express, namely a preference for $B$ over $A$, and for $C$ over
$D$.

Why might you have these preferences? Notice that the second choice is
essentially a choice between £1000 for sure and a gamble in which you might
get either £1000 (most likely) or £0 (least likely) or £1200. If
you're risk averse, it makes sense to take the sure £1000. By
contrast, in the first choice the most likely outcome is £0 either
way, and it seems reasonable to take the 19\% chance of getting £1200
rather than the 20\% chance of getting £1000.

You can  check that there is no way of assigning utilities to
monetary payoffs that makes these preferences conform to the MEU
Principle. In fact, the preference for $B$ over $A$, and $C$ over $B$
appears to violate the Independence axiom both in von Neumann and
Morgenstern's form and in Savage's form. To see this (for the von
Neumann and Morgenstern version), let $L_1$ be a lottery that pays
£1000 if a green or blue ball is drawn (otherwise £0), and let $L_2$
be a lottery that pays £1200 if a green ball is drawn (otherwise
£0). Note that $A$ is (in effect) a lottery that leads to $L_1$ with
probability 0.2 and otherwise to £0. Similarly, $B$ is a lottery that
leads to $L_2$ with probability 0.2 and otherwise to £0. By the
Independence axiom, if $L_1 \succsim L_2$, then $A \succsim B$. By
parallel reasoning with $C$ and $D$, it follows from Independence that
\begin{enumerate}
  \itemsep0em 
\item if $L_1 \succsim L_2$, then $A \succsim B$ and $C \succsim D$; and
\item if $L_2 \succsim L_1$, then $B \succsim A$ and $D \succsim C$.
\end{enumerate}
No matter how you rank $L_1$ and $L_2$, you can't have $B \succ A$ and $C \succ D$.

\begin{exercise2}
  Does the preference of $B$ to $A$ and $C$ to $D$ violate strong
  separability, weak separability, or neither? (Explain briefly.)
\end{exercise2}

\cmnt{%
  Weatherson: The STP says that if $A\land C \epref B\land C$ and
  $A\land \neg C \epref B\land \neg C$, then $A \epref B$. I.e., if
  you prefer $A$ to $B$ given $C$ and also given $\neg C$, then you
  should prefer $A$ to $B$.

  Note that the following similar principle is false: if $A\land C
  \epref B\land C$ and $A\land D \epref B\land D$, and $Cr(B \lor
  D)=1$, then $A \epref B$. E.g., ``I'm going to toss two coins. Let p
  be the proposition that they will land differently, i.e. one heads
  and one tails. I offer you a bet that pays you \$2 if p, and costs
  you \$3 if $\neg p$. This looks like a bad bet, since Pr(p) = 0.5,
  and losing \$3 is worse than gaining \$2. But consider the following
  argument.  Let D be that at least one of the coins landing heads. It
  isn't too hard to show that Pr(p|D) = 2/3. So conditional on D, the
  expected return of the bet is 1/3. That's a positive return. So if
  we let A be taking the bet, and B be declining the bet, then
  conditional on D, A is better than B, because the expected return is
  positive. Dito for C=at least one of the coins lands heads.''
} %

Now, the fact that many people prefer $B$ to $A$ and $C$ to $D$ in
Allais's Paradox is, by itself, not a problem for the MEU Principle.
After all, these people may simply be irrational. The problem is that
the Allais preferences seem to make perfect sense for an agent who is
risk averse.

One might respond that rational agents should not be averse to risk.
But that would go against the Humean spirit of our model: we don't
want to make substantive assumptions about what people should care
about. Moreover, there are other problem cases in which this line of
response is untenable. The following case goes back to Peter Diamond.

\begin{example}\label{ex:diamond}
  A mother has a treat that she can give either to her daughter Abbie
  or to her son Ben. She considers three options: giving the treat to
  Abbie, giving it to Ben, and tossing a fair coin, so that Abbie gets
  the treat on heads and Ben on tails. Her decision problem might be
  summarized by the following matrix (assuming for simplicity that if
  the mother decides to give the treat directly to one of her
  children, she nonetheless tosses the coin, just for fun).
%
  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
    \gr & \gr \text{Heads} & \gr \text{Tails} \\\hline
    \gr \text{Give treat to Abbie ($A$)} & \text{Abbie gets treat} & \text{Abbie gets treat} \\\hline 
    \gr \text{Give treat to Ben ($B$)} & \text{Ben gets treat} & \text{Ben gets treat} \\\hline 
    \gr \text{Let coin decide ($C$)} & \text{Abbie gets treat} & \text{Ben gets treat} \\\hline 
    \end{tabular}
  \end{center}
%
  \noindent
  The mother's preferences are $C \succ A$, $C \succ B$, $A \succ B$.
\end{example}

Like in Allais's Paradox, there is no way of assigning utilities to
the outcomes that makes the mother's preferences conform to the MEU
Principle. Yet these preferences are surely not irrational. The
mother prefers $C$ because $C$ is the most fair of the three
options. It would be absurd to claim that rational agents cannot
value fairness.

\begin{exercise3}
  Which of the von Neumann and Morgenstern axioms do the mother's
  preferences in example \ref{ex:diamond} appear to violate? (Explain
  briefly.)  
\end{exercise3}

\cmnt{%

  What axiom(s) do the Diamond preferences violate?  By Continuity, if
  $C \succ A$ and $A \sim B$, then there are lotteries between $C$ and
  $B$ that the agent regards as equivalent to $A$. But mom would
  strictly prefer any such lottery to $A$.

} %

\section{Redescribing the outcomes}

When confronted with an apparent counterexample to the MEU Principle,
the first thing to check is always whether the decision matrix has
been set up correctly. In particular, we need to check if the outcomes
in the matrix specify all attributes that matter to the agent (and
that vary between the outcomes).

The matrices in example \ref{ex:allais} (Allais's Paradox) specify how
much money you get depending on your choice and the drawn ball. But if
you're genuinely risk averse, then you don't just care about how much
money you will have. You also care about risk. So we need to add more
information to the outcomes.

There are two ways of doing that. The first adds to the monetary
payoffs further things that will happen as a result of the relevant
choices and draws.

Consider the bottom right cell of the second matrix in example
\ref{ex:allais}. What will happen if you chose $D$ and the blue ball
is drawn? You get £0. But you'd plausibly also feel frustrated about
your bad luck: there was a 99\% chance of getting at least £1000, and
you got £0! You might also feel regret about your choice: if only you
had chosen the safe alternative $C$, you'd now have £1000. You
probably don't like feelings of frustration and regret. If so, these
feelings should be added to the outcome. The outcome in the bottom
right cell of the second matrix would then say something like `£0 and
considerable frustration/regret'. By contrast, consider the bottom
right cell of the first matrix. If you choose $B$ and the blue ball is
drawn, you get £0. But the chance of getting £0 was 81\%, so you'll be
much less frustrated about your bad luck. You may still regret not
having taken $A$, but since $A$ was just as unsafe as $B$, you
probably wouldn't think you made a terrible mistake. So the outcome in
that cell might say something like `£0 and a little
frustration/regret'. With these changes, the preference for $B$ over
$A$, and for $C$ over $D$ is easily reconciled with the MEU Principle.

\begin{exercise1}
  Assign utilities to the outcomes in the two matrices, with the
  changes just described, so that $EU(B) > EU(A)$ and $EU(C) >
  EU(D)$. 
\end{exercise1}

The problem with this first type of response is that it doesn't always
work. For example, suppose you face Allais's Paradox towards the end
of your life. The ball will only be drawn after your death, and the
money will go to your children. In that case, you will not be around
to experience frustration or regret about the outcome, nor might your
children, if the whole process is kept secret from them. But if you're
risk averse, you might still prefer $B$ to $A$ and $C$ to
$D$.

The second strategy for redescribing outcomes gets around this
problem. As before, we want to distinguish the outcomes in the bottom
right cell of the two decision matrices. So let's ask again what will
happen if you choose $D$ and the blue ball is drawn. One thing that
will happen is that you get £0. You may or may not experience
frustration and regret. But here's another thing that is guaranteed to
happen: you \emph{will have chosen a risky option instead of a safe
  alternative}. If you are risk averse, then plausibly (indeed,
obviously!) you care about whether your choices are risky. So we
should put that into the outcome. By contrast, the outcome in the
bottom right cell of the first matrix does not have this feature --
that you will have chosen a risky option instead of a safe alternative
-- because the alternative $A$ is not safe.  So we can once again
distinguish the two outcomes.

In general, the first strategy appeals only appeals to things that happen as a
causal consequence of the relevant choice. Let's call such attributes
\textbf{local}. On the first strategy for redescribing outcomes, we expand the
original outcomes by local propositions whose truth is a causal consequence of
the relevant act in the relevant state.

By contrast, the second strategy also allows for non-local attributes
in the outcomes. To choose $D$ rather than $C$ \emph{is} to choose a
risky option instead of a safe alternative. That you chose a risky
option is not a causal consequence of your choice; it does not depend
on the causal structure of the world; it is not a separate event that
happens after your choice.

Let's say (following Lara Buchak) that outcomes are
\textbf{individuated locally} if outcomes are only distinguished by
local attributes -- propositions whose truth is a causal consequence of the relevant act in the
relevant state. If outcomes are not individuated locally, they are
\textbf{individuated globally}.

\begin{exercise1}\label{e:diamond}
  Redescribe the outcomes in example \ref{ex:diamond} so that the
  mother's preferences conform to the MEU Principle.
\end{exercise1}

\begin{exercise2}
  \leavevmode\vspace{-2em}
  \begin{enumerate}
  \itemsep0em 
  \item[(a)] In your solution to exercise \ref{e:diamond}, did you
    individuate the outcomes locally or globally?
  \item[(b)] Either way, can you find another answer to the exercise that
    individuates outcomes the other way?
  \end{enumerate}
  \vspace{-2em}
\end{exercise2}

If global individuation of outcomes is allowed, we can defuse the
threat of Allais's Paradox, even in cases without regret or
frustration. Moreover, only this second strategy seems to capture what
really motivates a risk averse agent. Intuitively, just as risk
aversion is not the same as declining marginal utility of money, it is
also not the same as fear of regret or frustration. If you are risk
averse, then one of the things you care about is safety. Given that
outcomes should specify everything the agent cares about -- all her
desires or motives -- we should therefore include the safety or
riskiness of a choice in its outcomes.

Nonetheless, the vast majority of decision theorists assume that
outcomes must be individuated locally. The assumption is so common
that it doesn't even have a name; let's call it
\textbf{localism}. According to localism, in cases where local
features like frustration or regret can't explain the Allais
preferences, these preferences really do contradict the MEU
Principle. We must either declare the preferences irrational or revise
the MEU Principle.

Many such revisions to the MEU Principle have been proposed. An
elegant recent example is Lara Buchak's \emph{Risk-Weighted
  Expected Utility Theory}. On Buchak's model, the choiceworthiness of an
act is not determined by the agent's credences and utilities, the
latter of which pertain only to local attributes of outcomes. Instead,
agents are also assumed to have a ``risk function'' representing their
attitudes towards the non-local features of safety and risk. Buchak
shows that if an agent satisfies a variant of Savage's axioms with a
weakened Independence Axiom, then they can be represented as
maximizing risk-weighted expected utility.

\cmnt{%
  
Lara Buchak notes that standard EU theory doesn't adequately handle
attitudes towards risk and suggests a more complex account on which
the ``risk-weighted expected utility'' of a gamble $[A ? x : y]$, with
$x<y$, is $x + r(P)(A)[y-x]$, rather than $x+P(A)[y-x]$. If $r$ is
convex, the agent is risk-averse.

One problem with this is that $r$ shouldn't depend only on $P$. E.g.,
Buchak's account implies that if I prefer a gamble between 1 util and
-1 util to the status quo, then I must also prefer a gamble between
100 utils and -100 utils to the status quo. But why can't I be more
risk-averse when more is at stake?

Arguably, it can also matter what the gamble is about. Maybe I like
gambles with money but not gambles with lifes or candy. 

There's a general problem here with the motivation for any such theory
(including ``standard'' EU theory): if we want to explain e.g.\ why
people gamble, surely we have to take into account the fact that they
retrieve pleasure from gambling; so the outcomes can't be
characterized in terms of history-free monetary payoffs. Similarly, in
Machina's example of the mother who prefers to give a present by a
fair (random) process, we have to build the method of decision into
the outcomes. But once we do that, why don't we always use this trick
to explain rational cases of risk-aversion or risk-seeking?

\bigskip

A. Sen has a nice example of a case where relational properties
matter: an agent who out of politeness takes the second-largest
cake. This is hard to explain by a simple preference order on
intrinsic features of the cakes. 

} %

But why should one go with localism in the first place? What is wrong
with a global individuation of outcomes?

\begin{exercise2}
  Risk and fairness are two non-local attributes that many people care
  about. Can you think of another such attribute?
  % Decision is my own idea.
  % Conforms to norms.
  \cmnt{%
    On the face of it, this constraint looks implausible and
    arbitrary. Many people seem to care about non-local attributes,
    and not just about risk. For example, you might assign value to
    resisting temptations -- say, to stick with salad when all your
    friends are having fries. The value, we suppose, does not just lie
    in the likely consequences of eating salad compared to the
    consequences of eating fries. Nor does it lie in any feelings (of
    moral superiority, perhaps) you may or may not have as a result of
    the decision. It seems perfectly intelligible that you might put
    value directly on overcoming the temptation to have fries. To
    account for such values, outcomes cannot be individuated locally.

    Or suppose you care about whether your acts conform to certain
    rules of tradition, convention, or morality, or to promises you
    gave at an earlier point. These rules or promises might exclude
    buying a lottery ticket (even in a very favourable lottery); they
    might require you to leave the last slice of cake to others, and
    so on. Localism does not allow for such motives.
  } %
\end{exercise2}

\cmnt{%
\begin{exercise}
  Does psychological hedonism conform to localism? That is, can we
  individuate outcomes locally to represent the motives of a hedonist
  agent? (Explain briefly.) $\star$
\end{exercise}
} %

\section{Localism}

\cmnt{%

  I should make clearer that localism is really a view on legitimate
  attributes.

  Maybe I should have raised the issue in the previous chapter of what
  attributes agents might care about. E.g., can we always set aside
  the past? No. ``Being the first to climb a mountain depends on the
  past.'' (Weirich)

} %

\cmnt{%
  Localism is a view about outcomes. To assess the view, we therefore
  have to think about the role of outcomes in expected utility theory.
} %

To understand the prevalence of localism, we need to go back in
history. The formalism of expected utility theory was originally
developed to compute the monetary value of gambles: a gamble with
expected payoff £100 was assumed to be equivalent in value to £100.
Bernoulli showed that this assumption is false, since money has
declining marginal utility. But Bernoulli did not reject expected
utility theory. Instead, he argued that we should distinguish between
the monetary payoff itself and the utility of that payoff for an
agent. A gamble is worth £100 if the gamble's expected \emph{utility}
equals the \emph{utility} of £100.

For a while, people then understood utility to measure the degree of
pleasure or welfare that a choice might bring about. It was only in
response to the ordinalist challenge of the early 20th century that
utility was -- officially -- no longer defined as a measurable
quantity causally downstream from the choice, but as reflecting the
agent's motives or preferences before, or at the time of, the choice.

This second shift made it possible to regard expected utility theory
as a general model of practical rationality, without assuming that
rational agents only care about their personal pleasure or welfare.

But as we saw in chapter \ref{ch:utility}, the second shift never
fully caught on. Many authors still assume that utility is a measure
of the pleasure or welfare or wealth that results from a choice. On
that conception, if an act $A$ leads to a particular amount of
pleasure/welfare/wealth in state $S_1$, and act $B$ leads to
the very same amount of pleasure/welfare/wealth in state $S_2$, then
the outcome of choosing $A$ in $S_1$ must have the same utility as the
outcome of choosing $B$ in $S_2$.
\cmnt{%

  Another bad reason is that the bearers of utility are standardly
  called `outcomes' or `rewards' or `prizes'. It is odd to describe an
  intrinsic feature of an act, such as its riskiness or its adherence
  to a moral rule, as an ``outcome'' of the act. In ordinary language,
  outcomes are causal consequences. The standard terminology of
  decision theory therefore makes localism sound like a tautology,
  while in fact it is a substantive assumption.

} %

This is one explanation for the widespread acceptance of localism. If
we were interested in modelling selfish agents who only care about
their future pleasure or welfare, localism would be harmless. But that
is not our topic. We want to model agents who may care about other
things -- risk, for example, or fairness, or other people's welfare.

\cmnt{%
  Is fairness an example, contra Buchak? Not clear: whether an act is
  fair arguably doesn't depend on the alternative acts that are
  available. (Well, but does riskiness?)
} %

\cmnt{%
  From a Humean perspective, Buchak's model is unsufficiently general
  because it assumes agents only care about one global feature: risk.%
} %

However, even on the preference-based conception of utility, there is
some pressure to endorse localism. Recall von Neumann's method for
determining the utility of a reward $c$ in comparison to other rewards
$a$ and $b$. The method considers the agent's preferences over certain
lotteries between these rewards. For example, we would check whether
she prefers a fair lottery between $a$ and $c$ to $b$. But if $a$,
$b$, and $c$ have non-local attributes, then these lotteries may well
be logically impossible.

The problem is that anything that comes about as the result of a
lottery inevitably has the non-local attribute of coming about as a
result of that lottery. Suppose we wanted to use von Neumann's method
to determine the utility function for the mother in example
\ref{ex:diamond}. Let $a$ and $b$ be the outcomes of directly giving
the treat to Abby or Ben, respectively. If the mother cares about
fairness, then one relevant (non-local) aspect of $a$ and $b$ is that
who gets the treat is not decided by a chance process. By von
Neumann's method, we should now ask whether the mother prefers some
other outcome $c$ to a \emph{lottery between $a$ and $b$}. This
lottery would be a chance process that leads to outcomes which don't
come about through a chance process. That's logically impossible.

Things are even worse for Savage. Savage assumes that for any possible
outcomes $a$ and $b$ and any state $S$, the agent has preferences
concerning prospects of the form $\bet{S}{a}{b}$. But if the agent is
allowed to care about arbitrary attributes, then one of these
attributes might well be (or entail) $S$. And if, say, $b$ entails
$S$, then there can't be any prospect $\bet{S}{a}{b}$, for such a
prospect would lead to $a$ if $S$ and to $b$ if $\neg S$. Savage's
approach therefore requires a particular kind of localism, known as
``state independence'' of utilities.

\cmnt{%
  If we individuate outcomes as including their entire histories, then
  the Independence axioms, among others, becomes empty, unless we
  allow for logically impossible outcomes.  %
} %

\cmnt{%

  But the conclusion is not inevitable. Some authors, Buchak for
  example, do think preferences can be defined for logically
  impossible events. Although that drives a further wedge between
  preferences and choice dispositions. One certainly never faces a
  choice between impossible options.

} %

The upshot is that if we want to allow agents to care about non-local
attributes such as risk and fairness -- as we should if we are
interested in a general model of practical rationality -- then we
can't build that model on the foundations of von Neumann and
Morgenstern or Savage (or Ramsey).

But we don't have to give up the idea that utilities (and credences)
might be derived from preferences, or that the MEU Principle might be
justified by more fundamental axioms concerning preferences. Other
foundations can be provided. The best known of these alternatives was
developed by Ethan Bolker and Richard Jeffrey in the 1960s.

In the Bolker/Jeffrey construction, the objects of utility are taken
to be arbitrary propositions, not ``rewards'' or ``outcomes'' or
lotteries between rewards or outcomes. No restrictions are imposed on
an agent's basic desires. Like Savage, Bolker and Jeffrey show that if
an agent's preferences between propositions satisfy certain axioms,
then they can be represented by a probabilistic credence function
$\Cr$ and a utility function $U$ that satisfies Jeffrey's axiom --
which, in turn, supports the MEU Principle by the argument outlined in
section \ref{sec:why-meu}. I will not explain all the axioms or the
details of the construction. However, I do want to mention that
since preferences are now defined between arbitrary propositions, the gap
between preferences and choice dispositions is even larger in the
Bolker/Jeffrey construction than it was for von Neumann and
Morgenstern or Savage. %
\cmnt{%
  for many propositions $A$ and $B$, the hypothesis that someone faces
  a choice between $A$ and $B$ will be logically impossible, because
  $A$ and $B$ entail that no-one ever faced such a choice. %
} %

This brings me to one final point in favour of localism. Localism
promises to give the MEU Principle predictive power. Localism ensures
that the very same outcome can figure in different decision problems.
If outcomes can be individuated globally, then we can always find
differences between outcomes in different decision problems. And then
we can never find out, just from the agent's choices, that she
violates transitivity, independence, or any of the other classical
axioms. No matter what the agent does, there will always be some
utility function relative to which all her choices maximize expected
utility. Localism blocks this threat of trivialization.

But should we want to block the threat? Shouldn't we rather accept that on
a broadly Humean conception of practical rationality, no pattern of
behaviour could, all by itself, show that the agent is practically
irrational? Couldn't the agent have a basic desire to display just
this pattern of behaviour? It is not obvious that we should expect a
general theory of practical rationality to make testable predictions,
without any assumptions about the agent's beliefs and desires.

Localism in effect encodes one such assumption: that the agent only
cares about local features of outcomes. But the assumption is not only
implausible, it also barely helps with the trivialization worry. For
even if we restrict ourselves to local properties, we can almost
always find differences between outcomes in different decision
problems. I suspect this explains why in practice, psychologists and
social scientists often fall back on the 19th century conception of
utility as a measure of personal wealth or welfare. That gives the MEU
Principle considerable predictive power. It also renders the principle
obviously false, both as a normative principle about what people
should do and as a descriptive principle about people's actual
behaviour.

\cmnt{%
  But we have to appreciate that we can't make interesting predictions
  about choices and at the same time define people's goals through
  their choices.%
} %

\cmnt{%
  It seems to me that there is a better response: make assumptions
  about what people actually prefer. These don't need to be normative
  assumptions.
} %

\cmnt{%
  Broome's idea that outcomes should be distinguished if it is
  rational to care differently about them is similar to Weatherson's
  idea to use evidential probability rather than credence to define
  EU. In either case a norm on attitudes is folded into the decision
  rule. It's better to keep these apart: there's what you actually
  believe, what you actually desire, and what you should do in light
  of these attitudes, whether they are rational or not. Here we want
  to remain largely neutral on what the attitudes may be. Then there's
  the question of whether your attitudes are reasonable. In practice,
  holding on to this division of labour is not entirely possible
  because it's hard to spell out decision rules for incoherent
  agents. So we'll need to impose probabilism, transitivity of
  preferences etc.
} %

\section{Further reading}

Some good (and rare) discussion of the localism issue can be found in

\begin{itemize}
\item Jamie Dreier: ``Rational preference: Decision theory as a theory of practical rationality'' (1996), 
\item Lara Buchak: ``Redescription'', chapter 4 of her \emph{Risk and Rationality} (2013),
\item Paul Weirich: ``Expected Utility and Risk'' (1986).
\end{itemize}
%
If you want to know more about Buchak's risk-weighted expected utility
theory, have a look at

\begin{itemize}
\item Lara Buchak: \href{https://philosophy.berkeley.edu/file/754/Buchak_Risk_and_Tradeoffs.pdf}{``Risks and tradeoffs''}. (2014)
\end{itemize}

The Jeffrey/Bolker construction is described in
\begin{itemize}
\item Richard Jeffrey: \emph{The Logic of Decision}, chapter 9 (1983).
\end{itemize}

The paper by Harsanyi mentioned in section \ref{sec:why-meu} is
\begin{itemize}
\item John Harsanyi: ``Rule utilitarianism, rights, obligations and the theory of rational behavior'' (1980).
\end{itemize}

\begin{essay}
  Discuss Buchak's defence of localism in the ``Redescription''
  chapter. What are her main arguments? Can you think of objections or
  further support?
\end{essay}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
