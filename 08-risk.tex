\chapter{Risk}\label{ch:risk}

\cmnt{%

I need to clarify that $U$ is a function of outcomes.

} %


\section{Why maximize expected utility?}\label{sec:why-meu}

So far, we have largely taken for granted that rational agents
maximize expected utility. It is time to put this assumption under
scrutiny.

In chapter \ref{ch:overview}, I motivated the MEU Principle by arguing
that an adequate decision rule should consider all the outcomes an act
might bring about -- not just the best, the worst, or the most likely
-- and that it should weigh outcomes in proportion to their
probability, so that more likely outcomes are given proportionally
greater weight. These are fairly natural assumptions, and they rule
out many alternatives to the MEU Principle.

We encountered another, quite different, argument for the MEU
Principle in chapter \ref{ch:preference}. The argument began with the
assumption that utility can be measured by the methods described by
von Neumann and Morgenstern or Savage. The MEU Principle can then be
shown to reduce to certain ``axioms'' concerning the agent's
preferences. To conclude the argument, we would have to convince
ourselves that these axioms are genuine norms of rationality. We will
look at some apparent counterexamples below.

Yet another argument for the MEU Principle was hiding in chapter
\ref{ch:utility}.  There we saw that the desirability (or utility) of
a proposition can plausibly be understood as a probability-weighted
average of the desirability of its parts, as described by Jeffrey's
axiom. Now consider a schematic decision problem with two acts and two
states.
%
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr $S_1$ & \gr $S_2$ \\\hline
    \gr $A$ & $O_1$ & $O_2$ \\\hline
    \gr $B$ & $O_3$ & $O_4$ \\\hline
  \end{tabular}
\end{center}
%
Assume the outcomes are all logically incompatible with one
another. Choosing $A$ then effectively means choosing to bring about
$O_1 \lor O_2$; choosing $B$ means choosing to bring about $O_3
\lor O_4$. By Jeffrey's axiom,
\[
U(O_1 \lor O_2) = U(O_1)\times \Cr(O_1/O_1 \lor O_2) + U(O_2)\times \Cr(O_2/O_1 \lor O_2).
\]
What is $\Cr(O_1/O_1 \lor O_2)$? It is, of course, the agent's
credence that $O_1$ will come about on the supposition that $O_1$ or
$O_2$ will come about. On the supposition that $O_1$ or $O_2$ will
come about, it is certain that $O_1$ will come about just in case
$S_1$ is the case.  Moreover, supposing $O_1 \lor O_2$ is tantamount
to supposing that the agent will choose $A$.  So $\Cr(O_1/O_1 \lor
O_2)$ is also the agent's credence in $S_1$ on the supposition that
she will choose $A$. But in a properly construed decision problem, the
states must be independent of the acts; so the agent's credence in
$S_1$ on the supposition that she chooses act $A$ should equal her
unconditional credence in $S_1$. So $\Cr(O_1/O_1 \lor O_2) =
\Cr(S_1)$.  By the same reasoning, $\Cr(O_2/O_1 \lor O_2) =
\Cr(S_2)$. Plugging these into the above instance of Jeffrey's axiom,
we get
\[
U(O_1 \lor O_2) = U(O_1)\times \Cr(S_1) + U(O_2)\times\Cr(S_2).
\]

This says that the desirability of the proposition $O_1 \lor O_2$,
which the agent would bring about by choosing act $A$, equals the
expected utility of $A$! Assuming that an agent should prefer to bring
about more desirable propositions rather than less desirable
propositions, it follows that the agent should maximize expected
utility.  (We will reconsider the present argument in chapter
\ref{ch:cdt}.)

So we have seen three arguments in favour of the MEU Principle. I will
mention two more, before I turn to objections.

The next argument is simply that the MEU Principle delivers the
intuitively correct result in many applications, both artificial and
realistic. For example, consider a decision problem in which there are
just two possible outcomes, \emph{Good} and \emph{Bad}, and a range of
options leading to these outcomes with different probabilities. If
\emph{Good} has greater utility than \emph{Bad}, one should clearly
choose whichever option maximizes the probability of
\emph{Good}. Similarly, in a case where there is no relevant
uncertainty, one should clearly choose whichever option is known to
bring about the best outcome. The MEU Principle passes these test.

In real life, people occasionally seem to violate the MEU Principle,
but in these cases it is often plausible that they are making a
mistake. For example, people often make suboptimal choices because
they overlook certain options. You go to the shop, but forget to buy
soap. You walk along the highway because it doesn't occur to you that
you could instead take the nicer route through the park. The relevant
options (buying soap, taking the nicer route) are available to you,
and they are better by the lights of your beliefs and desires, so it
is a mistake that you don't choose them. 

Another well-known type of mistake concerns acts that come with a very
low probability of either great or terrible outcomes. People tend to
give those outcomes either far too much weight or far too little. For
example, many people worry about dying in a plane crash or a terrorist
attack, and take steps to avoid these events, but don't think twice
about driving to the mall, even after being informed that they are
much more likely to die on their way to the mall than on a plane trip
or in a terror attack.

\begin{exercise}
  In the National Lottery, a £2 ticket typically has an expected
  payoff of around £1. Many people are aware of this fact but still
  play the lottery. One explanation is that they are violating the MEU
  Principle. Can you think of an alternative explanation? $\star$
\end{exercise}

Suppose you try to avoid plane trips not because you are afraid of a
crash but because you care about your carbon footprint. Your friend
argues that the effort is pointless since no plane is going to stay on
the ground just because you don't fly. Is she right?

It is certainly unlikely that fewer flights will be scheduled as a
result of a single person deciding not to take a plane. On the other
hand, the number of flights is sensitive to demand: if, one by one,
fewer people decide to fly, at some point fewer flights will be
scheduled. So there must be some chance that avoiding a single plane
trip will reduce overall air traffic. To be sure, the chance is very
low. On the other hand, the reduction in carbon emissions would be
very significant. As a result, the \emph{expected} reduction in carbon
emissions -- the probability-weighted average of the reduction in
carbon emissions -- from avoiding a single flight is much lower than
the flight's emissions, but not zero. It typically works out to be a
little less than the flight's emissions divided by the number of seats
on the plane. This is how much overall carbon emissions are reduced,
on average, as a result of one passenger deciding not to take a
flight. So your friend is wrong.

Even Nobel-price winning decision theorists are not immune to this
kind of mistake. In 1980, John Harsanyi published a paper in which he
argued that utilitarian citizens who care a lot about the common good
would still have little incentive to participate in elections, given
that any individual vote is almost certain not to make a
difference. Here is one of Harsanyi's simplified examples.

\begin{example}
  ``1000 voters have to decide the fate of a socially very desirable
  policy measure $M$. All of them favor the measure. Yet it will pass
  only if all 1000 voters actually come to the polls and vote for
  it. But voting entails some minor costs in terms of time and
  inconvenience. The voters cannot communicate and cannot find out how
  many other voters actually voted or will vote.''
\end{example}

Harsanyi claims that even if the 1000 eligible voters are utilitarians
whose aim is to bring about the best overall result for everyone,
defeat of the measure is likely, since ``each voter will vote only if
he is reasonably sure that all other 999 voters will vote''.

Harsanyi is making the same mistake as your hypothetical friend
above. To flesh out the scenario, let's assume that each voter would
lose 1 degree of pleasure by voting. For the case to be interesting,
we can assume that the very desirable measure $M$ would more than
offset the cost of voting, so that the group of 1000 is better off if
everyone votes than if everyone stays at home. Since the total degree
of pleasure in the society is reduced by 1000 if everyone votes, the
measure $M$ must therefore increase the total degree of pleasure in
the society by more than 1000. Now consider a utilitarian voter who
values outcomes by their effect on the total degree of pleasure in the
society. If the agent is certain that only, say, 900 others will vote,
it would clearly be sensible for her (in line with the MEU Principle)
to stay at home. But she doesn't need to be ``reasonably sure'', as
Harsanyi claims, that all other 999 will vote, to make it worthwhile
for her to go out and vote -- just as you don't need to be reasonably
sure that fewer flights will be scheduled if you don't take a
plane. If you do the math, you can see that voting maximizes expected
utility even if the probability of all others voting is as low as
0.001.

\begin{exercise}
  Do the math. That is, describe the decision matrix for a voter in
  Harsanyi's scenario, and confirm that voting maximizes expected
  utility if the probability of all others voting is
  0.001. $\star\star\star$
\end{exercise}

\section{The long run}

The last argument for the MEU Principle that I want to mention relates
to a consequence of the probability axioms we haven't used until now:
the \textbf{law of large numbers}.

Suppose you repeatedly toss a fair coin, keeping track of the total
number of heads and tails. You will find that over time, the
proportion of each outcome approaches its objective probability,
\nicefrac{1}{2}. After one toss, you will have 100\% heads or 100\%
tails. After ten tosses, it's very unlikely that you'll still have
100\% heads or 100\% tails. But 60\% heads and 40\% tails wouldn't be
unusual. The probability of getting 40\% tails or less in 10
independent tosses of a coin is 0.377. For 100 tosses, it is 0.028;
for 1000, it is less than 0.000001. After 1000 tosses, the probability
that the proportion of tails lies between 45\% and 55\% is 0.999.

In general, the law of large numbers states that if there is a sequence
of ``trials'' $T_1,T_2,\ldots,T_n$ each of which can result in the
same outcomes, and the probability of the outcomes is the same in each
trial and independent of the earlier trials, then the probability that
the percentage of any outcome (across all trials) differs from its
probability by more than an arbitrarily small amount $\epsilon$
converges to 0 as the number of trials gets larger and larger. Loosely
speaking: in the long run, probabilities turn into proportions.

How is that relevant to the MEU Principle? Well, consider a bet on a
fair coin flip: if the coin lands heads, you get £1, otherwise you get
£0. The bet costs £0.40. If you are offered this deal again and again,
the law of large numbers entails that the percentage of heads will
(with high probability) converge to 50\%. So if you buy the bet each
time, you can be confident that you will loose £0.40 in about half the
trials and win £0.60 in the other half. The \emph{expected payoff}
turns into the \emph{average payoff}; the \emph{expected utility}
turns into the \emph{average utility}. In this kind of scenario, the
MEU Principle thus effectively says that you should prefer options
with greater average utility to options with lower average
utility. Which seems obviously correct.

In reality, of course, there are limits to how often one can encounter
the very same decision problem. ``In the long run, we are all dead'',
as John Maynard Keynes quipped. The practical relevance of the present
argument therefore derives not just from the law of large numbers, but
from the fact that probabilities can be expected to converge to
proportions \emph{reasonably fast}: it doesn't take trillions of
tosses until the percentage of heads is almost certain to exceed 40\%

Even so, the argument as it stands only supports the MEU Principle in
rather unusual cases where an agent faces the same decision problem
again and again and again (and where she regards the states in each
problem as probabilistically independent of the earlier
states). Outside gambling and investing, most of the choices we face
are not like that. You won't have an opportunity to marry the same
person, under the same circumstances, hundreds or thousands of times.

In response, one can try to broaden the relevance of the argument. For
example, the law of large numbers doesn't require that the repeated
``trials'' involve literally the same acts, states, and outcomes; it
is enough if the utilities and probabilities associated with each act
are the same -- or even just roughly the same.

\begin{exercise}
  If you play roulette in a casino, the possible outcomes vary wildly:
  you might lose a lot or you might win even more. Nonetheless,
  roulette operators have fairly stable income streams and almost
  never run out of money. How is that possible?  $\star$
\end{exercise}

Long-run considerations offer some support to the MEU Principle; but
they also point towards a possibly serious problem.

From what I said, you might expect that professional gamblers and
investors generally put their money on the options with greatest
expected payoff. But they do not. (Those who do don't remain
professional gamblers or investors for long.) To see why, imagine you
are offered to invest in a startup company that tries to find a cure
for snoring. If they succeed, your investment will pay back
tenfold. If they don't, the investment is lost. The chance of success
is 20\%, so the expected return is $0.2 \times 1000\% + 0.8 \times 0\% =
200\%$. Even if this exceeds the expected return of all other
investment possibilities, you would be mad to put all your money into
that company. If you repeatedly faced that kind of decision and went
all-in each time, then after ten rounds you would be bankrupt with a
probability of $1-0.2^{10} = 0.9999998976$.%
\cmnt{%
  (With the remaining 0.0000001024 probability your wealth would have
  increased 1,000,000,000,000\%.)%
} %

Sensible investors balance expected returns and risks. A safe
investment with lower expected returns is often preferred to a risky
investment with greater expected returns. Should we therefore adjust
the MEU Principle, so that agents can factor in risk in addition to
expected utility?

\begin{exercise}
  Each year, an investor is given £100,000, which she can invest
  either in a risky startup of the kind described (a different one
  each year), or put in a bank account at 0\% interest. If she always
  chooses the second option, she will have £1,000,000 after ten years.
  \begin{enumerate}
  \item[(a)] What are the chances that she would do at least as well
    (after ten years) if she always chooses the first option, without
    reinvesting previous profits? (Hint: Compute the chance that she
    would do worse.)
  \item[(b)] How does the answer to (a) mesh with the claim (in the
    text) that an investor who always goes with the risky option is
    virtually guaranteed to go bankrupt before long?
  \end{enumerate}
  $\star\star$
\end{exercise}

\section{Risk aversion}

Aversion to risk is very common, and it does not seem
irrational. Let's see if it poses a genuine threat to the MEU
Principle.

A risk averse agent might prefer £100 for sure to a lottery with an
80\% chance of £0 and a 20\% chance of £1000. But we can account for
these preferences without giving up the MEU Principle. We only need to
assume that, for this agent, the difference in utility between £1000
and £100 is less than five times the difference in utility between
£100 and £0. For example, if $U(\text{£0}) = 0$, $U(\text{£100}) = 1$,
and $U(\text{£1000}) = 4$, then the lottery has expected utility $0.8
\times 0 + 0.2 \times 4 = 0.8$, which is less than the guaranteed
utility of the £100.

\cmnt{%
More generally, as long as a risk averse agent doesn't violate the
axioms of von Neumann and Morgenstern or Savage, we know that there is
a utility function relative to which she ranks all options by their
expected utility.%
} %

Along these lines, the standard way to model monetary risk aversion in
economics is to assume that utility is a ``concave function of
money'', meaning that the amount of utility an extra £100 would add to
an outcome of £1000 is less than the amount of utility the same £100
would add to an lesser outcome of, say, £100. We have already
encountered this phenomenon in chapter \ref{ch:utility}, where we saw
that for most people, money has declining marginal utility: the more
you have, the less utility you get from an extra £100. According to
standard economics, risk aversion is the flip side of declining
marginal utility.

That may seem strange. Intuitively, the fact that the same amount of
money becomes less useful the more money you already have has nothing
to do with risk. Money could have declining marginal utility even for
an agent who loves risk. Conversely, an agent might value every penny
as much as the previous one, but shy away from risks.

The problem is that risk neutrality combined with declining marginal
utility seems to support the exact same choices as risk aversion
combined with non-declining marginal utility. So if an agent's utility
function is defined through her preferences or choice dispositions --
by the von Neumann and Morgenstern method, perhaps -- then the
intuitive difference between risk aversion and declining marginal
utility seems to collapse.

You can spin this both ways. You can conclude that we should reject
the intuitive difference between risk aversion and declining marginal
utility. Or you can hold on to the intuitive difference and conclude that
utilities cannot be defined through preferences.

In any case, things are not so simple. The following scenario, due to
Maurice Allais, seems to show that risk aversion is not equivalent to
declining marginal utility after all, and that it is incompatible with the MEU
Principle.

\begin{example}[Allais's Paradox]\label{ex:allais}
  A ball is drawn from an urn containing 80 red balls, 19 green balls,
  and 1 blue ball. Consider first the choice between the following two
  gambles. Which do you prefer?

  \begin{center}
  \begin{tabular}{|r|c|c|c|}\hline
    \gr & \gr Red & \gr Green & \gr Blue \\\hline
    \gr $A$ & £0 & £1000 & £1000 \\\hline
    \gr $B$ & £0 & £1200 & £0  \\\hline
  \end{tabular}
  \end{center}
  %
  Next, consider the alternative gambles $C$ and $D$, based on the
  same draw from the urn. Which of these do you prefer?
 
  \begin{center}
  \begin{tabular}{|r|c|c|c|}\hline
    \gr & \gr Red & \gr Green & \gr Blue \\\hline
    \gr $C$ & £1000 & £1000 & £1000 \\\hline
    \gr $D$ & £1000 & £1200 & £0 \\\hline
  \end{tabular}
  \end{center}

\end{example}

The MEU Principle does not settle the answer to either question. But
it seems to rule out a combination of preferences many people in fact
express, namely a preference for $B$ over $A$, and for $C$ over
$D$. Why might you have these preferences? Notice that the second
choice is essentially one between £1000 for sure and a gamble in which
you might get either £1000 (most likely) or £0 (least likely) or
£1200. If you're moderately risk averse, it makes sense to take the
sure £1000. By contrast, in the first choice the most likely outcome
is £0 either way, and it seems reasonable to take the gamble with the
greater possible payoff (£1200 vs.\ £1000), even though the
probability of the payoff is slightly lower.

You can easily check that there is no way of assigning utilities to
monetary payoffs that makes these preferences conform to the MEU
Principle. In fact, the preference for $B$ over $A$ and $C$ over $B$
appears to violate the Independence axiom both in von Neumann and
Morgenstern's form and in Savage's form. To see this (for the von
Neumann and Morgenstern version), let $L_1$ be a lottery that pays
£1000 if a green or blue ball is drawn (otherwise £0), and let $L_2$
be a lottery that pays £1200 if a green ball is drawn (otherwise
£0). Note that $A$ is (in effect) a lottery that leads to $L_1$ with
probability 0.2 and otherwise to £0. Similarly, $B$ is a lottery that
leads to $L_2$ with probability 0.2 and otherwise to £0. By the
Independence axiom, if $L_1 \succsim L_2$, then $A \succsim B$. By
parallel reasoning with $C$ and $D$, it follows that
\begin{enumerate}
  \itemsep0em 
\item if $L_1 \succsim L_2$, then $A \succsim B$ and $C \succsim D$; and
\item if $L_2 \succsim L_1$, then $B \succsim A$ and $D \succsim C$.
\end{enumerate}
No matter how you rank $L_1$ and $L_2$, you can't have $B \succ A$ and $C \succ D$.

\begin{exercise}
  Does the preference of $B$ to $A$ and $C$ to $D$ violate strong
  separability, weak separability, or neither? (Explain briefly.)
  $\star\star$
\end{exercise}

\cmnt{%
  Weatherson: The STP says that if $A\land C \epref B\land C$ and
  $A\land \neg C \epref B\land \neg C$, then $A \epref B$. I.e., if
  you prefer $A$ to $B$ given $C$ and also given $\neg C$, then you
  should prefer $A$ to $B$.

  Note that the following similar principle is false: if $A\land C
  \epref B\land C$ and $A\land D \epref B\land D$, and $Cr(B \lor
  D)=1$, then $A \epref B$. E.g., ``I'm going to toss two coins. Let p
  be the proposition that they will land differently, i.e. one heads
  and one tails. I offer you a bet that pays you \$2 if p, and costs
  you \$3 if $\neg p$. This looks like a bad bet, since Pr(p) = 0.5,
  and losing \$3 is worse than gaining \$2. But consider the following
  argument.  Let D be that at least one of the coins landing heads. It
  isn't too hard to show that Pr(p|D) = 2/3. So conditional on D, the
  expected return of the bet is 1/3. That's a positive return. So if
  we let A be taking the bet, and B be declining the bet, then
  conditional on D, A is better than B, because the expected return is
  positive. Dito for C=at least one of the coins lands heads.''
} %

The fact that many people prefer $A$ to $B$ and $C$ to $D$ in Allais's
Paradox is, by itself, not a problem for the MEU Principle. We have
already acknowledged that people's preferences are sometimes
irrational. Maybe we've just found another example. The problem is
that the Allais preferences seem to make perfect sense for an agent
who is risk averse.

One might respond that rational agents shouldn't be averse to
risk. But that would go against the Humean spirit of our model: we
don't want to make substantive assumptions about what people should
care about. Moreover, there are other problem cases in which this line
of response is untenable. The following case goes back to Peter
Diamond.

\begin{example}\label{ex:diamond}
  A mother has a treat that she can give either to her daughter Abbie
  or to her son Ben. She considers three options: giving the treat to
  Abbie, giving it to Ben, and tossing a fair coin and giving the
  treat to Abbie on heads and to Ben on tails. Her decision problem
  might be summarized by the following matrix (assuming for simplicity
  that if the mother decides to give the treat directly to one of her
  children, she nonetheless tosses the coin, just for fun).

  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
    \gr & \gr \text{Heads} & \gr \text{Tails} \\\hline
    \gr \text{Give treat to Abbie ($A$)} & \text{Abbie gets treat} & \text{Abbie gets treat} \\\hline 
    \gr \text{Give treat to Ben ($B$)} & \text{Ben gets treat} & \text{Ben gets treat} \\\hline 
    \gr \text{Let coin decide ($C$)} & \text{Abbie gets treat} & \text{Ben gets treat} \\\hline 
    \end{tabular}
  \end{center}
  
  The mother's preferences are $C \succ A$, $C \succ B$, $A \succ B$.
\end{example}

Like in Allais's Paradox, there is no way of assigning utilities to
the outcomes that makes the mother's preferences conform to the MEU
Principle. Yet these preferences are surely not irrational. The
mother prefers $C$ because $C$ is the most fair of the three
options. It would be absurd to claim that rational agents cannot
value fairness.

\begin{exercise}
  Which of the von Neumann and Morgenstern axioms do the mother's
  preferences in example \ref{ex:diamond} appear to violate? (Explain
  briefly.)  $\star \star \star$
\end{exercise}

\cmnt{%

  What axiom(s) do the Diamond preferences violate?  By Continuity, if
  $C \succ A$ and $A \sim B$, then there are lotteries between $C$ and
  $B$ that the agent regards as equivalent to $A$. But mom would
  strictly prefer any such lottery to $A$.

} %

\section{Redescribing the outcomes}

When confronted with an apparent counterexample to the MEU Principle,
the first thing to check is always whether the decision matrix has
been set up correctly. In particular, we need to check if the outcomes
in the matrix specify all attributes that matter to the agent (and
that vary between the outcomes).

The matrices in example \ref{ex:allais} (Allais's Paradox) specify how
much money you get depending on your choice and the drawn ball. But if
you're genuinely risk averse, then arguably you don't just care about
how much money you will have. 

What else might you care about? There are two kinds of answers, and
thus two ways of redescribing the outcomes. The first adds to the
monetary payoffs further things that will happen as a result of the
relevant choices and draws.

Consider the bottom right cell of the second matrix in example
\ref{ex:allais}: what will happen if you chose $D$ and the blue ball
is drawn? You get £0. But you'd plausibly also feel frustrated about
your bad luck: there was a 99\% chance of getting at least £1000, and
you got £0! You might also feel regret about your choice: if only you
had chosen the safe alternative $C$, you'd now have £1000. You
probably don't like feelings of frustration and regret. If so, they
should be added to the outcome. The outcome in the bottom right cell
of the second matrix would then say something like `£0 and
considerable frustration/regret'. By contrast, consider the bottom
right cell of the first matrix. If you choose $B$ and the blue ball is
drawn, you get £0. But the chance of getting £0 was 81\%, so you'll be
much less frustrated about your bad luck. You may still regret not
having taken $A$, but since $A$ was just as unsafe as $B$, you
probably wouldn't think you made a terrible mistake. So the outcome in
that cell should say something like `£0 and a little
frustration/regret'. With these changes, the preference for $B$ over
$A$, and for $C$ over $D$ is easily reconciled with the MEU Principle.

\begin{exercise}
  Assign utilities to the outcomes in the two matrices, with the
  changes just described, so that $EU(B) > EU(A)$ and $EU(C) >
  EU(D)$. $\star$
\end{exercise}

The problem with this first type of response is that it doesn't always
work. For example, suppose you face Allais's Paradox towards the end
of your life. The ball will only be drawn after your death, and the
money will go to your children. In that case, you will not be around
to experience frustration or regret about the outcome, nor might your
children, if the whole process is kept secret from them. But if you're
risk averse, you might still prefer $B$ to $A$ and $C$ to
$D$.

The second strategy for redescribing outcomes gets around this
problem. As before, we want to distinguish the outcomes in the bottom
right cell of the two decision matrices. So let's ask again what will
happen if you choose $D$ and the blue ball is drawn. One thing that
will happen is that you got £0. You may or may not experience
frustration and regret. But here's another thing that is guaranteed to
happen: you \emph{will have chosen a risky option instead of a safe
  alternative}. If you're risk averse, then plausibly (indeed,
obviously!) you care about whether your choices are risky. So we
should put that into the outcome. By contrast, the outcome in the
bottom right cell of the first matrix does not have this feature --
that you will have chosen a risky option instead of a safe alternative
-- because the alternative $A$ is not safe.  So we can once again
distinguish the two outcomes.

In general, the first strategy appeals only to local features of the
outcomes: to causal consequence of the relevant choice. The second
strategy also allows for more global, history-dependent attributes in
the outcomes. To choose $D$ rather than $C$ \emph{is} to choose a
risky option instead of a safe alternative. That you chose a risky
option is not a causal consequence of your choice; it does not depend
on the causal structure of the world; it is not a separate event that
happens after your choice.

Let's say (following Lara Buchak) that outcomes are
\textbf{individuated locally} if outcomes are only distinguished by
features that are a genuine causal consequence of the relevant act. If
outcomes are not individuated locally, they are \textbf{individuated
  globally}. 

\begin{exercise}\label{e:diamond}
  Redescribe the outcomes in example \ref{ex:diamond} so that the
  mother's preferences conform to the MEU Principle. $\star$
\end{exercise}

\begin{exercise}
  \leavevmode\vspace{-2em}
  \begin{enumerate}
  \itemsep0em 
  \item[(a)] In your solution to exercise \ref{e:diamond}, did you
    individuate the outcomes locally or globally?
  \item[(b)] Either way, can you find another answer to the exercise that
    individuates outcomes the other way?
    $\star\star$
  \end{enumerate}
  \vspace{-2em}
\end{exercise}

If global individuation of outcomes is allowed, we can defuse the
threat of Allais's Paradox, even in cases without regret or
frustration. Moreover, only this second strategy seems to capture what
really motivates a risk averse agent. Intuitively, just as risk
aversion is not the same as declining marginal utility of money, it is
also not the same as fear of regret or frustration. If you are risk
averse, then one of the things you care about is safety. Given that
outcomes should specify everything the agent cares about -- all her
desires or motives -- we should therefore include the safety or
riskiness of a choice in its outcomes.

Nonetheless, the vast majority of decision theorists assume that
outcomes must be individuated locally. The assumption is so common
that it doesn't even have a name; let's call it
\textbf{localism}. According to localism, in cases where local
features like frustration or regret can't explain the Allais
preferences, these preferences really do contradict the MEU
Principle. We must either declare the preferences irrational or revise
the MEU Principle.

Many such revisions to the MEU Principle have been proposed. An
especially elegant recent example is Lara Buchak's \emph{Risk-Weighted
  Expected Utility Theory}. On Buchak's model, the choiceworthiness of an
act is not determined by the agent's credences and utilities, the
latter of which pertain only to local attributes of outcomes. Instead,
agents are also assumed to have a ``risk function'' representing their
attitudes towards the non-local features of safety and risk. Buchak
shows that if an agent satisfies a variant of Savage's axioms with a
weakened Independence Axiom, then they can be represented as
maximizing risk-weighted expected utility.

\cmnt{%
  
Lara Buchak notes that standard EU theory doesn't adequately handle
attitudes towards risk and suggests a more complex account on which
the ``risk-weighted expected utility'' of a gamble $[A ? x : y]$, with
$x<y$, is $x + r(P)(A)[y-x]$, rather than $x+P(A)[y-x]$. If $r$ is
convex, the agent is risk-averse.

One problem with this is that $r$ shouldn't depend only on $P$. E.g.,
Buchak's account implies that if I prefer a gamble between 1 util and
-1 util to the status quo, then I must also prefer a gamble between
100 utils and -100 utils to the status quo. But why can't I be more
risk-averse when more is at stake?

Arguably, it can also matter what the gamble is about. Maybe I like
gambles with money but not gambles with lifes or candy. 

There's a general problem here with the motivation for any such theory
(including ``standard'' EU theory): if we want to explain e.g.\ why
people gamble, surely we have to take into account the fact that they
retrieve pleasure from gambling; so the outcomes can't be
characterized in terms of history-free monetary payoffs. Similarly, in
Machina's example of the mother who prefers to give a present by a
fair (random) process, we have to build the method of decision into
the outcomes. But once we do that, why don't we always use this trick
to explain rational cases of risk-aversion or risk-seeking?

\bigskip

A. Sen has a nice example of a case where relational properties
matter: an agent who out of politeness takes the second-largest
cake. This is hard to explain by a simple preference order on
intrinsic features of the cakes. 

} %

But why should one go with localism in the first place? What is wrong
with a global individuation of outcomes?

\begin{exercise}
  Risk and fairness are two non-local attributes many people care
  about. Can you think of another such attribute? $\star\star$
  \cmnt{%
    On the face of it, this constraint looks implausible and
    arbitrary. Many people seem to care about non-local attributes,
    and not just about risk. For example, you might assign value to
    resisting temptations -- say, to stick with salad when all your
    friends are having fries. The value, we suppose, does not just lie
    in the likely consequences of eating salad compared to the
    consequences of eating fries. Nor does it lie in any feelings (of
    moral superiority, perhaps) you may or may not have as a result of
    the decision. It seems perfectly intelligible that you might put
    value directly on overcoming the temptation to have fries. To
    account for such values, outcomes cannot be individuated locally.

    Or suppose you care about whether your acts conform to certain
    rules of tradition, convention, or morality, or to promises you
    gave at an earlier point. These rules or promises might exclude
    buying a lottery ticket (even in a very favourable lottery); they
    might require you to leave the last slice of cake to others, and
    so on. Localism does not allow for such motives.
  } %
\end{exercise}

\cmnt{%
\begin{exercise}
  Does psychological hedonism conform to localism? That is, can we
  individuate outcomes locally to represent the motives of a hedonist
  agent? (Explain briefly.) $\star$
\end{exercise}
} %

\section{Localism}

\cmnt{%

  I should make clearer that localism is really a view on legitimate
  attributes.

  Maybe I should have raised the issue in the previous chapter of what
  attributes agents might care about. E.g., can we always set aside
  the past? No. ``Being the first to climb a mountain depends on the
  past.'' (Weirich)

} %

\cmnt{%
  Localism is a view about outcomes. To assess the view, we therefore
  have to think about the role of outcomes in expected utility theory.
} %

The formalism of expected utility theory was originally developed to
compute the monetary value of gambles: a gamble with expected payoff
£100 was assumed to be equivalent in value to £100. Bernoulli showed
that this assumption is false since money has declining marginal
utility. But Bernoulli did not reject expected utility
theory. Instead, he argued that we should distinguish between the
monetary payoff itself and the utility of that payoff for an agent. A
gamble is worth £100 if the gamble's expected \emph{utility} equals
the \emph{utility} of £100.

For a while, people then understood utility to measure the degree of
pleasure or welfare a choice might bring about. It was only in
response to the ordinalist challenge of the early 20th century that
utility was -- officially -- no longer defined as a measurable
quantity causally downstream from the choice, but as reflecting the
agent's motives or preferences before, or at the time of, the choice.

This second shift made it possible to regard expected utility theory
as a general model of practical rationality, without assuming that
rational agents only care about their personal pleasure or welfare.

But as we saw in chapter \ref{ch:utility}, the second shift never
fully caught on. Many authors still assume that utility is a measure
of pleasure or welfare or wealth. On that conception, if one act leads
to a particular amount of pleasure or welfare or wealth in a given
state of the world, and another act leads to the very same amount of
pleasure or welfare or wealth in some other state of the world, then
the utility of these outcomes is trivially the same. 
\cmnt{%

  Another bad reason is that the bearers of utility are standardly
  called `outcomes' or `rewards' or `prizes'. It is odd to describe an
  intrinsic feature of an act, such as its riskiness or its adherence
  to a moral rule, as an ``outcome'' of the act. In ordinary language,
  outcomes are causal consequences. The standard terminology of
  decision theory therefore makes localism sound like a tautology,
  while in fact it is a substantive assumption.

} %

So that is one explanation for the widespread acceptance of
localism. If we were interested in modelling selfish agents who only
care about their future pleasure or welfare, localism would be
harmless. But that is not our topic. We want to model agents who may
care about other things -- risk, for example, or fairness, or other
people's welfare.

\cmnt{%
  Is fairness an example, contra Buchak? Not clear: whether an act is
  fair arguably doesn't depend on the alternative acts that are
  available. (Well, but does riskiness?)
} %

\cmnt{%
  From a Humean perspective, Buchak's model is unsufficiently general
  because it assumes agents only care about one global feature: risk.%
} %

Even on the preference-based conception of utility, however, there is
some pressure to endorse localism. Recall that in von Neumann and
Morgenstern's method to determine the utility of some outcome $c$ in
comparison to other outcomes $a$ and $b$, we consider the agent's
preferences over certain lotteries between these outcomes -- for
example, whether she prefers a fair lottery between $a$ and $c$ to
$b$. But if $a$, $b$, and $c$ have non-local attributes, then these
lotteries may well be logically impossible.

The problem is that anything that comes about as the result of a
lottery inevitably has the non-local attribute of coming about as a
result of that lottery. So if one thing you value or disvalue about a
given outcome $c$ is that it doesn't come about as the result of a
chance process, then a lottery between $a$ and $c$ would have to be a
chance process that can lead to an outcome that doesn't come about
through a chance process. That kind of lottery is logically
impossible.

Things are even worse for Savage. Savage assumes that for any possible
outcomes $a$ and $b$ and state $S$ the agent has preferences
concerning prospects of the form $\bet{S}{a}{b}$. But if the agent is
allowed to care about arbitrary attributes, then one of these
attributes might well be (or entail) $S$. And if, say, $b$ entails
$S$, then there can't be any prospect $\bet{S}{a}{b}$ that leads to
$a$ if $S$ and to $b$ if $\neg S$. So Savage requires not just
localism but what is known as  ``state independence'' of utilities.

\cmnt{%
  If we individuate outcomes as including their entire histories, then
  the Independence axioms, among others, becomes empty, unless we
  allow for logically impossible outcomes.  %
} %

\cmnt{%

  But the conclusion is not inevitable. Some authors, Buchak for
  example, do think preferences can be defined for logically
  impossible events. Although that drives a further wedge between
  preferences and choice dispositions. One certainly never faces a
  choice between impossible options.

} %

The upshot is that if we are interested in a general model of
practical rationality, we arguably can't build that model on the
foundations of von Neumann and Morgenstern or Savage (or Ramsey). 

But we don't have to give up the idea that utilities (and credences)
might be derived from preferences, or that the MEU Principle might be
justified by more fundamental axioms concerning preferences. Other
foundations can be provided. The best known of these alternatives was
developed by Ethan Bolker and Richard Jeffrey in the 1960s.

In the Bolker/Jeffrey construction, the objects of utility are taken
to be arbitrary propositions, not ``rewards'' or ``outcomes'' or
lotteries between rewards or outcomes. No restrictions at all are
imposed on an agent's basic values. Like Savage, Bolker and Jeffrey
show that if an agent's preferences between propositions satisfy
certain axioms, then they can be represented by a probabilistic
credence function $\Cr$ and a utility function $U$ that satisfies
Jeffrey's axiom -- which, in turn, supports the MEU Principle by the
argument outlined in section \ref{sec:why-meu}. I will not explain all
the axioms or the details of the construction. I do want to mention
though that since preferences are defined between arbitrary
propositions, the gap between preferences and choice dispositions is
even larger in the Bolker/Jeffrey construction than it was for von
Neumann and Morgenstern or Savage. %
\cmnt{%
  for many propositions $A$ and $B$, the hypothesis that someone faces
  a choice between $A$ and $B$ will be logically impossible, because
  $A$ and $B$ entail that no-one ever faced such a choice. %
} %

This brings me to one final point in favour of localism. Localism
promises to give the MEU Principle predictive power. If the same
outcome never occurs in different decisions faced by an agent, we
could never find out, just from the agent's choices, that she violates
transitivity, independence, or any of the other classical axioms. No
matter what the agent does, there will always be some utility function
relative to which all her choices maximize expected utility. And if
outcomes can be individuated globally, then we can always find
differences between outcomes in different decision problems. Localism
promises to block this threat of trivialization.

But should we want to block the threat? Shouldn't we rather accept that on
a broadly Humean conception of practical rationality, no pattern of
behaviour could, all by itself, show that the agent is practically
irrational? Couldn't the agent have a basic desire to display just
this pattern of behaviour? It is not obvious that we should expect a
general theory of practical rationality to make testable predictions,
without any assumptions about the agent's beliefs and desires.

Localism in effect encodes one such assumption: that the agent only
cares about local features of outcomes. But the assumption is not only
implausible, it also hardly weakens the trivialization problem. For
even if we restrict ourselves to local properties, we can almost
always find differences between outcomes in different decision
problems. I suspect this explains why in practice, psychologists and
social scientists tend to fall back on the 19th century conception of
utility as a measure of personal wealth or welfare. That gives the MEU
Principle considerable predictive power. It also renders the principle
obviously false, both as a normative principle about what people
should do and as a descriptive principle about people's actual behaviour.

\cmnt{%
  But we have to appreciate that we can't make interesting predictions
  about choices and at the same time define people's goals through
  their choices.%
} %

\cmnt{%
  It seems to me that there is a better response: make assumptions
  about what people actually prefer. These don't need to be normative
  assumptions.
} %

\cmnt{%
  Broome's idea that outcomes should be distinguished if it is
  rational to care differently about them is similar to Weatherson's
  idea to use evidential probability rather than credence to define
  EU. In either case a norm on attitudes is folded into the decision
  rule. It's better to keep these apart: there's what you actually
  believe, what you actually desire, and what you should do in light
  of these attitudes, whether they are rational or not. Here we want
  to remain largely neutral on what the attitudes may be. Then there's
  the question of whether your attitudes are reasonable. In practice,
  holding on to this division of labour is not entirely possible
  because it's hard to spell out decision rules for incoherent
  agents. So we'll need to impose probabilism, transitivity of
  preferences etc.
} %

\section{Further reading}

Some good (and rare) discussion of the localism issue can be found in

\begin{itemize}
\item Jamie Dreier: ``Rational preference: Decision theory as a theory of practical rationality'' (1996), 
\item Lara Buchak: ``Redescription'', chapter 4 of her \emph{Risk and Rationality} (2013),
\item Paul Weirich: ``Expected Utility and Risk'' (1986).
\end{itemize}
%
If you want to know more about Buchak's risk-weighted expected utility
theory, have a look at

\begin{itemize}
\item Lara Buchak: \href{https://philosophy.berkeley.edu/file/754/Buchak_Risk_and_Tradeoffs.pdf}{``Risks and tradeoffs''}. (2014)
\end{itemize}

The Jeffrey/Bolker construction is described in
\begin{itemize}
\item Richard Jeffrey: \emph{The Logic of Decision}, chapter 9 (1983).
\end{itemize}

The paper by Harsanyi mentioned in section \ref{sec:why-meu} is
\begin{itemize}
\item John Harsanyi: ``Rule utilitarianism, rights, obligations and the theory of rational behavior'' (1980).
\end{itemize}

\begin{essay}
  Discuss Buchak's defence of localism in the ``Redescription''
  chapter. What are her main arguments? Can you think of objections or
  further support?
\end{essay}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End: