\chapter{Evidential and Causal Decision Theory}\label{ch:cdt}

\section{Evidential decision theory}\label{sec:edt}

The traditional method for evaluating an agent's options in a decision
situation begins by setting up a decision matrix which identifies the
relevant states, acts, and outcomes. The expected utility of each act
is then computed as the weighted average of the utility of the
possible outcomes, weighted by the probability of the corresponding
states.

Finding an adequate decision matrix is not always easy. Among other
things, we have to make sure that the propositions we choose as the
states are independent of the acts. Exercise \ref{e:exam} illustrated
why this is needed: a student, wondering whether to study for an exam,
drew up the following matrix.
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr Will Pass (0.5) & \gr Won't Pass (0.5) \\\hline
    \gr Study & Pass \& No Fun (1) & Fail \& No Fun (-8) \\\hline
    \gr Don't Study & Pass \& Fun (5) & Fail \& Fun (-2) \\\hline
  \end{tabular}
\end{center}
To the student's delight, the expected utility of studying (-3.5) comes out
lower than that of not studying (1.5). This is so even if studying would
dramatically improves the chances of passing. The student's calculation is wrong
because the states `Will Pass' and `Won't Pass' are not independent of the acts.

What exactly does independence require? There are several notions
of independence.
\begin{itemize}
\itemsep0em
\item Two propositions $A$ and $B$ are \textbf{logically independent}
  if all the combinations $A \land B$, $A \land \neg B$, $\neg A \land
  B$, and $\neg A \land \neg B$ are logically possible.
\item $A$ and $B$ are \textbf{probabilistically independent} relative
  to some credence function $\Cr$ if $\Cr(B/A) = \Cr(B)$. (See section
  \ref{sec:conditional}.)  
  \item $A$ and $B$ are \textbf{causally independent} if, whether one of them is
    true has no causal influence over whether the other is true.
\end{itemize}

\begin{exercise}{1}
  In which of the three senses are the states in the student's decision
  matrix (`Will Pass', `Won't Pass') independent of the acts?
\end{exercise}

When we require that the states in a matrix should be independent of the acts,
we don't just mean logical independence. But it is not obvious whether we should
require probabilistic independence or causal independence. The question turns
out to mark the difference between two fundamentally different approaches to
rational choice. If we require probabilistic independence (also known as
`evidential independence'), we get \textbf{evidential decision theory} (EDT, for
short). If we require causal independence, we get \textbf{causal decision
  theory} (CDT).

Both forms of decision theory say that rational agents maximize
expected utility, and they both appear to use the same definition of
expected utility: if act $A$ leads to outcomes $O_1,\ldots,O_n$
in states $S_1,\ldots,S_n$ respectively, then
\[
EU(A) =_\text{def} U(O_1) \cdot \Cr(S_1) + \ldots + U(O_n) \cdot \Cr(S_n).
\]
But EDT and CDT disagree on what counts as an adequate state. Each
camp accuses the other of making a similar mistake as the student who
used `Will Pass' and `Won't Pass' as the states. If we require states
to be probabilistically independent of the acts, the definition
defines \textbf{evidential expected utility}; if we require causal
independence, it defines \textbf{causal expected utility}.

\cmnt{%
  Remember that we are interested in what an agent should do from her
  own perspective. So when CDT says that the states should be causally
  independent of the acts, it really means that the agent should be
  \emph{certain} that the states are causally independent of the
  acts. It is not enough that (unbeknown to the agent) the states are
  in fact causally independent of the acts.
} %

\cmnt{%
  Often some ingenuity is required to find act-independent states. In
  the student's problem, distinguishing three possibilities about the
  exam would do the trick. One possibility is that the exam is
  \emph{easy}, so that the student will pass whether or not she
  learns; the second possibility is that the exam is \emph{hard}, so
  that the student will pass only if she learns; the third possibility
  is that the exam is \emph{very hard} so that the student will fail
  even if she learns. Which of these possibilities obtains is causally
  independent of whether the student studies. An adequate decision
  matrix might therefore look like this.
  % 
  \begin{center}
    \begin{tabular}{|r|c|c|c|}\hline
      \gr & \gr Exam easy (0.2) & \gr Exam hard (0.7) & \gr Exam very hard (0.1) \\\hline
      \gr Study & Pass \& No Fun (1) & Pass \& No Fun (1) & Fail \& No Fun (-8) \\\hline
      \gr Don't Study & Pass \& Fun (5) & Fail \& Fun (-2) & Fail \& Fun (-2) \\\hline
    \end{tabular}
  \end{center}

  In the new matrix, not studying no longer dominates. Indeed,
  studying has expected utility $1 \cdot 0.2 + 1 \cdot 0.7 + (-8) \cdot
  0.1 = 0.1$, while not studying has expected utility $5 \cdot 0.2 + (-2)
  \cdot 0.7 + (-2) \cdot 0.1 = -0.6$. So studying maximizes expected utility.
} %

Before we look at examples where the two notions come apart, I want to
mention three advantages of the evidential conception.

First, probabilistic independence is much better understood than
causal independence. Provided $\Cr(B) > 0$, probabilistic independence
between $A$ and $B$ simply means that $\Cr(A) = \Cr(A \land
B)/\Cr(B)$. By contrast, ever since Hume philosophers have argued that
our conception of causality or causal influence is highly
problematic. Bertrand Russell, for example, held that ``the word
`cause' is so inextricably bound up with misleading associations as to
make its complete extrusion from the philosophical vocabulary
desirable.'' All else equal, it would be nice if we could keep causal
notions out of our model of rational choice.

A second advantage of EDT is that it allows us to compute expected
utilities in a way that is often simpler and more intuitive than the
method we've used so far.

Return to the student's matrix. Intuitively, the problem with the
matrix is that the `Will Pass' state is more likely if the student
studies than if she doesn't study. When we evaluate the expected
utility of studying, we should therefore give greater weight to worlds
in which she passes than when we evaluate the expected utility of not
studying.

This suggests that instead of finding a description of the student's
decision problem with act-independent states, we might stick with the
student's original description, but let the probability of the states
vary with the acts. Like so:
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr Will Pass & \gr Won't Pass \\\hline
    \gr Study & Pass \& No Fun & Fail \& No Fun \\
    \gr & ($U=1, \Cr=0.9$) & ($U=-8, \Cr=0.1$) \\\hline
    \gr Don't Study & Pass \& Fun & Fail \& Fun  \\
    \gr  & ($U=5, \Cr=0.2$) & ($U=-2, \Cr=0.8$) \\\hline
  \end{tabular}
\end{center}
%
`$\text{Cr}=0.9$' in the top left cell indicates that the student is 90\%
confident that she will pass \emph{if she studies}. By contrast, she
is only 20\% confident that she will pass \emph{if she doesn't study},
as indicated by `$\Cr=0.2$' in the bottom left cell. We no longer care
about the absolute, unconditional probability of the states. To
compute the expected utility of each act we simply multiply the
utilities and credences in each cell and add up the products. So the
expected utility of studying is $1 \cdot 0.9 + (-8) \cdot 0.1 = 0.1$;
for not studying we get $5 \cdot 0.2 + (-2) \cdot 0.8 = -0.6$.

In general, the \textbf{new method} for computing expected utilities
goes as follows. As before, we set up a decision matrix that
distinguishes all relevant acts and outcomes, but we no longer care
whether the states are independent of the acts (in any sense). If an
act $A$ leads to outcomes $O_1,\ldots,O_n$ in states $S_1,\ldots,S_n$
respectively, then the expected utility of $A$ is computed as
\[
EU(A) = U(O_1) \cdot \Cr(S_1/A) + \ldots + U(O_n) \cdot \Cr(S_n/A).
\]
The unconditional credences $\Cr(S_i)$ in the old method have
been replaced by conditional credences $\Cr(S_i/A)$.

\cmnt{%

  Here is the general rule for computing expected utilities by this
  new method.
  \begin{genericthm}{Conditional expected utility}
    If there are $n$ states $S_1,\ldots, S_n$ (which may or may not be
    independent of the acts), and a given act $A$ leads to outcomes
    $O_1,\ldots,O_n$ respectively in these states, then the
    \textbf{conditional expected utility} of $A$ is defined as
    \[
    CEU(A) = \Cr(S_1/A) \cdot U(O_1) + \ldots + \Cr(S_n/A) \cdot U(O_n).
    \]
  \end{genericthm}

  I say `conditional expected utility' rather than `expected utility'
  because we have yet to figure out whether $CEU(A)$ equals the quantity
  we previously introduced as `$EU(A)$'.

} %

\begin{exercise}{2}
  You have a choice of going to party $A$ or party $B$. You prefer
  party $A$, but you'd rather not go to a party if Bob is there. Bob,
  however, wants to go where you are, and there's a 50\% chance that
  he will find out where you go. If he does, he will come to the same
  party, otherwise he will randomly choose one of the two
  parties. Here is a matrix for your decision problem.
  \vspace{-0.2em}
  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
      \gr & \gr Bob at $A$ (0.5) & \gr Bob at $B$ (0.5) \\\hline
      \gr Go to $A$ & Some fun (1) & Great fun (5) \\\hline
      \gr Go to $B$ & Moderate fun (3) & No fun (0) \\\hline
    \end{tabular}
  \end{center}
  \vspace{-1.2em}
  \begin{enumerate}
    \itemsep0em
  \item[(a)] Explain why this is not an adequate matrix for computing
    expected utilities by the old method.
  \item[(b)] Use the new method to compute the expected utilities.
  \end{enumerate}
  \vspace{-2em}
\end{exercise}

% A nice feature of the new method is that the expected utility of an
% act turns out to not depend on which states we've used in the
% computation.
%as long as the states distinguish all possible outcomes
%of the act (so that for each state, there is exactly one outcome that
%is certain to result from the act in that state).
% This is known as \textbf{partition invariance}.

We can go one step further. Suppose the outcomes $O_1,\ldots,O_n$ that might
result from act $A$ are all distinct, so that $A$ leads to a outcome $O_1$ only
if $S_1$ obtains, to outcome $O_2$ only if $S_2$ obtains, and so on. On the
assumption that the agent chooses $A$, it is then certain that $S_1$ obtains iff
$O_1$ occurs, that $S_2$ obtains iff $O_2$ occurs, and so on. It follows that
$\Cr(S_1/A) = \Cr(O_1/A)$, $\Cr(S_2/A) = \Cr(O_2/A)$, etc. Substituting these
terms in the new formula for $EU(A)$, we get
\[
EU(A) = U(O_1) \cdot \Cr(O_1/A) + \ldots + U(O_n) \cdot \Cr(O_n/A).
\]
So we can equivalently compute the expected utility of $A$ directly in
terms of outcomes, without even mentioning any states.

To derive the equivalence, I have assumed that the same outcome never
occurs in different states. But the equivalence still holds if we drop
that assumption. For example, suppose $A$ leads to $O_1$ in both $S_1$
and $S_2$ (and in no other state). On the assumption that the agent
chooses $A$, it is then certain that $O_1$ is true iff $S_1\lor S_2$
is true. Since the states are mutually incompatible, it follows that
$\Cr(O_1/A) = \Cr(S_1/A)+\Cr(S_2/A)$. And this means that we can
substitute
\[
  U(O_1)\cdot \Cr(S_1/A) + U(O_1)\cdot \Cr(S_2/A)
\]
in the new formula for expected utility by
\[
  U(O_1)\cdot \Cr(O_1/A).
\]

% The preceding para could be an exercise.

The upshot is that the ``new method'' is equivalent to a \textbf{state-free method}
for computing expected utilities. Here we only need to figure out all the
outcomes $O_1,\ldots, O_n$ that a given act might bring about. We then consider
how likely each of these outcomes is on the supposition that the act is chosen,
and take the sum of the products:
%
\[
EU(A) = U(O_1) \cdot \Cr(O_1/A) + \ldots + U(O_n) \cdot \Cr(O_n/A).
\]
In practice, this method is often simpler and more intuitive than
the old method.

\begin{exercise}{1}
  You have two options. You can get £10 for sure (utility 1), or flip
  a fair coin and get £20 on heads (utility 1.8) or £0 on tails
  (utility 0). The coin will not be flipped if you take the £10.  In
  cases like this, it is hard to find a suitable set of states. Use
  the state-free method.
\end{exercise}

I have shown that the ``new method'' and the state-free method always
yield the same result. I will now show that if we compute the expected
utility of an act by one of these methods, what we get is the act's
\emph{evidential} expected utility, as defined above. The
user-friendliness of the new methods is therefore an argument in
favour of EDT.

The proof is easy. The evidential expected utility of an act $A$ is
defined as
\[
U(O_1) \cdot \Cr(S_1) + \ldots + U(O_n) \cdot \Cr(S_n),
\]
where $O_1,\ldots,O_n$ are the outcomes that result from $A$ in states
$S_1,\ldots,S_n$ respectively; the states are probabilistically independent of
the acts. By the new method, we would instead compute the expected utility as
\[
U(O_1) \cdot \Cr(S_1/A) + \ldots + U(O_n) \cdot \Cr(S_n/A).
\]
But since the states are probabilistically independent of the acts, in
all these terms, $\Cr(S_i) = \Cr(S_i/A)$. So the two methods here
yield the same result. But we know that \emph{any} application of the
new method yields the same result as the state-free method. It follows
that any application of either the new method or the state-free method
yields the same result as the old method as understood by EDT.

\cmnt{%

  Why do we have partition independence?

  Let's say we have two outcomes; compare two partitions:

  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
      \gr & \gr S1 & \gr S2 \\\hline
      \gr $A$ & O1 & O2 \\\hline
      \gr $B$ & O3 & O4 \\\hline
    \end{tabular}
    \begin{tabular}{|r|c|c|}\hline
      \gr & \gr S1' & \gr S2' & S3' \\\hline
      \gr $A$ & O1 & O2 & O2 \\\hline
      \gr $B$ & O3 & O3 & O4 \\\hline
    \end{tabular}
  \end{center}
  
  \[
  CEU(A) = \Cr(S1/A)U(O1) + \Cr(S2/A)U(O2).
  \]
  In effect, you multiply each outcome utility by the probability of
  getting that outcome conditional on the act. In the second table,
  U(O2) is multiplied by $\Cr(S2'/A)$ and again by $\Cr(S3'/A)$ and
  the results are added, but that's equivalent to multiplying $U(O2)$
  by the sum $\Cr(S2') + \Cr(S3'/A)$, which equals $\Cr(S2'\lor
  S3'/A)$, which equals $\Cr(O2/A)$.

} %

\cmnt{%
\begin{exercise}
  In example \ref{ex:diamond}, a mother has a choice between ($A$)
  giving a treat to Abbie, ($B$) giving the treat to Ben, or ($C$)
  tossing a coin and giving the treat to Abbie on heads or to Ben on
  tails. Suppose she will only toss the coin if she goes for option
  $C$. What is wrong with the following decision matrix?
  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
      \gr & \gr Heads & \gr Tails \\\hline
      \gr $A$ & Abbie gets treat  & Abbie gets treat \\\hline
      \gr $B$ & Ben gets treat & Ben gets treat \\\hline
      \gr $C$ & Abbie gets treat & Ben gets treat \\\hline
    \end{tabular}
  \end{center}
\end{exercise}
\begin{exercise}
  Apply this formula to the mother case and confirm that you get the
  same result.
\end{exercise}
} %

A third advantage of EDT is that it provides a powerful argument in support of
the MEU Principle. (I mentioned this argument in section \ref{sec:why-meu}.) The
argument is that the evidential \emph{expected utility} of an act equals the
act's \emph{utility}. The principle to maximize evidential expected utility is
therefore equivalent to the principle that one should choose acts that are most
desirable in light of one's total beliefs and desires. And that sounds very
plausible.

We can use the state-free method for computing evidential expected utility to
see why an act's utility equals its evidential expected utility. Suppose $A$ has
$O_1,\ldots, O_n$ as (distinct) possible outcomes. Then $A$ is logically
equivalent to the disjunction of all conjunctions of $A$ with the outcomes:
$(A \land O_1) \lor \ldots \lor (A \land O_n)$.
% I should have let students prove this earlier!
By Jeffrey's Axiom for utility,
\[
U(A) = U(A\land O_1) \cdot \Cr(A \land O_1/A) + \ldots +
       U(A\land O_n) \cdot \Cr(A \land O_n/A).
\]
Since $\Cr(A \land O_i/A) = \Cr(O_i/A)$, this simplifies to
\[
U(A) = U(A\land O_1) \cdot \Cr(O_1/A) + \ldots +
       U(A\land O_n) \cdot \Cr(O_n/A).
\]
Moreover, if the outcomes specify everything that matters to the
agent, then $U(A \land O_i) = U(O_i)$. So we can simplify once more:
\[
U(A) = U(O_1) \cdot \Cr(O_1/A) + \ldots + U(O_n) \cdot \Cr(O_n/A).
\]
The right-hand side is the state-free definition of evidential expected utility.

\cmnt{%
  It's not ideal that vNM etc also appeal to $U(A)$...
} %

\cmnt{%
  Why do we get partition invariance? xxx should have proved in
  ch:utility?
} %

\cmnt{%
\begin{exercise}
  We can also apply the new rule even if we don't have states that
  distinguish all relevant outcomes, assuming the treatment of utility
  from chapter \ref{ch:utility}. Suppose we use `easy' and `hard or
  very hard' as states. The ``outcome'' in the cell is the disjunction
  of the previous outcomes.
  
  \begin{center}
    \begin{tabular}{|r|c|c|c|}\hline
      \gr & \gr Exam easy or hard (0.9) \gr Exam very hard (0.1) \\\hline
      \gr Study & Pass \& No Fun (1) & Fail \& No Fun (-8) \\\hline
      \gr Don't Study & (Pass \& Fun) $\lor$ (Fail \& Fun) & Fail \& Fun (-2) \\\hline
    \end{tabular}
  \end{center}
  
  What is the utility of \emph{(Pass \& Fun) $\lor$ (Fail \& Fun)}? By
  Jeffrey's axiom, it is the weighted average of $U(\emph{Pass \& Fun})$
  and $U(\emph{Fail \& Fun}$, weighted by the conditional
  probability of the two outcomes:
  \[
  U(PF \lor FF) = U(PF)\cdot \Cr(PF/PF \lor FF) + U(FF)\cdot \Cr(FF/PF\lor FF).
  \]
  
\end{exercise}
} %

\section{Newcomb's Problem}

In 1960, the physicist William Newcomb invented the following puzzle.

\begin{example}[Newcomb's Problem]
  In front of you are a black box and a transparent box. You can see that the
  transparent box contains £1000. You can't see what's in the black box. You
  have two options. You can take (only) the black box and keep whatever is
  inside. Alternatively, you can take both boxes keep their content. A demon
  has tried to predict what you will do. If she predicted that you will take
  both boxes, then she put nothing in the black box. If she predicted that you
  will take just the black box, she put £1,000,000 in the box. The demon is very
  good at predicting this kind of choice. Your options have been offered to many
  people in the past, and the demon's predictions have almost always been
  correct.
\end{example}
%
What should you do, assuming you want to get as much money as
possible?

Let's see how EDT and CDT answer the question, starting with CDT. If
you only care about how much money you will get, then the following
matrix adequately represents your decision problem, according to CDT.
%
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr £1,000,000 in black box & \gr £0 in black box \\\hline
    \gr Take only black box & £1,000,000 & £0 \\\hline
    \gr Take both boxes & £1,001,000 & £1000 \\\hline
  \end{tabular}
\end{center}
%
Note that the states are causally independent of the acts, as CDT
requires: whether you take both boxes or just the black box -- in
philosophy jargon, whether you \emph{two-box} or \emph{one-box} -- is
certain to have no causal influence over what's in the boxes. This is
crucial to understanding Newcomb's Problem. By the time of your
choice, the content of the boxes is settled. The demon won't magically
change what's in the black box in response to your choice; her only
superpower is predicting people's choices.

It is obvious from the decision matrix that taking both boxes
maximizes causal expected utility, since it dominates one-boxing: it
is better in every state. We don't need to fill in the precise
utilities and probabilities.

Turning to EDT, we do need to specify a few more details. Let's say
you are 95\% confident that there is a million in the black box if you
one-box, and 5\% confident that there is a million in the black box if
you two-box. Let's also assume (for simplicity) that your utility is
proportional to the amount of money you will get. Using the ``new
method'', the evidential expected utility of the two options then works
out as follows (`1B' is one-boxing, `2B' is two-boxing):
\begin{align*}
  EU(\text{1B}) &= U(\text{£1,000,000})\cdot
  \Cr(\text{£1,000,000}/\text{1B}) + U(\text{£0})\cdot
  \Cr(\text{£0}/\text{1B})\\
  &= 1,000,000 \cdot 0.95 + 0 \cdot 0.05 = 950,000.\\
  EU(\text{2B}) &= U(\text{£1,001,000})\cdot
  \Cr(\text{£1,001,000}/\text{2B}) + U(\text{£1000})\cdot
  \Cr(\text{£1000}/\text{2B})\\
  &= 1,001,000 \cdot 0.05 + 1000 \cdot 0.95 = 51,000.
\end{align*}
One-boxing comes out as significantly better than two-boxing.

So CDT says that you should two-box, and EDT says you should
one-box. Who has it right? Philosophers have been debating the
question for over 50 years, with no consensus in sight.

Some think one-boxing is obviously the right choice. After all, you're
almost certain to get more if you one-box than if you two-box. Look at
all the people that have been offered the choice in the past. Those
who one-boxed almost always walked away with a million, while those
who two-boxed walked away with a thousand. Wouldn't you rather be in
the first group than in the second? It's your choice!

\cmnt{%
  Another argument for one-boxing is that (as we have seen) it is
  supported by what is widely agreed to be the conceptually clearest
  and most elegant form of decision theory -- Evidential Decision
  Theory. %
} %

Others think it equally obvious that you should take both boxes. If
you take both boxes, you are guaranteed to get £1000 more than
whatever you'd get if you took just the black box. Remember that the
content of the boxes is settled. The black box either contains a
thousand or a million. And since one-boxing and two-boxing will both
give you the black box, it is settled that you will get however much
is in that box. The only thing that isn't settled -- the only thing
over which you have any control -- is whether you also get the £1000
from the transparent box. And if you prefer more money to less money,
then clearly (so the argument) you should take the additional £1000.

The argument for two-boxing can be strengthened by the following
observation. Imagine you have a friend who helped the demon prepare
the boxes. Your friend knows what's in the black box. You've agreed to
a secret signal by which she will let you know whether it would be
better for you to choose both boxes or just the black box. If you
trust your friend, it seems that you should follow her advice. But
what will she signal? If the box is empty, she will advise you to take
both boxes, so that you get at least the thousand. If the box contains
a million, she will also advise you to take both boxes, so that you
get £1,001,000 rather than £1,000,000. Either way, she will signal to
you that you should take both boxes. But this means you can follow
your friend's advice without even looking at her signal. Indeed, you
can (and ought to) follow her advice even if she doesn't actually
exist.

Why should you follow the advice of your imaginary friend? Think about
why we introduced the notion of expected utility in the first place.
In chapter \ref{ch:overview}, we distinguished between what an agent
ought to do \emph{in light of all the facts}, and what she ought to do
\emph{in light of her beliefs}. In the miner problem (example
\ref{ex:miners}), the best choice in light of all the facts is to
block whichever shaft the miners are actually in. But since you don't
know where the miners are, you don't know what would be the best
choice in light of all the facts. You have to go by the limited
information you have. The best choice in light of that information is
arguably to block neither shaft. But in Newcomb's problem, you
actually know what is best in light of all the facts: you know what
someone who knows all relevant facts would advise you to do. She would
advise you to two-box. (Equivalently, you know what you would decide
to do if \emph{you} knew what's in the black box: you would decide to
two-box.) Plausibly, if you are certain that some act is best in light
of all the facts, then you should choose that act.

\begin{exercise}{2}
  Show that if you follow EDT, you would not want to know what's in
  the black box. You'd be willing to pay the demon £500 for not
  revealing to you the content of the box. 
\end{exercise}

What about the fact that one-boxers are generally richer than
two-boxers? Doesn't that show that the one-boxers are doing something
right? Not so, say those who advocate two-boxing. The two-boxers who
walked away with a mere thousand were never given a chance to get a
million. They were confronted with an empty black box and a transparent
box containing £1000; it's hardly their fault that they didn't get a
million. On the other hand, all those one-boxers who got a million
were effectively given a choice between £1,001,000 and £1,000,000. The
fact that they got a million hardly shows that they made the right
choice. As an analogy, imagine there are two buttons labelled `dark'
and `blonde'. If you press the button that matches your hair colour,
you get a million if your hair is blonde and a thousand if it is
dark. Almost everyone who presses `blonde' walks away with a million,
while almost everyone who presses `dark' walks away with a
thousand. It clearly doesn't follow that everyone should have pressed
`blonde'. Those with dark hair never had a chance to get a million.

\cmnt{%
  For responses to why ain'cha rich, see Gibbard and Harper, 1978,
  p. 153; Joyce, 1999, pp. 151–154; Arntzenius, 2008, pp. 289–290.

  It's clear that success is not always a sign of having made the
  right choice. Adam Bales mentions presenting people two buttons
  labelled black and blonde. If they press the button matching their
  hair colour, they get a million if they have black hair and 100 if
  they are blonde. Almost everyone who presses black will be
  rich. Surely it doesn't follow that pressing black is the right
  choice. A comparison of returns must be fair. For the causalist,
  this means that the circumstances must be the same for all agents
  concerning everything the agent's choice makes a difference to. The
  evidentialist appeals to a different criterion of fairness. That's
  why the disagreement is so often called a stalemate.

  Also it clearly can pay off to be irrational, e.g. when making threats.
} %

\section{More realistic Newcomb Problems?}

Newcomb's Problem is  science fiction. Nobody ever faces that
situation. Why should we care about the answer?

Philosophers care because the problem brings to light a more
general issue: whether the norms of practical rationality must
involve causal notions. Those who favour two-boxing in Newcomb's
Problem argue that the apparent advantage of EDT, that it does not
appeal to causal notions, is actually a flaw. In effect, EDT
recommends choosing acts whose choice would be good news. One-boxing
in Newcomb's Problem would be good news because it would provide
strong evidence that the black box (which you're certain to get)
contains a million. By contrast, two-boxing would provide strong
evidence that the black box is empty; it would be bad news. But the aim
of rational choice, say advocates of CDT, is to \emph{bring about good
  outcomes}, not to \emph{receive good news}. In Newcomb's Problem,
one-boxing is evidence for something good, but it does not contribute in
any way to bringing about that good. If the million is in the black
box, then it got in there long before you made your choice.

This difference between EDT and CDT can also show up in more
realistic scenarios. Some versions of the Prisoner's Dilemma (example
\ref{ex:pd}) are plausible candidates. Suppose you only care about
your own prison term. We can then represent the Prisoner's Dilemma by
the following matrix, in which the ``states'' (your partner's options)
are causally independent of the acts.
%
\begin{center}
  \begin{tabular}{|r|c|c|}\hline
    \gr & \gr Partner confesses & \gr Partner silent\\\hline
    \gr Confess & 5 years (-5) & 0 years (0) \\\hline
    \gr Remain silent & 8 years (-8) & 1 year (-1) \\\hline
  \end{tabular}
\end{center}
%
No matter what your partner does, confessing leads to the better
outcome. But now suppose your partner is in certain respects much like
you, so that she is likely to arrive at the same decision as
you. Concretely, suppose you are 80\% confident that your partner will
choose whatever you will choose, so that $\Cr(\text{she
  confesses}/\text{you confess}) = \Cr(\text{she is silent}/\text{you
  are silent}) = 0.8$. As you can check, EDT then recommends remaining
silent. Friends of CDT think that this is wrong. Under the given
assumptions, remaining silent is good news, as it indicates that your
partner will also remain silent -- and note how much better the
right-hand column is than the left-hand column. But that is no reason
for you to remain silent.

\begin{exercise}{1}
  Compute the evidential expected utility of confessing and remaining
  silent. 
\end{exercise}

Another potential example are so-called \textbf{Medical Newcomb
  problems}. In the 1950s, it became widely known that the cancer rate
is a lot higher among smokers than among non-smokers. Fearing that a
causal link between smoking and cancer would hurt their profits,
tobacco companies promoted an alternative explanation for the
finding. The correlation between smoking and cancer, they suggested,
is due to a common cause: a genetic disposition that causes both a
desire to smoke and cancer. The cancer, on that explanation, isn't
caused by smoking, but directly by the genetic factors that happen
to also cause smoking.

Why would the tobacco industry be interested in promoting this
hypothesis? Because they assumed that if people believed that smoking
does not actually increase the risk of cancer, but merely indicates a
genetic predisposition for cancer, then people would keep smoking.
According to EDT, however, it seems that people should give up smoking
either way, for on either hypothesis smoking is bad news.

Let's work through an example. Suppose you assign some (sub)value to
smoking, but greater (sub)value to not having cancer, so that your
utilities for the possible combinations of smoking and getting cancer
are as follows:
%
\begin{align*}
U(\text{smoking} \land \neg\text{cancer}) &= 1\\
U(\neg \text{smoking} \land \neg\text{cancer}) &= 0\\
U(\text{smoking} \land \text{cancer}) &= -9\\
U(\neg\text{smoking} \land \text{cancer}) &= -10
\end{align*}
Suppose you are convinced by the tobacco industry's explanation: you
are sure that smoking does not cause cancer. But you think
smoking is evidence for the cancer gene. So
$\Cr(\text{cancer}/\text{smoking}) > \Cr(\text{cancer}/\neg
\text{smoking})$. Let's say $\Cr(\text{cancer}/\text{smoking}) = 0.8$
and $\Cr(\text{cancer}/\neg\text{smoking}) = 0.2$. It follows that the
evidential expected utility of smoking is $-9 \cdot 0.8 + 1 \cdot 0.2
= -7$, while the evidential expected utility of not smoking is $-10
\cdot 0.2 + 0 \cdot 0.2 = -2$.  According to EDT, then, you should
stop smoking even if you buy the tobacco industry's
explanation. Indeed, it should make no difference to you whether
smoking causes cancer or merely indicates a predisposition for cancer.

That is not what the tobacco industry expected. And it does seem
odd. In the example, you are sure that smoking will not bring about
anything bad. On the contrary, it is guaranteed to make things
better. At the same time, it would be evidence that you have the
cancer gene. By not smoking, you can suppress this piece of evidence.
But you can't affect the likelihood of getting cancer. If what you
really care about is whether or not you get cancer, rather than
whether or not you \emph{know} that you get cancer, what's the point
of making your life worse by suppressing the evidence?

Friends of EDT have a response to this kind of example. If the case is
to be realistic, they have argued, smoking actually won't be evidence
for cancer: $\Cr(\text{cancer}/\text{smoking})$ won't be greater than
$\Cr(\text{cancer}/\neg \text{smoking})$. Why is that? We have assumed that the
gene causes smoking by causing a desire to smoke. But suppose you feel
a strong desire to smoke. That desire provides evidence that you have
the gene. Acting on the desire would provide no further
evidence. Similarly if you don't feel a desire to smoke: not feeling
the desire is evidence that you don't have the gene, and neither
smoking nor not smoking then provides any further evidence. So once
you've taken into account the information you get from the presence or
absence of the desire,
$\Cr(\text{cancer}/\text{smoking})=\Cr(\text{cancer}/\neg
\text{smoking})$, and then EDT recommends smoking.

This response has come to be known as the ``tickle defence'' of EDT,
because it assumes that the cancer gene would cause a noticeable
``tickle'' whose presence or absence provides all the relevant
evidence. 

\begin{exercise}{1}
  You wonder whether to vote in a large election between two
  candidates $A$ and $B$. You assign (sub)value 100 to $A$ winning and
  0 to $B$ winning. Voting would add a (sub)value of -1, since it
  would cause you some inconvenience. Your credence that your vote
  will make a difference is 0.001. You figure out that not voting
  maximizes expected utility. But then you realize that other
  potential voters are likely to go through the same thought process
  as you. You estimate that around 1\% supporters of $A$ might go
  through the same process of deliberation as you, and will reach the
  same conclusion that you will reach. Does that change the causal
  expected utility of voting?  Does it change the evidential expected
  utility? (Explain briefly, without computing anything.) 
\end{exercise}

\cmnt{%
     make diff | not make diff
  v    +99         -1 
 -v    0            0
} %


\cmnt{%
  One response to the tickle defence is that there needn't be a
  noticeable tickle. Many of our motives or desires are plausibly
  unconscious: we might only become aware of them by reflecting on our
  choices. For example, it has been argued that even people with no
  conscious prejudices against women or racial minorities often have
  an \textbf{implicit bias} against these groups that becomes apparent
  in their behaviour. Couldn't the cancer gene cause an implicit bias
  in favour of smoking?%
} %

\cmnt{%

  There are many reasons that could motivate a person to smoke, even
  if she doesn't have the gene: a desire to mirror the behaviour of
  one's friends, a desire to be perceived as cool or rebellious, and
  so on. Arguably, we are often mistaken about our true motives. So it
  is not out of the question that a gene could cause a basic desire to
  smoke which the agent rationalizes as another kind of desire (for
  conformity, say) that would not be evidence for the gene. xxx ok,
  but then smoking is not evidence for cancer!

} %

\cmnt{%
  But it is not obvious that there would have to be a noticeable
  tickle. There are many reasons that could motivate a person to
  smoke, even if she doesn't have the gene: a desire to mirror the
  behaviour of one's friends, a desire to be perceived as cool or
  rebellious, and so on. Arguably, we are often mistaken about our
  true motives. So it is not out of the question that a gene could
  cause a basic desire to smoke which the agent rationalizes as
  another kind of desire (for conformity, say) that would not be
  evidence for the gene. xxx ok, but then smoking is not evidence for
  cancer!%
} %

\cmnt{%
  Now I can also tell you \emph{how} the gene affects people's
  choices. It does not affect their desires. If those who carry the
  gene would find the soup unappealing, then of course they shouldn't
  eat it. (xxx well suppose only those who carry it find it
  appealing!) The puzzle assumes that the soup is equally appealing
  whether or not you carry the gene. Hence you can't figure out
  whether you carry the gene from the fact that you find it
  appealing. The gene also doesn't affect people's motor control. It's
  not that those who carry it decide to accept the offer but fail to
  control their behaviour and end up saying ``no'' anyway. What the
  gene does is simply that it makes people disposed to follow the

  principle of maximising indicative value.
} %

\cmnt{%
  It is easy to imagine a creature facing a decision without being
  aware of her desires; that is, without being certain that she has
  the desires which she actually has. Of course, such a creature
  couldn't consciously apply the decision-theoretic calculations. But
  remember that decision theory does not require such conscious
  calculations. A decision-theoretically rational creature could
  delegate her decision to a subconscious module that has full access
  to the creature's beliefs and desires. If such a creature faced the
  brussel sprouts situation, and the module uses EDT, it would decide
  not to eat the sprouts.

  Re (b), there are cases where the actual decision is stronger
  evidence for something bad than the desires that lead to it. This
  can only (?)  happen if the agent is not absolutely certain that her
  choice is entirely determined by her beliefs and desires. xxx van
  Fraassen. (Doesn't this require that the agent can fail to carry
  through on a decision, and so that `acts' are not mere exercises of
  the will?)

  In practice, such cases are rare, and so EDT and CDT almost always
  issue the same recommendation. But from the perspective of CDT, this
  is just a lucky coincidence. EDT gives the right recommendations for
  the wrong reasons.

} %

\section{Causal decision theories}

Those who are convinced by the case against EDT believe that some
causal notion must figure in an adequate theory of rational choice:
rational agents maximize causal expected utility.

One way to define causal utility is the classical definition in terms of states,
acts, and outcomes, where the states are required to be causally independent of
the acts. But we can also construct a version of CDT that looks more like EDT,
and shares at least some of EDT's attractive features. The key to this
construction is a point I briefly mentioned in section \ref{sec:conditional}:
that there are two ways of supposing a proposition.

What would have happened if the Nazi program to build nuclear weapons
had succeeded in 1944? When we contemplate this question, we consider
possible histories that are like the history of our world until 1944
but then depart in some minimal way to allow the Nazi's nuclear
weapon program to succeed. (The departure should be ``minimal''
because we're not interested in worlds where, say, the Nazis learned
how to build nuclear weapons from gigantic aliens who invaded the
Earth in 1944, destroying the continent of America as they landed.)
After that departure, the alternative histories should continue to
develop in accordance with the general laws of our world. Since
Hitler's character is the same in the alternative worlds and in the
actual world, it seems likely that Hitler would have used the nuclear
weapons, possibly leading to an Axis victory in World War II.

This is an example of \textbf{subjunctive supposition}. In general,
in subjunctively supposing an event, we consider what a world would
be like that closely resembles the actual world up to the relevant
time, then departs minimally to allow for the event, and afterwards
develops in accordance with the general laws of the actual world.
Subjunctive supposition is a causal kind of supposition. When
we subjunctively suppose that the Nazis had nuclear weapons, we
consider what this event \emph{would have brought about}.

By contrast, when we \textbf{indicatively suppose} that the Nazis had nuclear
weapons, we hypothetically add the supposed proposition to our beliefs and
revise the other beliefs in a minimal way to restore consistency. We ask what is
the case if the Nazis \emph{actually had} nuclear weapons in 1944. When
contemplating this question, we do not suspend our belief that the Nazis lost
the war, that they did not use nuclear weapons, etc. On the indicative
supposition that the Nazis had nuclear weapons in 1944, we conclude that
something prevented the use of the weapons -- an act of sabotage perhaps.

In a probabilistic framework, $\Cr(B/A)$ is an agent's credence in $B$ on the
indicative supposition that $A$; if $\Cr(A)>0$ it equals
$\Cr(B \land A)/\Cr(A)$. Let `$\Cr(B/\!/A)$' (with two dashes) denote an agent's
credence in $B$ on the \emph{subjunctive} supposition that $A$. There is no
simple analysis of $\Cr(B/\!/A)$ in terms of the agent's credence in $A$ and $A$
and logical combinations of these. Whether $A$ would bring about $B$ is
generally not a matter of logic, but depends on the laws of nature and various
particular facts besides $A$ and $B$.

Some have suggested that the probability of $B$ on the subjunctive supposition
that $A$ equals the probability of the corresponding \emph{subjunctive
  conditional}: the sentence `if $A$ were the case then $B$ would be the case'.
Let `$A \boxright B$' abbreviate this sentence. The suggestion is that
$\Cr(B/\!/A)$ can be understood as $\Cr(A \boxright B)$. Whether this is a step
forward depends on what more we can say about the proposition $A \boxright B$.
We won't pursue the question any further.

Now return to the new method for computing expected utilities from
section \ref{sec:edt}. The idea was to use conditional probabilities
instead of unconditional probabilities, which allowed us to drop the
requirement that states and acts are independent:
\[
EU(A) = U(O_1) \cdot \Cr(S_1/A) + \ldots + U(O_n) \cdot \Cr(S_n/A).
\]
Here the conditional probabilities are indicative. But we can just
as well use subjunctive conditional probabilities, considering what
the relevant act $A$ would be likely to bring about:
\[
EU(A) = U(O_1) \cdot \Cr(S_1/\!/A) + \ldots + U(O_n) \cdot \Cr(S_n/\!/A).
\]
As before, one can show (under plausible assumptions) that this method
for computing expected utilities yields the same result as our
original definition of causal expected utilities. It is also
equivalent to a state-free method:
\[
EU(A) = U(O_1) \cdot \Cr(O_1/\!/A) + \ldots + U(O_n) \cdot \Cr(O_n/\!/A).
\]


\cmnt{%
  In chapter \ref{ch:utility}, we saw that to assess the utility of a
  proposition $A$, we need to consider the probability of various
  sub-possibilities \emph{within} $A$, but we don't want to take into
  account the probability of $A$ itself. That is, to compute the
  desirability of $A$, we \emph{suppose} that $A$ is true, and then
  compute the probability-weighted average of the utility of the
  various regions within $A$.

  But now remember from section \ref{sec:conditional} that there are
  two ways of supposing a proposition. There is indicative supposition
  and subjunctive supposition. Supposing indicatively that Shakespeare
  \emph{didn't} write Hamlet, I am confident that someone else wrote
  Hamlet. Supposing subjunctively that Shakespeare \emph{hadn't}
  written Hamlet, I am confident that Hamlet would not have been
  written at all. Indicative supposition is captured by the Ratio
  Formula for conditional credence; subjunctive supposition is not. So
  far, we have used indicative supposition to determine an agent's
  utility function.  But we could also use subjunctive
  supposition. That yields a different representation of an agent's
  desires -- a representation that turns out to track causal
  dependencies.

  Let `$\Cr(A/\!/B)$' (with two dashes) denote an agent's credence in
  $A$ on the subjunctive supposition that $B$. Thus if $S$ is the set
  of worlds where Shakespeare wrote Hamlet and $N$ is the set of
  worlds where nobody wrote Hamlet, then for me, $\Cr(N/\neg S)$ is
  low but $\Cr(N/\!/\neg S)$ is high.

  Now consider the question how much you would like $\neg S$ to be the
  case. The question can be interpreted in two ways. On the first
  reading, the question is (roughly speaking) how pleased you would be
  to discover that $\neg S$ is actually true: that Shakespeare
  actually didn't write Hamlet. For example, if you've placed a bet
  that Shakespeare wrote Hamlet, you would not be pleased to learn
  that he didn't.

  On the second reading, the question is how much you would have liked
  an alternative state of the world in which $\neg S$ is the
  cases. Here you don't suspend your belief that Shakespeare wrote
  Hamlet. Rather you consider what would have happened if for some
  reason Shakespeare hadn't written Hamlet. All kinds of things would
  have been different, and you ask yourself whether the resulting
  state of the world would have been good or bad. For example, if you
  think Hamlet is the greatest piece of literature ever written, you
  probably think the world would be worse in an important respect if
  Shakespeare hadn't written Hamlet.

  So there are two ways of measuring the desirability of a
  proposition, depending on whether the proposition is supposed
  indicatively or subjunctively. In chapter \ref{ch:utility}, we used
  indicative supposition in our analysis of utility -- notably, in
  Jeffrey's axiom. So what we called simply `$U(A)$' should maybe have
  been called `$U^i(A)$', to indicate that it measures indicative
  desirability, in contrast to subjunctive desirability, `$U^s(A)$',
  where indicative supposition would be replaced by subjunctive
  supposition.

  Both kinds of desirability can be expressed in English. For example,
  `(how much) do you hope that Shakespeare didn't write Hamlet?'
  clearly asks about indicative desirability, whereas `(how much) do
  you wish that Shakespeare hadn't written Hamlet?' asks about
  subjunctive desirability.

  Subjunctive desirability is in some sense more natural. We desire
  various features. But you can't just add features to a possible
  world and leave the rest the same. Some kind of minimal revision is
  required. Moreover, we generally don't know the exact present state
  of the world. We don't know whether we'd lose our friends in the
  minimally revised world where we're rich. This very naturally leads
  to an imaging model.

  Indicative desirability is non-interventionist. Here we don't want
  something to be added or changed in the world. Rather, we want the
  world to be one of the ways it might be, for all we know.

  Equipped with the concept of subjunctive supposition and subjunctive
  utility, we can recover many of the nice features of Jeffrey's
  Evidential Decision Theory. Thus we can simply replace $\Cr(O_i/A)$
  by $\Cr(O_i/\!/A)$ in the rule for computing $EU^i$:
  \[
  EU^s(A) = \Cr(S_1/\!/A) \cdot U(O_1) + \ldots + \Cr(S_n/\!/A) \cdot U(O_n).
  \]
  And we can motivate the requirement to maximize $EU^s$ by pointing out
  that it is equivalent to a requirement to choose the most desirable
  act -- but where desirability is understood subjunctively.
} %

To get a feeling for how this works, let's first apply it to a simple
case inspired by Newcomb's problem. Depending on the outcome of a coin
toss, a box has been filled with either £1,000,000 or £0. You can take
the box or leave it. How much would you get if you were to take the
box? It depends on what's inside. If the box contains £1,000,000, then
you would get £1,000,000 on the (subjunctive) supposition that you
take the box. If the box contains £0, you would get £0 if you were to
take the box. Both possibilities have equal probability, so
\begin{gather*}
  \Cr(\text{£1,000,000}/\!/\text{Take box}) = 0.5\\
  \Cr(\text{£0}/\!/\text{Take box}) = 0.5.
\end{gather*}

In general, if a box contains a certain amount of money, and you have
the option of taking the box, and you are certain that taking the box
would not alter what's inside the box, then on the subjunctive
supposition that you take the box, you are certain to get however much
is in the box. Any uncertainty about how much you would get
boils down to uncertainty about how much is in the box.

Let $x$ be your credence that the black box in Newcomb's Problem
contains a million. Accordingly,
\begin{gather*}
  \Cr(\text{£1,000,000}/\!/1B) = x;\\
  \Cr(\text{£0}/\!/1B) = 1-x;\\
  \Cr(\text{£1,001,000}/\!/2B) = x;\\
  \Cr(\text{£1000}/\!/2B) = 1-x.
\end{gather*}
Using the new method for computing causal expected utilities, it follows that
\begin{gather*}
  EU(1B) = 1,000,000 \cdot x + 0 \cdot (1-x);\\
  EU(2B) = 1,001,000 \cdot x + 1000 \cdot (1-x).
\end{gather*}
No matter what $x$ is, taking both boxes maximizes (causal) expected
utility.

\cmnt{%
Under certain assumptions about subjunctive supposition and causal
independence, one can show that the present method of computing
expected utility is equivalent to the method that uses unconditional
probabilities of act-independent states.
} %

\begin{exercise}{3}
  Consider the third argument in favour of EDT from section
  \ref{sec:edt}: that an act's evidential expected utility equals the
  act's utility. Can we adapt this line of argument to CDT? How would
  we have to change the theory of utility from section
  \ref{sec:structure-of-utility}? 
\end{exercise}



\cmnt{%
  EDT still has one clear advantage over this -- and indeed any --
  version of CDT. The advantage is that the formal treatment of
  $\Cr(A/B)$ is much cleaner than that of $\Cr(A/\!/B)$. $\Cr(A/B)$
  equals the ratio of $\Cr(A \land B)$ over $\Cr(B)$, except in the
  unusual case where $\Cr(B)=0$. No simple formula like this exists
  for $\Cr(A/\!/B)$.  You can't compute $\Cr(A/\!/B)$ from the
  probability of $A$, $B$ and their Boolean combinations. That's
  because $\Cr(A/\!/B)$ tracks causal dependencies, and causal
  dependency is not a purely logical notion.
} %

\cmnt{%
  Let's try to get a little clearer about subjunctive supposition. A
  simplified example may help.
  % 
  \begin{example}
    Last autumn, you visited a friend who just found a mushroom in the
    forest and was going to cook it for dinner. You could tell that the
    mushroom is either a delicious \emph{paddy straw} or a poisonous
    \emph{death cap}. You advised her not to eat it. Since you left
    before dinner, you don't know if she followed your advise, and you
    never asked about it. But you've seen your friend several times
    since then, so you're sure she is alive.
  \end{example}
  
  We may ask: \emph{What if your friend ate the mushroom?} If the
  mushroom was a death cap, she would no longer be alive. But your
  friend is alive. So if she ignored your advise and ate the mushroom,
  then the mushroom was almost certainly not a death cap. Accordingly,
  $\Cr(\emph{Paddy Straw}/\emph{Ate})$ is close to 0, and
  $\Cr(\emph{Survived}/\emph{Ate})$ is 1.

  Alternatively, we may ask: \emph{What if your friend had eaten the
    mushroom?} Here the answer depends on whether mushroom was a death
  cap or a paddy straw. If it was a death cap, your friend would have
  died. If it was a paddy straw, she would have survived. Let's say
  you think it is just as likely that the mushroom was a paddy straw
  as that that it was a death cap. Your credence in the proposition
  that your friend would have survived, if she had eaten the mushroom,
  is then \nicefrac{1}{2}.

  The example suggests that uncertainty about what would have happened
  if some proposition $A$ were the case generally arises from
  uncertainty about ordinary matters of fact -- about whether the
  mushroom was in fact a paddy straw or a death cap. If we knew enough
  about the world, we could tell exactly what would have happened if
  your friend had eaten the mushroom.

  So let's assume that for every possible world $w$ and propositions
  $A$ and $B$, the facts at $w$ somehow settle whether $B$ would be
  the case if $A$ were the case. For some worlds $w$, the answer is
  yes, for others it is no. So the set of all possible worlds divides
  into two: those in which $B$ would be the case if $A$ were the case
  and those at which $\neg B$ would be the case if $A$ were the
  case. A set of worlds is a proposition. Let's write `$A \boxright
  B$' for the set of worlds where the answer is yes.

  Roughly speaking, $A \boxright B$ is the proposition expressed by
  the subjunctive conditional `if $A$ were the case then $B$ would be
  the case'. But that is not how $A \boxright B$ is defined, and
  focusing on subjunctive conditionals can be misleading. For it is
  odd to use a subjunctive conditional if $A$ assumed to be true. If
  we know that your friend did eat the mushroom, it is odd to ask what
  \emph{would} have happened if she \emph{had} eaten the
  mushroom. Nonetheless, we will assume that either $\emph{Eat}
  \boxright \neg \emph{Dead}$. Intuitively, $\emph{Eat} \boxright \neg
  \emph{Dead}$ is true in all worlds where your friend's mushroom
  wasn't poisonous.

  The general definition of $A \boxright B$ is a matter of active
  research.%
  \cmnt{%
    In cases where $A$ is a concrete event, we are considering worlds
    that are just like $w$ up until the point of $A$, then minimally
    diverge to allow for $A$, and then continue by the general laws of
    nature in $w$.%
  } %
  Setting that question aside, we can define subjunctive supposition:
  \[
  \Cr(A/\!/B) = \Cr(A \boxright B).
  \]
  
  \cmnt{%
    The conjunction of all the propositions $B$ that would have been
    true if $A$ were the case is another possible world (a maximally
    specific proposition). Let's denote this world by
    `$w^A$'. Informally, $w^A$ is what $w$ would have been if $A$ had
    been the case.

    Maybe: for all $A,B$, there are worlds where $A$ leads to $B$ and
    others where it doesn't.

    Given $A$ and $B$, we can divide the space of worlds into two:
    there are worlds where $B$ would have been the case if $A$ had
    been the case, and there are worlds where $\neg B$ would have been
    the case. A set of worlds is a proposition. Let's write $A
    \boxright B$. And now we can define subjunctive supposition:
    \[
    \Cr(A/\!/B) = \Cr(A \boxright B).
    \]
  } 
  
  \begin{genericthm}{The Imaging Axiom}
    If $A$ and $B$ are logically incompatible, then 
    \[ U(A \lor B) =
    U(A)\cdot \Cr(A/\!/(A\lor B)) + U(B)\cdot \Cr(B/\!/(A \lor B)). \]
  \end{genericthm}
  
  But there is another one. Consider the proposition that Oswald
  didn't kill Kennedy. Suppose you think that Oswald acted on his own,
  and that if he hadn't killed Kennedy, chances are that Kennedy would
  have survived for many years. You might say you wish that Oswald
  hadn't killed Kennedy. But conditional on Oswald not having killed
  Kennedy, it is almost certain that someone else did. And you don't
  wish that. Now we can say that what you really desire is that nobody
  killed Kennedy, which is a more specific proposition than that
  Oswald didn't kill him. But then we might also say that you don't
  really desire not breaking your leg, but only not breaking your leg
  without benefitial side-effects. There is another way to represent
  the desirability that gets the Oswald case right: subjunctive
  desirability. Imaging.


  Which of them is the right measure? We don't have to choose. Note
  that both of them agree on the value of elementary
  possibilities. The question is only how to extend those values to
  non-elementary propositions. It is not clear how we should test
  which of them is right.

  ---

  One can also find a conditional formulation of CDT by using the
  principle of maximising \emph{subjunctive} value. The subjunctive
  value of $A$, recall, was a measure of how good thing \emph{would
    be} if $A$ \emph{were} the case:
  \begin{equation}\tag{SV}
    U(A) = \sum_w V(w) P(w // A)
  \end{equation}
  
  This coincides with expected utility a la Lewis-Skyrms if and only
  if $P(w // A) = \sum_S P(S) P(w/ A\& S)$, where $S$ is a suitable
  state partition.
  
  In ch.xxx, we've seen that there are several ways to understand $P(A
  // B)$. The standard one is in terms of imaging. As Lewis shows,
  under plausible assumptions one can go from imaging to suitable
  partitions and back. So on some precisification, subjunctive
  conditional decision theory coincides precisely with the theory we
  have learnt so far.

  The earliest versions of CDT used subjunctive conditionals instead
  of subjunctively conditional probabilities:
  \begin{equation}\tag{CU}
    CU(A) = \sum_S V(S) P(A \boxright S),
  \end{equation}
  where $S$ is again some suitable partition, such as value-level or
  individual worlds. (E.g. the trivial partition with a single cell
  won't do: it would collapse $CU(A)$ onto $V(A)$.) Example: BRTD.
} %


\section{Unstable decision problems}\label{sec:unstable}

A curious phenomenon that can arise in CDT is that the
choiceworthiness of an option changes during deliberation.

\begin{example}
  There are three boxes: one red, one green, one transparent. You can
  choose exactly one of them. The transparent box contains £100. A
  demon with great predictive powers has anticipated your choice. If
  she predicted that you would take the red box, she put £60 in the
  red box and £300 in the green box. If she predicted that you would
  take the green box, she put £150 in the green box and £60 in the red
  box. If she predicted that you would take the transparent box, she
  put £100 in both the red and the green box.
\end{example}

Here is a matrix for the example. `$R$', `$G$', `$T$' are the three
options (red, green, transparent).
\begin{center}
  \begin{tabular}{|r|c|c|c|}\hline
    \gr & \gr Predicted $R$ & \gr Predicted $G$ & \gr Predicted $T$ \\\hline
    \gr $R$ & £60 & £200 & £100 \\\hline
    \gr $G$ & £60 & £150   & £100 \\\hline
    \gr $T$ & £100 & £100 & £100 \\\hline
  \end{tabular}
\end{center}

Let's say you initially assign equal credence to the three predictions, and your
utility for money is proportional to the amount of money. It is easy to see that
$R$ then maximizes (causal) expected utility. So you should be tempted to choose
the red box. But if you are tempted to choose the red box, then it is no longer
rational to treat all three predictions as equally likely: you should become
more confident that the demon predicted $R$. But if you are sufficiently
confident that the demon predicted $R$, then $R$ no longer maximizes expected
utility!

\begin{exercise}{2}
  Can you see where this process of deliberation will end? (Explain briefly.)
  \cmnt{%
    if (a=='T') return 100;

    u_r_given_r = 200;
    u_r_given_g = 100;
    u_r_given_t = 90;
    
    if (a=='R') return u_r_given_r * p['R'] + u_r_given_g * p['G'] + u_r_given_t * p['T'];
    
    u_g_given_r = 250;
    u_g_given_g = 0;
    u_g_given_t = 90;
    
    if (a=='G') return u_g_given_r * p['R'] + u_g_given_g * p['G'] + u_g_given_t * p['T'];
  } %
  \cmnt{%
    if (a=='T') return 100;

    u_r_given_r = 90;
    u_r_given_g = 130;
    u_r_given_t = 100;
    
    if (a=='R') return u_r_given_r * p['R'] + u_r_given_g * p['G'] + u_r_given_t * p['T'];
    
    u_g_given_r = 110;
    u_g_given_g = 80;
    u_g_given_t = 90;
    
    if (a=='G') return u_g_given_r * p['R'] + u_g_given_g * p['G'] + u_g_given_t * p['T'];
  } %
\end{exercise}

It can even happen that whatever option you currently favour makes an
alternative option look more appealing, so that it becomes impossible
to reach a decision.

\begin{example}[Death in Damascus]
  At a market in Damascus, a man meets Death. Death looks surprised;
  ``I am coming for you tomorrow'', he says. Terrified, the man buys a
  horse and rides all through the night to Aleppo. The next day, Death
  knocks on the door of the room where the man is hiding. ``I was surprised
  to see you in Damascus'', Death explains, ``for I knew I
  had an appointment with you here today.''
\end{example}

Suppose you're the man in the story, having just met Death in Damascus. Death
has predicted where you are going to be tomorrow. Like in Newcomb's Problem,
let's assume the prediction is settled, and not (causally) affected by what you
decide to do. But Death is a very good predictor. So if you go to Aleppo, you
can be confident that Death will wait for you there, while if you stay in
Damascus, you can be confident that Death will be in Damascus. The more you are
inclined towards one option, the more attractive the other option becomes.

If we interpret the MEU Principle causally, then our model of
rationality seems to rule out both options in \emph{Death in
  Damascus}: you can't rationally choose to go to Aleppo, for then you
should be confident that Death will wait in Aleppo, in which case
staying in Damascus maximizes expected utility; for parallel reasons,
you also can't rationally choose to stay in Damascus. But you only
have these two options! How can both of them be wrong?

\begin{exercise}{2}
  Let's modify the \emph{Death in Damascus} scenario by assuming that
  staying in Damascus would have some benefits, independently of
  whether Death will find you. So the matrix might look like this:
  \vspace{-1mm}
  \begin{center}
    \begin{tabular}{|r|c|c|}\hline
    \gr & \gr Death in Aleppo & \gr Death in Damascus \\\hline
    \gr Go to Aleppo & death (-100) & life (5) \\\hline
    \gr Stay in Damascus & life \& benefits (10) & death \& benefits (-95) \\\hline
    \end{tabular}
  \end{center}
  \vspace{-1mm}
  Now what does CDT say you should do? What about EDT? 
\end{exercise}


\section{Further reading}

The dispute over Newcomb's Problem has become quite sophisticated, as
you can see from the following articles (the first of which defends
one-boxing, the second two-boxing):

\begin{itemize}
\item Arif Ahmed: ``Causation and Decision'' (2010)
\item Jack Spencer and Ian Wells: \href{http://web.mit.edu/ianwells/www/WhyTakeBothBoxes.pdf}{``Why Take Both Boxes?''} (2017)
\end{itemize}

The classical treatment of EDT is Richard Jeffrey's \emph{Logic of
  Decision} (1965/1983). A classical exposition of CDT is
\begin{itemize}
\item David Lewis: \href{http://www.andrewmbailey.com/dkl/Causal_Decision_Theory.pdf}{``Causal Decision Theory''} (1981).
\end{itemize}
Lewis argues that various methods for computing causal expected
utility are plausibly equivalent.

The model of deliberation outlined in section \ref{sec:unstable} is
due to Brian Skyrms. A good introduction can be found in
\begin{itemize}
\item Frank Arntzenius: \href{https://57854434-a-62cb3a1a-s-sites.googlegroups.com/site/easwaran/readings/Arntzenius08.pdf?attachauth=ANoY7crVEwFQQqEi2qaTYEHCGkbpO7qAojrYZZoXo0j0xuI76_qOkWaKZvbySj_4A0QeixdNPVU8Gs1iA2UeS58tXWSPMAwN3Kqr2Wnr1KfNVCw1z-A5qTIJ54Ltb3gTmqcO-xHmdQOxDw0FCa7aSB63PrADPQq36qxA2N-4XkJXg3DHWS3V_hqlMRlNs8Cks6Rnb47vvsp3OtYG5Z8azK1bDotPUCMdjNmp_t4q1qSoEOMHE1vPh08\%3D\&attredirects=0}{``No Regrets, or: Edith Piaf Revamps Decision Theory''} (2008).
\end{itemize}

\cmnt{%
  Mark Sainsbury’s Logical Forms (Blackwell second edition 2001)
  contains much useful material about conditionals and their
  connection with probabilities.
} %

\begin{essay}
  Should a rational agent take both boxes in Newcomb's Problem or just
  the black box? Can you think of an argument for either side not
  mentioned in the text?
\end{essay}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
