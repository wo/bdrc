\chapter{Further Constraints on Rational Belief}\label{ch:constraints}

% Exercises too hard/tedious for weak students

\cmnt{%

  I could add a section on confirmation and uniqueness. The two go
  nicely together, since the problem of subjectivity for SBCT
  effectively assumes uniqueness; and those who emphasize subjectivity
  by pointing at values etc in science effectively argue against
  uniqueness.

} %

\cmnt{%

  Students found this section too hard, and the anthropic section
  confusing. Maybe replace that section with a slightly more in-dept
  discussion of induction, separated from indifference. (One very good
  student didn't understand the problem that indifference creates for
  induction. Weaker students didn't understand the bird example at
  all.)

  In the induction section, I might explain convergence: if we begin
  with three hypotheses about the color distribution of ravens, and
  observe a few ravens, we'll move towards the same distribution. But
  not if one of the initial hypotheses is ``first 100 black, next
  green''.

} %

\section{Belief and perception}

We have looked at two assumptions about rational belief. The first,
the MEU Principle, relates an agent's beliefs and desires to her
choices. The second, probabilism, imposes an internal, structural
constraint on rational beliefs: that they satisfy the axioms of
probability. But there is more.

\begin{example}[The Litmus Test]\label{ex:litmus}
  You are unsure whether a certain liquid is acidic. Remembering that
  acid turns litmus paper red, you dip a piece of litmus paper into
  the liquid. The paper turns red.
\end{example}

When you see the paper turn red, your credence in the hypothesis that
the liquid is acidic should increase. But as far as probabilism and
the MEU Principle are concerned, you could just as well remain unsure
whether the liquid is acidic or even become certain that it is
\emph{not} acidic, as long as your new credences are probabilistic and
your choices maximize expected utility (by the light of your beliefs
and desires).

So there are further norms on rational belief. In particular, there
are rules for how beliefs should change in response to perceptual
experience. Like the MEU Principle (and unlike probabilism), these
rules state a connection between beliefs and something other than
belief -- perceptual experience. Informally speaking, the MEU Principle
describes the causal ``output'' of beliefs: the effects an agent's
beliefs (and desires) have on her behaviour. Now we turn to the
``input'' side. We want to know how a rational agent gets to have
such-and-such beliefs in the first place.

To state a connection between perceptual experience and belief, we
need a way to identify different kinds of perceptual experience. How
do we do that? We could identify perceptual experiences by their
phenomenology, by ``what it's like'' to have the experience. But there
is no canonical standard for expressing phenomenal qualities. Besides,
we may want our norm to handle unconscious perceptions and the
perceptions of artificial agents for whom it is doubtful whether they
have any phenomenal experience.

An alternative to identifying perceptions by their phenomenology is to
identify them by their physiology, by the neurochemical or electrical
events that take place in the agent's sense organs. But that would go
against the spirit of our general approach, which is to single out
high-level patterns in rational cognition that are neutral on
details of biological or electrical implementation.

The usual strategy is therefore to identify perceptions neither by
their phenomenology nor by their physiology, but by their effect on
rational belief. The idea is that perceptual experiences provide an
agent with direct information about certain aspects of the world, so
we can distinguish perceptual experiences by the information they
provide. In the Litmus Test example, your visual experience tells you
that the litmus paper has turned red. It does not directly tell you
that the liquid is acidic; this is something you infer from the
experience with the help of your background beliefs.

In the simplest and best known version of this model, we assume that
the information conveyed to an agent by a given experience is captured
by some proposition of which the agent becomes certain. The model can
be extended to allow for cases in which the perceptual information is
uncertain and equivocal, but we will stick to the simplest version.



\section{Conditionalization}\label{sec:conditionalization}

So assume through perceptual experience an agent learns some
proposition $E$ (for ``evidence''), of which she rationally becomes
certain. How should the rest of her beliefs change to take into
account the new information?

Return to the Litmus Test. Let $\Cro$ be your credence function before
you dipped the paper into the liquid, and $\Crn$ your credence
function after seeing the paper turn red. If you are fairly confident
that red litmus paper indicates acidity, you will also be confident,
before dipping the paper, that the liquid is acidic \emph{on the
  supposition that} the paper will turn red. So your initial degrees
of belief might have been as follows.

\begin{quote}
$\Cro(\emph{Acid}) = \nicefrac{1}{2}$.\newline
$\Cro(\emph{Acid}/\emph{Red}) = \nicefrac{9}{10}$.
\end{quote}

What is your new credence in \emph{Acid}, once you learn that the
paper has turned red? Plausibly, it should be \nicefrac{9}{10}. Your
previous conditional credence in \emph{Acid} given \emph{Red} should
turn into your new unconditional credence in \emph{Acid}.

This kind of belief change is called \textbf{conditionalization}: we
say that you conditionalized \textbf{on} the information
\emph{Red}. Let's formulate the general rule.

\begin{genericthm}{The Principle of Conditionalization}
  Upon receiving information $E$, a rational agent's new credence
  in any proposition $A$ equals her previous credence in $A$
  conditional on $E$: 
  \[
    \Cr_\text{new}(A) = \Cr_\text{old}(A/E).
  \]
\end{genericthm}
%
Here it is understood that the agent's perceptual experience leaves no
room for doubts about $E$, and that $E$ is the \emph{total}
information the agent acquires, rather than part of her new
information. For example, if you see the paper turn red but at the
same time notice the smell of ammonium hydroxide, which you know is
alkaline, your credence in the \emph{Acid} hypothesis may not
increase to 0.9.

\begin{exercise1}
  Assume $\Cro(\emph{Snow}) = 0.3$, $\Cro(\emph{Wind}) = 0.6$, and
  $\Cro(\emph{Snow} \land \emph{Wind}) = 0.2$. By the Principle of
  Conditionalization, what is $\Crn(\emph{Wind})$ if the agent finds
  out that it is snowing? 
\end{exercise1}

\cmnt{%
  The next exercise should be clarified: show from the definition of
  conditionalization and the probability rules that ...%
} %

\begin{exercise2}
  Show from the definition of conditionalization and the rules of
  probability that if $\Crn$ results from $\Cro$ by conditionalizing
  on some information $E$ with $\Cro(E) > 0$, then $\Crn(E) = 1$.
\end{exercise2}


\cmnt{%
  Add exercise showing that stepwise conditioning equals cumulative
  conditioning?%
} %

\begin{exercise3}
  Assume that $\Crn$ results from $\Cro$ by conditionalizing on some
  information $E$ with $\Cro(E) > 0$, and that $\Cro$ satisfies the
  Kolmogorov axioms. Using the probability rules, show that $\Crn$
  then also satisfies the Kolmogorov axioms. (You may use any of the
  derived rules from chapter 2. Hint for Kolmogorov's axiom (ii): if
  $A$ is logically necessary, then $A\land E$ is logically equivalent
  to $E$.)
\end{exercise3}


The Principle of Conditionalization seems obvious enough. It is also
supported by a range of arguments. For example, one can show that an
agent who violates the Principle is vulnerable to a ``diachronic Dutch
Book'' -- a collection of bets, some offered before the arrival or the
new information and some afterwards, that together amount to a sure
loss. As you may have guessed, all these arguments are
controversial. Let's skip them and instead look at some applications.

When computing $\Cr_\text{new}(A)$, it is often helpful to expand
$\Cr_\text{old}(A/E)$ with the help of Bayes' Theorem. The Principle
of Conditionalization then turns into the following (equivalent) norm,
known as \textbf{Bayes' Rule}:
%
\[
\Cr_{\text{new}}(A) = \frac{\Cr_{\text{old}}(E/A) \cdot \Cr_{\text{old}}(A)}{\Cr_{\text{old}}(E)}, \text{ provided $\Cr_\text{old}(E) > 0$}.
\]
%
The usefulness of this formulation comes from the fact that it is
often much easier to evaluate the probability $\Cro(E/A)$ of the
evidence $E$ conditional on some hypothesis $A$  than to
evaluate the probability $\Cro(A/E)$ of the hypothesis $A$ conditional
on the evidence.

Let's do an example.
\begin{example}\label{ex:base}
  2\% of women in a certain population have breast cancer. A test is
  developed that correctly detects 95\% of cancer cases but also gives
  a false positive result in 10\% of cases without the cancer. A woman
  from the population takes the test, and gets a positive result. How
  confident should you be that the woman has breast cancer?
\end{example}

Let's imagine that you know all the statistical information before
learning test result. Knowing that the woman is from a
population in which 2\% of women have breast cancer, your initial
credence in the hypothesis $C$ that the woman has breast cancer should
plausibly be 0.02. So $\Cr_{old}(C) = 0.02$. Moreover, since you know
that the test yields a positive result in 95\% of cancer cases,
$\Cr_{\text{old}}(P/C) = 0.95$, where $P$ is the proposition that the
test result is positive. Similarly, since the test yields a positive
result in 10\% of non-cancer cases,
$\Cr_{\text{old}}(P/\neg C) = 0.1$. Now we simply plug these numbers
into Bayes' Rule, expanding the denominator by the Law of Total
Probability:
%
\begin{align*}
  \Cr_{\text{new}}(C) &= \frac{\Cr_{\text{old}}(P/C) \cdot
  \Cr_{\text{old}}(C)}{\Cr_\text{old}(P/C) \cdot \Cr_\text{old}(C) +
  \Cr_\text{old}(P/\neg C)\cdot \Cr_\text{old}(\neg C)}\\[3mm]
  &= \frac{0.95 \cdot 0.02}{0.95 \cdot 0.02 + 0.1 \cdot 0.99} = \frac{0.019}{0.019 + 0.098} = 0.16.
\end{align*}

The answer is much lower than many people think -- including many trained 
physicians. But it makes intuitive sense. Imagine we
took a large sample of 1000 women from the population. We would expect
around 2\%, or 20 women, in the sample to have breast cancer. If we
tested all women in the sample, we would expect around 95\% of those
with cancer to test positive. That's 95\% of 20 = 19 women. Of the 980
women without cancer, we would expect around 10\%, or 98 women, to
test positive. The total number of women who would test positive would
therefore be 19 + 98 = 117. Of these 117, 19 actually have cancer. So the
chance that a woman who tests positive has cancer is 19/117 = 0.16. If
you look back at the above application of Bayes' Theorem, you can see
that it basically encodes this line of reasoning.

The tendency to overestimate the significance of tests in cases like example
\ref{ex:base} is known as the \textbf{base rate fallacy} because it is
assumed to arise from neglecting the low ``base rate'' of 2\%.

\begin{exercise2}
  Box $A$ contains two black balls. Box $B$ contains one black ball
  and one white ball. I choose a box at random and blindly draw a
  ball. The ball is black. How confident should you be that I chose
  box $A$? 
\end{exercise2}

\begin{exercise3}[The Prosecutor's Fallacy]\label{ex-prosecutor}
  A murder has been committed on a remote island with a million
  inhabitants. In a database of blood donors, detectives find a record
  whose DNA seems to match the perpetrator's DNA from the crime
  scene. The DNA test is very reliable: the probability that it finds
  a match between distinct people is 1 in 100,000. So the person with
  the matching DNA is arrested and brought to court. The prosecutor
  argues that the probability that the defendant is innocent is
  1/100,000. Is that correct? As a member of the jury, how confident
  should you be in the defendant's guilt?
\end{exercise3}

\cmnt{%
\begin{exercise}[The Three Prisoners]
  Our total evidence often includes not only \emph{what} we learn, but
  also \emph{how} we learned it (intuitively speaking).

  A scientist wants to test if there are small fish in a certain
  lake. She goes out and 
\end{exercise}
} %

\cmnt{%
\begin{exercise}[The Monty Hall problem]\label{ex-montyhall}
  A game show host offers you a choice between three doors. Behind one
  of them is a prize. The host, who knows where the prize is,
  announces that after you've chosen a door, he will open one of the
  other two doors, revealing a door that does not hide the
  price. After you've made your choice of a door and the host has
  opened another door, he offers you an opportunity to switch to the
  remaining door. If you want to maximise the chance of winning the
  prize, should you switch? (Explain briefly.) $\star\star$
\end{exercise}
} %

\cmnt{%
  Prove that as long as cr1 pHq and cr1pEq are both nonextreme, condi-
  tionalizing on E increases the agentâ€™s credence in H when H entails
  E.
} %

\cmnt{%
Not everyone agrees that rational degrees of belief always evolve by
conditionalization.

\begin{example}[The Sleeping Beauty Problem]
  
\end{example}

Conditionalization seems to suggest 1/2. 
} %

\section{The Principle of Indifference}

If an agent's beliefs evolve by conditionalization, can we be sure
that her beliefs will adequately reflect all the evidence she receives
over time? No. If the agent starts out with crazy beliefs,
conditionalization will not make her sane.

\begin{example}\label{ex:grue}
  You are stranded on a remote island, which you find inhabited by a
  strange kind of flightless bird. In the first ten days of your stay
  on the island, you see 100 birds, all of which are green.
\end{example}
%
Plausibly, you should be fairly confident that the 101st bird will
also be green. The Principle of Conditionalization does not ensure
this. To see why, let $H$ be the proposition that the first 100 birds
you encounter on the island are atypical in colour. Suppose when you
first arrived on the island you were convinced of $H$ -- for no good
reason. The observation of 100 green birds does not challenge that
conviction, so after conditionalizing on these observations you are
still confident in $H$. And if you believe that the first 100 birds
you encountered were green and also atypical in colour, then
you'll expect the 101st bird to have a different colour. So if we
think you should be confident that the 101st bird will be green, we
have to say that you should not be confident in $H$ before receiving
any relevant evidence.

\cmnt{%
  The next exercise is confusing. Strip it.

\begin{exercise}
  Show that whenever some proposition $H$ is not entailed by an
  agent's evidence $E$, nor is its negation $\neg H$, then
  conditionalizing on $E$ is compatible with \emph{any} credence in $H$:
  for any $x \in [0,1]$ there is some credence $\Cr_\text{old}$ such
  that $\Cr_\text{old}(H/E) = x$. $\star\star$
\end{exercise}
} %

What we see here is Hume's problem of induction. As Hume pointed out,
there is no logical guarantee that the future will resemble the past,
or that the unobserved parts of the world resemble the
observed. The colour of the 101st bird is not entailed by the colour
of the first 100 birds. To infer that the 101st bird is green we thus
need a further premise about the ``uniformity of nature'': that the
101st bird is likely to have the same colour as the first 100
birds. How do we know this? We may have inferred it from our
experiences at 100 other islands, but to conclude that the lessons
from these islands carry over to the present island, we need another
premise about the uniformity of nature. Ultimately, some such premise
must be taken for granted.

In Bayesian terms, this means that we have to impose constraints on
what an agent may believe \emph{without any relevant
  evidence}. Scientifically minded people sometimes feel uneasy about such
constraints, and therefore speak about the \textbf{problem of the
  priors}. An agent's \textbf{priors} (or ``ultimate priors'') are her
credences before receiving any evidence. The problem of the priors is
to explain what rational priors should look like. 

So what should you believe if you have no evidence at all about a
certain subject matter? A natural thought is that you should be
maximally open-minded. For example, if you know that one of
three people has committed a murder, but you have no further
information about the case, then you should give equal credence to the
three possibilities. More generally, the following principle looks
appealing.

\begin{genericthm}{The Principle of Indifference}
  If $A_1,\ldots,A_n$ are $n$ propositions exactly one of which must
  be true, and an agent has no evidence relevant to these
  propositions, then her credence in each of the propositions should
  be \nicefrac{1}{n}.
\end{genericthm}

Unfortunately, the Principle of Indifference can't be right, because it is
inconsistent. For example, suppose you have no information about the
colour of my hat. Here are two possibilities:
\begin{enumerate*}
\item[$R$:] The hat is red.
\item[$\neg R$:] The hat is not red.
\end{enumerate*}
Exactly one of these must be true. By the Principle of Indifference,
you should therefore give credence \nicefrac{1}{2} to both $R$ and
$\neg R$. But we can also divide $\neg R$ into several
possibilities:
\begin{enumerate*}
\item[$R$:] The hat is red.
\item[$B$:] The hat is blue.
\item[$G$:] The hat is green.
\item[$Y$:] The hat is yellow.
\item[$O$:] The hat has some other colour.
\end{enumerate*}
Again exactly one of these must be true, so by the Principle of
Indifference, you should give credence \nicefrac{1}{5} to each. So the
Principle entails that your credence in $R$ should be \nicefrac{1}{2}
and also that it should be \nicefrac{1}{5}!

Some have concluded that in cases like these, rationality really does
require you to have \emph{several} credence functions: relative to one
of your credence functions, $R$ has probability \nicefrac{1}{2},
relative to another, it has probability \nicefrac{1}{5}. I'll set
this view aside for now, but we will return to it in section
\ref{sec:imprecise}.

Another response is to say more about the propositions
$A_1,\ldots,A_n$ to which the Principle applies. Intuitively, you
might say, the Principle does not hold for $R$ and $\neg R$ because
these two propositions are not on a par: there are more ways of being
non-red than there are of being red. Unfortunately, it is hard to make this
intuition precise, and harder still to turn it into a general rule, as
the following exercise illustrates.

\begin{exercise2}\label{e:cubefactory}
  A cube is hidden in a box. A sticker on the box reveals that the cube
  has a side length of at least 2 cm, but less than 4 cm. So here are
  two possibilities:
  \begin{enumerate*}
  \item[$S$:] The cube's side length lies between 2 cm and 3 cm (excluding 3).
  \item[$L$:] The cube's side length lies between 3 cm and 4 cm (excluding 4).
  \end{enumerate*}
  The intervals have the same length, so $S$ and $L$ are intuitively
  on a par. We might infer that you should give credence
  \nicefrac{1}{2} to both $S$ and $L$. But now observe that if a cube
  has side length $x$, then the cube's volume is $x^3$.
  \begin{enumerate*}
  \item[(a)] Can you restate the propositions $S$ and $L$ in terms of
    volume?
  \item[(b)] What should your credence in $S$ be if you treat equally
    sized ranges of volume as equally likely?
  \end{enumerate*}
  \vspace{-5mm}
\end{exercise2}

Another problem with the Principle of Indifference is that it actually
clashes with the ``uniformity of nature'' assumption required for
inductive inference. Return to example \ref{ex:grue}. For simplicity,
let's assume you know in advance that any bird on the island can only
be green or red. So there are four possibilities regarding the first
two birds you might see:
\begin{enumerate*}
\item[$GG$:] Both birds are green.
\item[$GR$:] The first bird is green, the second is red.
\item[$RG$:] The first bird is red, the second is green.
\item[$RR$:] Both birds are red. 
\end{enumerate*}
By the Principle of Indifference, you should give credence
\nicefrac{1}{4} to each of these possibilities. (They are also
intuitively on a par.) Now what happens when you see the first bird,
which is green? Your evidence $E$ rules out $RG$ and $RR$. By the
Principle of Conditionalization, your new credence in $GG$ equals your
previous credence in $GG$ conditional on $E$, which (as you should
check) is \nicefrac{1}{2}. So after having seen the first green bird,
your credence in the hypothesis that the next bird will be green is
\nicefrac{1}{2}.  By the same reasoning, your credence in the
hypothesis that the third bird will be green after having seen two
green birds, is also \nicefrac{1}{2}. In general, no matter how many
green birds you see, your credence in the next bird being green will
remain at \nicefrac{1}{2}. In other words, the Principle of
Indifference makes it impossible to draw inductive inferences from
experience.

Despite these problems, many instances of the Principle of
Indifference look very plausible. If you're investigating a murder,
and judge that suspect $A$ is three times as likely as suspect $B$ to
be the murderer, then there had better be some reason for this
judgement. In the absence of any relevant evidence, your judgement
would be irrational. Several authors have attempted to turn examples
like this into a fully general principle which, unlike the classical
Principle of Indifference, is consistent and allows for inductive
inference, but there is no agreement on whether this can be done and
on what the resulting principle should look like. In this form, at
least, the ``problem of the priors'' remains open.

\section{Probability coordination}

\cmnt{%
  The Principle of Indifference tries to identify a uniquely rational
  credence function for any agent without relevant evidence: if you
  don't know anything about a subject matter, spread your credence
  evenly over all possibilities. Many philosophers deny that the norms
  of rationality are that demanding.
} %

We turn from the highly controversial Principle of Indifference to
another norm for rational priors that is almost universally accepted
among Bayesians. The norm connects subjective probability with
objective probability, and plays a central role in Bayesian
confirmation theory and Bayesian statistics.

\begin{genericthm}{The Probability Coordination Principle}
  If an agent has no evidence about some proposition $A$, then her
  credence in $A$ on the supposition that the objective probability of
  $A$ is $x$, should be $x$:
  \[
    \Cr_0(A / \text{Pr}(A)\!=\!x) = x
  \]
\end{genericthm}
%
Here `Pr' stands for any kind of objective probability, such as
relative frequency or quantum physical chance. I've added a subscript
`0' to `$\Cr$' to indicate that the agent in question has no evidence
relevant to $A$. With `Pr' understood as quantum physical chance, the
Probability Coordination Principle is also known as the `Principal
Principle' (or rather, it's a special case of that Principle).

We have unwittingly assumed the Probability Coordination Principle all
along. In example \ref{ex:base}, for instance, we assumed that if all you
know about a woman is that she is from a population in which 2\% of
women have breast cancer, then your credence in the hypothesis that she has
breast cancer should be 0.02. This is clearly not entailed by the
Kolmogorov axioms. It is, however, entailed by the Probability
Coordination Principle, assuming that your credence can be modelled as
resulting from a prior state, in which you had no evidence at all
about the woman, by conditionalizing on the statistical information
that the cancer rate is 2\%.

To see why the Probability Coordination Principle is stated in terms
of conditional credence, consider a typical case of testing scientific
hypotheses. Often such hypotheses only make statistical predictions:
they entail that under circumstances $C$, there is a probability of
$x$ that outcome $O$ will occur.

Concretely, suppose you are undecided between two theories, $H_1$ and
$H_2$, giving credence \nicefrac{1}{2} to each. $H_1$ says that under
circumstances $C$, the probability of $O$ is 0.9; $H_2$ says it is
0.3. You set up an experiment with circumstances $C$ and observe
outcome $O$. How does that affect your credence in $H_1$ and $H_2$? By
Bayes' Rule,
\[
\Cr_{\text{new}}(H_1) = \frac{\Cr_{\text{old}}(O/H_1) \cdot
  \Cr_{\text{old}}(H_1)}{\Cr_\text{old}(O/H_1)\cdot \Cr_\text{old}(H_1) +
  \Cr_\text{old}(O/H_2)\cdot \Cr_\text{old}(H_2)}.
\]
We know that $\Cr_\text{old}(H_1) = \Cr_\text{old}(H_2) =
\nicefrac{1}{2}$. The Probability Coordination Principle tells us that
$\Cr_{\text{old}}(O/H_1) = 0.9$ and $\Cr_{\text{old}}(O/H_2) = 0.3$. Thus 
\[
\Cr_{\text{new}}(H_1) = \frac{0.9 \cdot 0.5}{0.9 \cdot 0.5 + 0.3 \cdot 0.5} = 0.75.
\]
So your credence in $H_1$ should increase to \nicefrac{3}{4}, and your
credence in $H_2$ should decrease to \nicefrac{1}{4}.



\begin{exercise2}
  You are unsure whether a certain coin is biased 2:1 towards heads or
  2:1 towards tails; initially you give credence \nicefrac{1}{2} to
  each possibility. Then you toss the coin twice, and both times it
  comes up heads. What is your new credence concerning the coin's
  bias? (If a coin is biased 2:1 towards heads, then heads has an
  objective probability of \nicefrac{2}{3}.) 
\end{exercise2}



\section{Anthropic reasoning}

How confident should an ideal agent, without any relevant evidence, be
that she exists? A strange question, but a question that sometimes comes
up in cosmology and certain philosophical puzzles. 

The following puzzle is due to Nick Bostrom.

\begin{example}[God's Coin Toss]\label{ex:godscoin}
  At the beginning of time, God flips a fair coin. If the coin lands
  heads, she creates two people in two rooms, one with blue eyes and
  one with green eyes. If the coin lands tails, God creates only one
  room with a blue-eyed person in it. You wake up, and God informs you
  of these facts. Then you look in the mirror and see that your eyes
  are blue. How confident should you be that God's coin landed heads?
\end{example}

At first, you might think the answer is \nicefrac{1}{2} on the grounds
that the objective probability of heads is \nicefrac{1}{2} and your
evidence of having blue eyes is equally compatible with heads and
tails. But notice that if you had found your eyes to be green, then
you could have inferred with certainty that God's coin landed
heads. And if some evidence $E$ increases the probability of a
hypothesis $H$, then $\neg E$ must decrease the probability of $H$. So
finding your eyes to be blue should decrease your credence in heads.
More specifically, if $\Cr_\text{old}(\emph{Heads}) =
\nicefrac{1}{2}$, then by Bayes' Rule, $\Cr_\text{new}(\emph{Heads}) =
\nicefrac{1}{3}$.

\cmnt{%
\begin{align*}
  \Cr_\text{new}(\emph{Heads}) &= \frac{\Cr_\text{old}(\emph{Blue}/\emph{Heads})\cdot \Cr_\text{old}(\emph{Heads})}{\Cr_\text{old}(\emph{Blue}/\emph{Heads})\cdot \Cr_\text{old}(\emph{Heads}) + \Cr_\text{old}(\emph{Blue}/\emph{Tails})\cdot \Cr_\text{old}(\emph{Tails})}\\
   &= \frac{0.5 \cdot \Cr_\text{old}(\emph{Heads})}{0.5 \cdot \Cr_\text{old}(\emph{Heads}) + 1 \cdot \Cr_\text{old}(\emph{Tails})}
 \end{align*}
}% 

\begin{exercise1}
  Show this. That is, assume $\Cr_\text{old}(\emph{Heads}) =
  \nicefrac{1}{2}$, and use Bayes' Rule to derive that $\Cr_\text{new}(\emph{Heads}) =
  \nicefrac{1}{3}$.%
  \cmnt{%
    And if $\Cr_\text{old}(\emph{Heads}) = \nicefrac{2}{3}$, then
    $Cr_\text{new}(\emph{Heads}) = \nicefrac{1}{2}$.  %
  }%
\end{exercise1}

To conclude that your credence in \emph{Heads}
should be \nicefrac{1}{3}, we would have to assume that
$\Cr_\text{old}(\emph{Heads}) = \nicefrac{1}{2}$. But that, too, could
be questioned. To be sure, the Probability Coordination Principle
requires that $\Cr_\text{old}(\emph{Heads}) = \nicefrac{1}{2}$
\emph{if you have no other relevant evidence}. But one might argue
that you do have further relevant evidence -- namely, the evidence
that you exist.

Why should that be relevant? The idea is that the more people there
are in a possible world, the more likely it is that one of these
people is you. In a heads world, there are two people; so the chance
that you are one of them is twice the chance that you're the single
person in a tails world. By that line of thought, the observation that
you exist, which you make before looking in the mirror, should
increase your credence in heads from \nicefrac{1}{2} to
\nicefrac{2}{3}. Finding that your eyes are blue then reduces it back to
\nicefrac{1}{2}.

\begin{exercise2}
  Show that
  \begin{enumerate*}
  \item[(a)] if $\Cr(\emph{Heads}) = \nicefrac{1}{2}$ and
    $\Cr(\emph{Exist}/\emph{Heads}) = 2 \cdot
    \Cr(\emph{Exist}/\emph{Tails})$, then by Bayes' Theorem,
    $\Cr(\emph{Heads}/\emph{Exist}) = \nicefrac{2}{3}$;
  \item[(b)] assuming
    $\Cr_\text{old}(\emph{Heads}) = \nicefrac{2}{3}$, then after
    seeing that your eyes are blue,
    $\Cr_\text{new}(\emph{Heads}) = \nicefrac{1}{2}$.
  \end{enumerate*}
  \vspace{-5mm}
\end{exercise2}

If you are sceptical about this argument for $\Crn(\emph{Heads}) =
\nicefrac{1}{2}$, you are not alone. Among other things, the argument seems to
assume that rational agents should initially give significant credence
to the hypothesis that they don't exist, and it's not clear why that
should be a requirement of rationality.

Anyway, let's give a name to the problematic assumption:
\begin{genericthm}{Dubious Principle}
  If $H_1$ is the hypothesis that there are $n$ people in total, and
  $H_2$ says that there are $k$ people, then $\Cr_0(\emph{Exist} /
  H_1) = \Cr_0(\emph{Exist}/H_2) \cdot \nicefrac{n}{k}$, where $\Cr_0$
  is the credence function of a rational agent without any evidence,
  and $\emph{Exist}$ is the proposition that the agent exists.
\end{genericthm}

So far, all this may look like idle sophistry. But now consider the
hypothesis that the human race will go extinct within the next few
years, at a point where the total number of people who ever lived will
be around 100 billion. By contrast, if humankind continues to prosper
for another million years or so, the total number of people who ever
lived will be at least a thousand times greater. To keep the maths
easy, let's pretend that these are the only two possibilities. Call
the first \emph{Doom} and the second \emph{No Doom}. A priori -- in
the absence of any evidence -- you might think \emph{Doom} and
\emph{No Doom} deserve roughly equal credence. But here's a piece of
evidence you have (call it \emph{Early}): you are one of the first 100
billion people. And this dramatically increases the probability of
\emph{Doom}.

Let's crunch the numbers. On the supposition that the total number of
people is 100 trillion (100,000 billion), only \nicefrac{1}{1000} of
all people are among the first 100 billion. So the prior probability
that you are one of the first 100 billion is arguably
\nicefrac{1}{1000}. By contrast, on the supposition that the total
number of people is 100 billion, the probability that you are one of
the first 100 billion is 1. So, by Bayes' Theorem,
\begin{align*}
  \Cr(\emph{Doom}/\emph{Early}) &= \frac{\Cr(\emph{Early}/\emph{Doom}) \cdot \Cr(\emph{Doom})}{\Cr(\emph{Early}/\emph{Doom}) \cdot \Cr(\emph{Doom}) + \Cr(\emph{Early}/\neg\emph{Doom}) \cdot \Cr(\neg \emph{Doom})}\\[3mm]
    &= \frac{1 \cdot \Cr(\emph{Doom})}{1 \cdot \Cr(\emph{Doom}) + 0.001 \cdot \Cr(\neg\emph{Doom})}
\end{align*}

\smallskip 

If $\Cr_0(\emph{Doom}) = \nicefrac{1}{2}$, it follows that
$\Cr_0(\emph{Doom}/\emph{Early}) = 1000/1001 \approx 0.999$. Taking into
account the fact that you are among the first 100 billion people who
ever lived, it seems that you should be almost certain that humankind is
about to go extinct!

The argument we've just rehearsed is known as the \textbf{doomsday
  argument}. The conclusion becomes a little less striking if we take
into account that there are more possibilities than \emph{Doom} and
\emph{No Doom}, but the upshot remains the same: we should be highly
confident that we are among the last people to have lived.

You may not be surprised to hear that. After all, we face a long list
of existential threats -- nuclear war, global warming,
pandemics, hostile AI, and so on. What's surprising is that none of
these threats are taken into account in the doomsday argument. The
conclusion is reached solely on the basis of population statistics.

Given that \emph{Early} strongly increases the probability of
\emph{Doom}, to block the conclusion that \emph{Doom} is highly
probable, we would have to assume that the prior probability of
\emph{Doom}, before taking into account \emph{Early}, is much lower
than the prior probability of \emph{No Doom}. But why should that be
so? A priori, the hypothesis that the total number of people is 100
billion is roughly on a par with the hypothesis that the number is
100,000 billion: taking the second to be 1000 times more likely than
the first, without any relevant evidence, surely seems irrational.

But perhaps we have evidence that favours $\emph{No Doom}$
over $\emph{Doom}$ -- namely, the evidence of our existence. By the
Dubious Principle, $\Cr_0(\emph{Exist}/\neg\emph{Doom})$ is 1000
times greater than $\Cr_0(\emph{Exist}/\emph{Doom})$. Consequently,
if $\Cr_0(\emph{Doom}) = \nicefrac{1}{2}$, then
$\Cr_0(\emph{Doom}/\emph{Exist}) = \nicefrac{1}{1000}$ and
$\Cr_0(\emph{Doom}/\emph{Exist} \land \emph{Early}) = \nicefrac{1}{2}$.

So it would be comforting if the Dubious Principle were true after all.

\cmnt{%
  Add exercise: fine-tuning
} %


\section{Further reading}

Chapter 15 of 
\begin{itemize}
\item Ian Hacking: \href{http://fitelson.org/confirmation/hacking_introduction_to_probability_and_inductive_logic.pdf}{\emph{An Introduction to Probability and Inductive Logic}} (2001)
\end{itemize}
goes into some more details about conditionalization. 

Several philosophers have recently defended versions of the Principle
of Indifference, and the more general claim that there is a unique
rational prior credence function. A critical overview and discussion
can be found in
\begin{itemize}
\item Christopher G.\ Meacham: \href{http://people.umass.edu/cmeacham/Meacham.Impermissive.Bayesiansim.pdf}{``Impermissive Bayesianism''} (2014).
\end{itemize}

\cmnt{%

  Support indifference by giving up unique credence:
  \cite{joyce05how}. Even more radical departure from the standard
  Bayesian model (to accommodate indifference) is advocated in
  \cite{norton08ignorance}.

} %

For more on the Doomsday Argument and related puzzles, see
\begin{itemize}
\item Nick Bostrom:
  \href{http://www.anthropic-principle.com/preprints/cau/paradoxes.html}{``The
    Doomsday Argument, Adam \& Eve, UN++, and Quantum Joe''} (2001).
\end{itemize}
(What I call the `Dubious Principle' is Bostrom's `Self-Indication Assumption', SIA.)

\begin{essay}
  Can you think of another way to block the Doomsday Argument, without
  relying on the Dubious Principle?
\end{essay}


\cmnt{%
  From IJ Good, via Jeffrey: ``...that a man is capable of
  extra-sensory perception, in the form of telepathy.  You may imagine
  an experiment performed in which the man guesses 20 digits (between
  0 and 9) correctly. If you feel that this would cause the
  probability that the man has telepathic powers to become greater
  than 1/2, then the [prior odds] must be assumed to be greater than
  10-20 . [. . .] Similarly, if three consecutive correct guesses
  would leave the probability below 1/2, then the [prior odds] must be
  less than 10-3 .
} %



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End: