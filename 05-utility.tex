\chapter{Utility}\label{ch:utility}


\section{Two conceptions of utility}

Daniel Bernoulli realized that rational agents don't always maximize expected
monetary payoff: £1000 has more utility for a pauper than for a rich man. But
what is utility?

Until the early 20th century, utility was widely understood to be some kind of
psychological quantity, often identified with degree of pleasure and absence of
pain. On that account, an outcome has high utility for an agent to the extent
that it increases the agent's pleasure and/or decreases her pain.

Let's assume for the sake of the argument that one can represent an agent's
total amount of pleasure and pain by a single number -- the agent's ``degree of
pleasure''. Can we understand utility as degree of pleasure? The answer depends
on what role we want the concept of utility to play.

One such role lies in ethics. According to \textbf{utilitarianism}, an act is
morally right just in case it would bring about the greatest total utility for
all people. In this context, identifying utility with degree of pleasure implies
that only pleasure and pain have intrinsic moral value; everything else --
autonomy, integrity, respect of human rights, and so on -- would be morally
relevant only insofar as it causes pleasure or pain. This assumption is known as
\textbf{ethical hedonism}. We will not pursue it any further.

\begin{exercise}{1}
  Suppose that money has declining marginal utility, and that the utility of
  different wealth levels are the same for all people (so that, for example, a
  net wealth of £1000 is as good for me as it is for you). Without any further
  assumptions about utility, it follows that if one person has more money than
  another, then the total utility in the population would increase if the
  wealthier person gave some of her money to the poorer person, decreasing the
  gap in wealth. Explain why.
\end{exercise}

Another role for a concept of utility lies in the theory of practical
rationality. According to the MEU Principle, practically rational agents choose
acts that maximize the credence-weighted average of the utility of the possible
outcomes. If we identify utility with degree of pleasure, the MEU principle
turns into what we might call the `MEP Principle':
%
\begin{genericthm}{The MEP Principle}
  Rational agents maximize their expected degree of pleasure. 
\end{genericthm}
%
\noindent%
An act's \emph{expected degree of pleasure} is the
probability-weighted average of the degree of pleasure that might
result from the act. 

The MEP Principle is a form of \textbf{psychological
  hedonism}. Psychological hedonism is the view that the only thing
that ultimately motivates people is their own pleasure and pain.

The founding fathers of modern utilitarianism, Jeremy Bentham and John Stuart
Mill, had sympathies for both ethical hedonism and psychological hedonism. As a
consequence, the two conceptions of utility -- the two roles associated with the
word `utility' -- were not properly distinguished. Today, both kinds of hedonism
have long fallen out of fashion, but the two conceptions are still often
conflated.

\cmnt{%
  Bentham: ``By utility is meant that property in any object, whereby
  it tends to produce benefit, advantage, pleasure, good, or
  happiness, (all this in the present case comes to the same thing) or
  (what comes again to the same thing) to prevent the happening of
  mischief, pain, evil, or unhappiness to the party whose interest is
  considered.'' [q.19]
} %

For the most part, contemporary utilitarians hold that the standard of moral
rightness is the total \emph{welfare} or \emph{well-being} produced by an act,
which is not assumed to coincide with total degree of pleasure. Thus `utility'
is nowadays often used as a synonym for `welfare' or `well-being'. But the word
is also widely used in the other sense, to denote whatever motivates (rational)
agents.

Some have argued that the two uses actually coincide: that the only
thing that motivates rational agents is their own welfare or
well-being. This may or may not be true. But it needs to be backed up
by data and argument; it does not become true through sloppy use of
language.

In these notes, `utility' is only used in the second sense. The utility of a
outcome measures the extent to which the agent in question wants the outcome to
obtain. We do not assume that the only thing agents ultimately want is to
increase their degree of pleasure, their welfare, their well-being, or anything
like that.

\cmnt{%
  Psychological hedonism faces a number of challenges. Most obviously,
  there seem to be clear counterexamples: parents who take on
  hardships to the benefit of their children, ascets who renounce
  pleasure and strive for privation, soldiers who choose a painful
  death to save their comrades, and so on. It is hard to believe that
  in all these cases, the agent mistakenly believes that their choice
  will increase their own pleasure and decrease their pain.
} %

Note that psychological hedonism, or the slightly more liberal claim that people
only care about their welfare, is at most a contingent fact about humans. One
can easily imagine agents who are motivated by other things. We can imagine a
mother who knowingly takes on hardships for the benefit of her children, or a
soldier who intentionally chooses a painful death in order to save her comrades.
Psychological hedonists hold that humans would never consciously do such things:
whenever an agent sacrifices her own good to benefit others, she mistakenly
believes that her choice will actually make herself better off than the
alternatives. Again, we don't need to argue over whether this is true. The
important point is that utility, as we use the term, does not \emph{mean} the
same as degree of pleasure or welfare or well-being.

\cmnt{%
  (If you want to build a robot whose goal is to find landmines or to
  win at chess, you don't have to make sure that the robot feels
  pleasure whenever it achieves its goal.)%
} %

A hedonist might object that while it is conceivable that an agent is motivated
by things other than her personal pleasure, such agents would be irrational.
After all, the MEP Principle only states that \emph{rational} agents maximize
their expected degree of pleasure; it doesn't cover irrational agents.

This brings us to a tricky issue. What do we mean by `rational'? The label
`rational' is sometimes associated with cold-hearted selfishness. On this usage,
a rational agent always looks out for her own advantage, with no concern for
others. This idea of ``economic rationality'' has its use, but it is not our
topic. The kind of rationality we're interested in is a more minimal notion.
Intuitively, it is the idea of ``making sense''. If you want to reduce animal
suffering, and you know you can achieve this by eating less meat, then it makes
sense that you eat less meat. If you are sure that a picnic will be cancelled if
it is raining, and you see that it is raining, then it doesn't make sense to
believe that the picnic will go ahead. The model we are studying is a model of
agents who ``make sense'' in this kind of way.

Even if we were interested in the cold-hearted and selfish sense of rationality,
we should not define utility as degree of pleasure or welfare. Consider a
hypothetical agent who cares not just about herself, who sacrifices some of her
own good to reduce the pain of others. The agent is ``irrational'' in the
cold-hearted and selfish sense. But what is irrational about her? Does the fault
lie in her beliefs, in her goals, or in the way she brings these together to
make choices? Plausibly, the ``fault'' lies in her goals. Her concern for others
is what goes against the standards of cold-hearted and selfish rationality. But
if we were to define utility as degree of pleasure or welfare, we would have to
say that the agent violates the basic norm of practical rationality, the MEU
Principle.

The point generalizes. Consider a person in an abusive relationship who is
manipulated into doing things that hurt or degrade her. We might reasonably
think that the person shouldn't do these things; it is against her interest to
do them. But what is at fault? Arguably, the fault lies in her (manipulated)
desires. What the person does may well be in line with what she wants to achieve
-- in particular, with her strong desire to please her partner. But a healthy,
self-respecting person, we think, should have other desires.

By understanding utility as a measure of whatever the agent in question desires,
we do not automatically sanction these desires as rational or praiseworthy. Our
usage of `utility' allows us to say that the person in the abusive relationship
shouldn't do what she is doing, because she should have different desires that
would not support her actions.

% Does our use of `utility' match the official usage in economics textbooks?
% Many textbooks define utility in terms of choice, using e.g. the vNM axioms.
% But these are incompatible with utility being sensitive to non-local
% attributes. Indeed, if we assume that the "outcomes" in the VNM axioms are
% material goods, then we'll trivially get a utility measure that regards
% material goods as the ultimate bearers of value.

\section{Sources of utility}\label{sec:sources-utility}

An outcome's utility measures the extent to which the agent is motivated to
bring about the outcome. I will often say that this is the degree to which the
agent \emph{desires} the outcome, but we need to keep in mind that the word
`desire' can be misleading. For one thing, we need to cover ``negative desire''.
Being hungry might have greater utility for you than being dead, even though you
do not desire either. More importantly, `desire' is often associated with a
particular type of motivational state. I might say that I got up early in the
morning despite my strong desire to stay in bed; I got up not because I desired
to get up, but because I had to. On this usage, my desires contrast with my
sense of duty.

Utility comprises everything that motivates the agent, all the reasons she has
for and against a particular action. As such, `utility' is an umbrella term for
a diverse set of psychological states or events. We can be motivated by bodily
cravings, by moral commitments, by our image of the kind of person we want to
be, by an overwhelming feeling of terror or love, and so on. These factors need
not be conscious. There is good evidence that our true motives are often not
what we believe or say they are. An agent's utility function represents their true
motives, and all of them.

\cmnt{%
  Ignorance of ones own motives, desires and preferences is part of the human
  condition, familiar from centuries of literature and volumes of psychological
  research. We humans frequently judge that we do not need something until we
  find out how we react when it is gone; on the other hand, we think we couldn't
  live without other things whose loss doesn't affect us much at all. We often
  ``rationalise'' our decisions with apparently sensible reasons that have
  little to do with the real reasons for our actions, which e.g. may be group
  pressure or addiction. Others often know better what motivates us and what we
  want than ourselves.

  We could still do decision theory on the basis of what the subject
  \emph{thinks} she desires and believes. This is especially true for normative
  decision theory. After all, isn't there something rationally defective about
  blindness to one's own motives? Ideal people would have perfect access to
  their beliefs and desires, and so they would choose in a way that matches
  their judgments. This may be true, but the question remains how people should
  behave who do not have perfect access to their attitudes. If judgments about
  desire and real desire come apart, should otherwise rational people be guided
  by the first or the second? We do not need to settle this question here,
  though it might be worth pointing out that the relevant judgments tend to be
  rather unstable, and dependent on how exactly they are elicited.

  Descriptively, it is clear that in the kind of situation where people are
  blind to their own motivations, it is not the conscious judgments that guide
  their behaviour. This is precisely why we say that their underlying motives
  are so-and-so: attributing these motives is what makes sense of their choices
  and their eventual (or counterfactual) reactions of despair or joy or regret.

} %

Why should we believe that all the factors that motivate an agent can be
amalgamated into a single numerical quantity? Would it not be better to allow
for a whole range of utility functions: moral utility, emotional utility, and so
on? We could certainly do that. But there are reasons to think that there must
also be an amalgamated, all-things-considered utility (although the determinacy
and numerical precision of utility functions is obviously an idealisation). When
you face a decision, you have to make a single choice. You can't choose one act
on moral grounds and a different act on emotional grounds. Somehow, all your
motives and reasons have to be weighed against each other to arrive at an
overall ranking of your options.

We will have a brief look at the weighing of different considerations in chapter
\ref{ch:separability}, but to a large extent this is really a topic for
empirical psychology and neuroscience. If it turns out that there are 23
distinct factors that influence our motivation in an intricate network of
inhibition and reinforcement, then so be it. We will model the whole network by
a single utility function, staying neutral on ``lower-level'' details that can
vary from agent to agent. But it's important to keep in mind that a lot of
interesting and complicated psychology is hiding in our seemingly simple concept
of utility.

Consider the following scenario.
%
\begin{example}(The endowment effect)
  Emily is buying a coffee mug. She is undecided between a red mug and a blue
  mug, and somewhat arbitrarily chooses the red one. A little later, someone
  offers Emily £1 if she swaps her red mug for the blue mug. Emily declines.
\end{example}

The kind of behaviour displayed by Emily is common. People tend to place a
greater value on things they own than on things they don't own. Initially, Emily
considered the two mugs equally desirable. Having bought the red mug, Emily
suddenly considers it better than the blue mug.

Psychologists have offered different explanations for this effect. Some say that
forgoing an owned item feels like a loss, and we don't like this feeling. Others
have argued that we treat goods that we own as part of our identity; forgoing
the good is thus perceived as a threat to our identity. We don't need to
adjudicate between these (and other) proposals. What's important for us is that
whichever explanation is correct, it should be reflected in Emily's utility
function. If Emily subconsciously regards her belongings as part of her
identity, and she is subconsciously motivated to preserve her identity, then her
utility for an outcome that involves giving up a previously owned good is
comparatively low.

Outside philosophy -- especially in economics -- utility is often assumed to be
a function of material goods (``commodity bundles''). On this usage, one can
speak of the utility (for Emily) of \emph{the red mug}, but one can't
distinguish between, for example, the utility of \emph{not getting the red mug}
and \emph{giving away the red mug}. No matter what utility we then assign to the
two mugs, Emily's behaviour is found to violate the MEU Principle. If the red
cup has greater utility than the blue cup, then Emily shouldn't have been
indifferent when she decided which cup to buy. If the two cups have equal
utility for Emily, then Emily should be happy to swap the red cup for the blue
cup.

On our usage of `utility', Emily's behaviour is perfectly compatible with the
MEU Principle. Emily doesn't just care about which material goods she
owns. She also cares about \emph{changes} to her possessions. If she is a real
person, she will also care about other things that have little to do with
material goods. If we want a general model of how beliefs and desires relate to
choices, we need to make room for all the desires an agent might have. We could
follow the economics tradition and restrict an agent's utility function to
material goods. But then we would have to add other elements to our model to
account for desires that don't pertain to the possession of material goods. We
will choose the theoretically simpler option of widening the definition of
`utility', so that an agent's utility function reflects everything the agent
cares about. We are going to return to this theme in chapter \ref{ch:risk}.

% People can, of course, use `utility' however they want. If `utility' is defined
% as a function of material goods, then Emily's behaviour really does violate the
% MEU Principle. We aren't going to use `utility' in this sense because we want
% to develop a general model of rational agency, without imposing strong and
% unrealistic demands on what the agent is allowed to care about.

\begin{exercise}{2}
  Amartya is offered a choice between a small slice of cake, a medium-sized
  slice, and a large slice. He chooses the medium-sized slice. If he had been
  offered a choice between only the small slice and the medium-sized slice, he
  would have chosen the small slice.
  \begin{exlist}
    \item Explain why this behaviour is incompatible with the MEU Principle if
    the utility function is a function of material goods.
    \item Explain why the behaviour is compatible with the MEU Principle on our
    use of `utility'.
  \end{exlist}
\end{exercise}

% When people say that Emily's behaviour violates the MEU Principle, they assume
% that `utility', as it figures in the MEU Principle, measures an agent's
% intrinsic desire towards material goods. On this usage, which is widespread
% among economists and psychologists, a desire for stability and familiarity is
% not reflected in the agent's utility function. Agents with such desires
% therefore tend to violate the MEU Principle.

% On our conception of utility, Emily does not violate the MEU Principle. Indeed,
% no pattern of behaviour whatsoever can, all by itself, violate the MEU
% Principle. For any pattern of behaviour, we can imagine that the agent has a
% basic desire to display just that pattern of behaviour. Displaying the behaviour
% then evidently maximizes expected utility.

% If we are interested in the MEU Principle as a descriptive hypothesis about real
% people's choices, and we interpret `utility' to measure whatever people care
% about, then the Principle is, in a sense, unfalsifiable. Whenever an agent seems
% to violate the MEU Principle, we can posit beliefs and desires that would make
% her choices conform to the principle. Psychologists and social scientists
% sometimes point at this fact in support of their decision to re-interpret
% `utility' as a function of material goods. A scientific hypothesis, they assume,
% is only worth taking seriously if it can be falsified. But a lot of respectable
% scientific theories are unfalsifiable \emph{in isolation}. Philosophers of
% science have long realized that one can generally only test scientific
% hypotheses in conjunction with a whole range of background assumptions.

% The same is true for the MEU Principle, understood as a descriptive hypothesis
% about human behaviour. \emph{Given} some assumptions about an agent's beliefs
% and desires, we can easily find that her choices do not conform to the MEU
% Principle. And often we have good evidence about the relevant beliefs and
% desires. For example, it is safe to assume that participants in the world
% chess tournament want to win their games, and that they are aware of the current
% position of the pieces in the game.

\cmnt{%
  Ultimately, the dialectic here resembles that from the previous section. If we
  want a simpler and more predictive model on which rational agents always
  promote their welfare, we can do that in one of two ways. One is to change the
  basic norm of practical rationality and say that rational choices don't
  promote the agent's desire, but their welfare. Or we can add an assumption on
  rational desire. The latter is the conceptually clearer move.%
} %

Officially, we will use `utility' to measure anything that motivates the
relevant agent. It is worth pointing out, however, that our model can be usefully
applied with other conceptions of utility. We might want to know, for example,
what an agent should do, \emph{from a moral perspective}, in a situation like
the Miners Problem from chapter 1, where crucial information about the world is
missing. A tempting idea is that the agent should maximize expected \emph{moral
  utility}, where the moral utility of an outcome is defined by some ethical
theory (utilitarianism, perhaps). Similarly, a corporation's board of directors
may want to know how to promote shareholder value in the light of such-and-such
common information. Here the relevant utility function might be derived from the
stipulated goal of promoting shareholder value, and the ``credence'' function
might be derived from the shared information. Neither of these needs to match
the beliefs and desires of any individual member of the board.

\begin{exercise}{3}
  Some choices predictably change our desires. One might argue that in such
  a case, a rational agent should be guided not by her present desires, but by
  the desires she will have as a result of her choice.

  Suppose you can decide right now how many drinks you will have tonight: zero,
  one, or two. (You have to order the drinks in advance and can't change the
  order at the time.) If you're sober, you prefer to have one drink rather than
  zero or two. But if you have a drink, you often prefer to have another. Draw a
  matrix for your decision problem, assuming that your goal is to maximize your
  expected future utility.
  % (Can you see why we don't need to change the MEU
  % Principle or our definition of utility?)
\end{exercise}

\cmnt{%
  People like the idea of a reflective desire or value to be what you
  desire to desire. But if desires come in degree, how do we measure
  degree of value?%
} %

\section{The structure of utility}\label{sec:structure-of-utility}

Now that we know what utility is, let's have a closer look at its formal
structure.

First of all, what are the bearers of utility? In ordinary language, we often
say that people desire \emph{things}: tea, cake, a concert ticket, a larger
flat. As we saw in the previous section, we need a more general conception to
capture an agent's desire \emph{not to lose a previously owned good}. We might
also desire that our friends are happy, that it won't rain tomorrow, that
so-and-so will win the next elections. Here the object of desire isn't a thing,
but a possible state of the world. Even when we say that people desire things,
plausibly the desire is really directed at a possible state of the world. When
you desire tea, you desire to \emph{drink the tea}. Your desire wouldn't be
satisfied if I gave you a certificate of ownership for a cup of tea that is
locked away in a safe.

So we'll assume that the objects of desire are the same kinds of things as the
objects of belief: propositions, or possible states of the world. As in the case
of belief, we don't distinguish between logically equivalent states of the
world. If you assign high utility to drinking tea then you also assign high
utility to \emph{drinking tea or coffee but not coffee}.

% We don't assume Booleanism for utility. Contradictions plausibly don't have a
% utility. Also, if A and B have a utility, their intersection may have
% probability zero, and it may not be a basic concern, in which case it may not
% have a well-defined utility. We could restrict utility to propositions with
% positive credence, but then we'd have to say that an agent's basic desires and
% concerns change when they receive information. Better not say that.

Let's study how an agent's desires towards logically related propositions are
related to one another. Suppose you assign high utility to the proposition that
it won't rain tomorrow (perhaps because you want to go on a picnic). Then you
should plausibly assign \emph{low} utility to the proposition that it
\emph{will} rain. You can't hope that it will rain and also that it won't rain.
In this respect, desire resembles belief: if you are confident that it will
rain, you can't also be confident that it won't rain. The Negation Rule of
probability captures the exact relationship between $\Cr(A)$ and $\Cr(\neg A)$,
stating that $\Cr(\neg A) = 1 - \Cr(A)$. Does the rule also hold for utility?
More generally, do utilities satisfy the Kolmogorov axioms? It will be
instructive to go through the three axioms.

Kolmogorov's axiom (i) states that probabilities range from 0 to 1. If there are
upper and lower bounds on utility, we could adopt axiom (i) for utilities as a
convention of measurement: we simply use 1 for the upper bound and 0 for the
lower bound. However, it is not obvious that there are such bounds. Couldn't
there be an infinite series $A_1, A_2, A_3, \ldots$ of states of increasing
utility in which the difference in utility between successive states is always
the same? If there is such a series, then utility can't be measured by numbers
between 0 and 1. Philosophers are divided over the question. Some think utility
must be \textbf{bounded}, others think it can be unbounded. There are arguments
for both sides. We will not pause to look at them.

Kolmogorov's axiom (ii) states that logically necessary propositions have
probability 1. If utilities satisfy the probability axioms, this would mean that
logically necessary propositions have maximal utility. However much you desire
that it won't rain tomorrow, your desire that \emph{it either will or won't
  rain} should be at least as great.

This does not look plausible. Intuitively, if something is certain to be the
case, it makes no sense to desire it. But this could mean two things. It could
mean that degrees of desire are not even defined for logically necessary
propositions. Or it could mean that an agent should always be indifferent
towards logically necessary propositions -- neither wanting them to be the case
nor wanting them to not be the case. Our common-sense conception of desire
arguably sides with the first option: if you are certain of something, even
asking how strongly you desire it seems odd. But the issue isn't clear. For our
purposes, it proves more convenient to go with the second option. We will say
that even logically necessary propositions have well-defined utility, and that
their utility measures the point between ``positive'' and ``negative'' desire.
If you positively want something to be the case, the utility you assign to it is
greater than the utility of a tautology. If you want something not to be the
case, its utility is lower than that of a tautology. Some authors make this more
concrete by adopting a convention that logically necessary propositions always
have utility 0.\label{utility0}

Axiom (iii) states that if $A$ and $B$ are logically incompatible, then the
probability of $A\lor B$ equals the sum of the probabilities of $A$ and $B$. To
illustrate, suppose there are three possible locations for a picnic: Alder Park,
Buckeye Park, and Cedar Park. Alder Park and Buckeye Park would be convenient
for you; Cedar Park would not. Now how much do you desire that the picnic takes
place in \emph{either Alder Park or Buckeye Park}? If axiom (iii) holds for
utilities, then if you desire Alder Park and Buckeye Park to equal degree $x$,
then your utility for the disjunction should be $2x$: you should be more pleased
to learn that the picnic takes place in either Alder Park or Buckeye Park than
to learn that it takes place in Alder Park. That's clearly wrong. Axiom (iii)
also fails.

What is the true connection between the utility of $A \lor B$ and the utilities
of $A$ and $B$? Intuitively, if $A$ and $B$ have equal utility $x$, then the
utility of $A \lor B$ should also be $x$. What if the utilities of $A$ and $B$
are not equal? What if, say, $\U(A) > \U(B)$? Then the utility of $A \lor B$
should plausibly lie in between the utilities of $A$ and $B$:
\[
  \U(A) \geq \U(A \lor B) \geq \U(B).
\]
That is, if Alder Park is your first preference and Buckeye your second, then
the disjunction \emph{either Alder Park or Buckeye Park} can't be
worse than Buckeye Park or better than Alder Park. But where does $\U(A
\lor B)$ lie in between $\U(A)$ and $\U(B)$? At the mid-point?

Suppose you prefer Alder Park to Buckeye Park, and Buckeye Park to
Cedar Park. You think it is highly unlikely that the picnic will take
place in Buckeye Park. Now how pleased would you be to learn the
picnic won't take place in Cedar Park -- equivalently, that it will
take place either in Alder Park or in Buckeye Park? You should be
quite pleased. If you're confident that $B$ is false, then
$\U(A \lor B)$ should plausibly be close to $\U(A)$. If you're confident
that $A$ is false, then $\U(A \lor B)$ should be near $\U(B)$.

Your utilities depend on your beliefs! On reflection, this should not come as a
surprise. A lot of the things we desire we only desire because we have certain
beliefs. If you want to buy a hammer to hang up a picture, then your desire for
the hammer is based (in part) on your belief that the hammer will allow you to
hang up the picture.

% There are also causal connections between desire and belief. Often a belief or
% judgement causes emotions like anger. That's not our topic.

Here is the general rule for $\U(A \lor B)$, assuming $A$ and $B$ are
incompatible. The rule was discovered by Richard Jeffrey in the 1960s and is our
only basic rule of utility, apart from the assumption that logically equivalent
propositions have the same utility.
%
\begin{genericthm}{Jeffrey's Axiom}
  If $A$ and $B$ are logically incompatible and  $\Cr(A\lor B) > 0$ then
  \vspace{-2mm}
  \[
    \U(A \lor B) =
    \U(A)\cdot \Cr(A\;/\;A\lor B) + \U(B)\cdot \Cr(B\;/\;A \lor B).
  \]
\end{genericthm}
%
\noindent%
In words: the utility of $A \lor B$ is the weighted average of the utility of
$A$ and the utility of $B$, weighted by the probability of the two disjuncts,
conditional on $A \lor B$.

Why `conditional on $A \lor B$'? Why don't we simply weigh the utility of $A$
and $B$ by their unconditional probability? Because then highly unlikely
propositions would automatically have a utility near 0. If you are almost
certain that the picnic will take place in Cedar Park, both
$\Cr(\emph{Alder Park})$ and $\Cr(\emph{Buckeye Park})$ will be close to 0. But
the mere fact that a proposition is unlikely does not make it undesirable. To
evaluate the desirability of a proposition, we should bracket its probability.
That's why Jeffrey's axiom defines $\U(A \lor B)$ as the probability-weighted
average of $\U(A)$ and $\U(B)$ \emph{on the supposition that $A \lor B$ is
  true}.

\begin{exercise}{2}
  You would like to win the lottery because that would allow you to travel the
  world, which you always wanted to do. Let \emph{Win} be the proposition that
  you win the lottery, and \emph{Travel} the proposition that you travel the
  world. Note that \emph{Win} is logically equivalent to
  $(\emph{Win} \land \emph{Travel}) \lor (\emph{Win} \land \neg \emph{Travel})$,
  and thus has the same utility. Suppose
  $\U(\emph{Win} \land \emph{Travel}) = 10$,
  $\U(\emph{Win} \land \neg \emph{Travel}) = 0$, and your credence that you will
  travel the world on the supposition that you will win the lottery is 0.9. By
  Jeffrey's axiom, what is $\U(\emph{Win})$?
\end{exercise}

\begin{exercise}{2}
  At the beginning of this section, I argued that if $\U(\neg A)$ is high, then
  $\U(A)$ should be low, and vice versa. Let's use the utility of the tautology
  $A \lor \neg A$ as a neutral point of reference, so that
  $\U(A \lor \neg A) = 0$. From this assumption, and Jeffrey's axiom, it follows
  that $\U(\neg A) > 0$ just in case $\U(A) < 0$. More precisely, it follows
  that
  \[
  \U(A)\cdot \Cr(A) = - \U(\neg A)\cdot \Cr(\neg A).
  \]
  Can you show how this follows?
  % (It's not as hard as it looks. Hint: $A$ is
  % logically incompatible with $\neg A$.)
\end{exercise}

The following consequence of Jeffrey's axiom is often useful. Assume that
$S_{1},\ldots,S_{n}$ are propositions for which it is guaranteed that exactly
one of them is true. That is, any two propositions in $S_{1},\ldots,S_{n}$ are
logically incompatible (no two of the propositions can be true together), and
the disjunction $S_{1}\lor\ldots\lor S_{n}$ is logically necessary (one of the
propositions must be true). Such a collection of propositions is called a
\textbf{partition}. Intuitively, a partition divides the space of possible
worlds into disjoint regions.

% $A$ and $\neg A$, for example, form a partition
% (for any $A$). So do $A \land B$, $A \land \neg B$, $\neg A \land B$, and
% $\neg A \land \neg B$ (for any $A$ and $B$).

Now, Jeffrey's axiom entails that if $S_{1},\ldots,S_{n}$ is a partition, then
for any proposition $A$ with $\Cr(A) > 0$,
\begin{equation*}
  \U(A) = \U(A \land S_1)\cdot \Cr(S_{1}/A) + \ldots + \U(A \land S_{n})\cdot\Cr(S_{n}/A).
\end{equation*}
Let's call this the \textbf{partition formulation} of Jeffrey's axiom.

Think of $A$ as a region in logical space. Each $A \land S_{i}$ is a disjoint
subregion of $A$. The partition formulation says that the desirability of the
whole region $A$ is a weighted average of the desirability of the subregions,
weighted by their probability conditional on $A$.

\begin{exercise}{3}[label=e:jeffrey-partition]
  Derive the partition formulation of Jeffrey's axiom from Jeffrey's (original) axiom.
\end{exercise}

\begin{exercise}{2}
  Derive Jeffrey's axiom from the partition formulation.
\end{exercise}


% \begin{exercise}{3}[label=e:jeffrey-gen]
%   Derive the following rule from Jeffrey's axiom and the rules of probability: 
%   if $A$, $B$, and $C$ are incompatible with one another and $\Cr(A\lor B \lor C) > 0$, then 
%   \begin{align*} \U(A \lor B \lor C) =
%   \U(A)\cdot \Cr(A\;/\;A\lor B \lor C) &+ \U(B)\cdot \Cr(B\;/\;A \lor B \lor C)\\
%   &+ \U(C)\cdot\Cr(C\;/\;A \lor B\lor C).
%   \end{align*}
% \end{exercise}

% Exercise \ref{e:jeffrey-gen} shows that Jeffrey's axiom holds not only for two,
% but also for three propositions. We can similarly extend it to four, five, six,
% or any other (finite) number of propositions. It follows that if a proposition
% $A$ divides into finitely many ``possible worlds'', then the utility of $A$ is
% the weighted average of the utility of these worlds, weighted by their
% probability conditional $A$.

\begin{exercise}{2}
  Give counterexamples to the following generalisations, assuming that an agent
  \emph{desires} a proposition $A$ iff $\U(A) > \U(\neg A)$. (Equivalently, iff
  $\U(A) > \U(A \lor \neg A)$.)

  \begin{exlist}
    \item Whenever an agent desires $A\land B$, they also desire $AB$.
    \item Whenever an agent desires $A$ and desires $B$, they also desire
    $A \land B$.
    \item Whenever an agent desires $A$, they desire $A \lor B$.
  \end{exlist}
  
  \cmnt{%
    Students were misled by pragmatics here: an offer of tea or coffee sounds
    good if you like tea and dislike coffee -- you'll just take tea.
    Pragmatically, being offered a disjunction suggests you will get to choose
    between the disjuncts. But we want to bracket this pragmatic effect. Best
    examples involve disjunctions that are not presented as disjunctions.%
  } %
\end{exercise}

% \begin{exercise}{2}
%   Hans is convinced that there is a ghost in his attic. He fears that the ghost
%   will keep him awake tonight. The following statement is intuitively true:
%   `Hans desires that the ghost in his attic will be quiet tonight'. Some have
%   found this puzzling because Hans would much prefer that there is no ghost in
%   his attic. If he had full control over what happens in his attic, he
%   would see to it that it is free of ghosts, not that it is occupied by a quiet
%   ghost. Explain why Hans nonetheless desires that there is a quiet ghost in his
%   attic, assuming the analysis of `desire' from the previous exercise.
% \end{exercise}

% There is also a puzzle about the reference of 'the ghost'. Maier 2017 represents
% this with linked DRS's as objects of the belief and desire, which has the
% consequence that one can't specify TCs for the desire. One could get the same
% result by taking the objects of belief and desire to be sets of world-assignment
% pairs. I think that's wrong, just as I think we shouldn't model the /content/ of
% imaginings as including the referential dependence on belief or other
% imaginings. There are also direct limits to this kind of approach. E.g. how
% would we represent what is expressed by: ``if there's a ghost I want it to be
% quiet''? This seems to display the same referential dependence as the Hans
% sentence, but it can't be analysed as directly reporting some simple and
% intuitive combination of DES and BEL.

\section{Basic desire}\label{sec:basic-desire}

I have presented Jeffrey's axiom as the sole formal requirement on rational
utility. Even this much is controversial. Many philosophers hold that
rationality imposes no constraints at all on an agent's desires. (In a way, this
is the opposite extreme of the hedonist doctrine that rational agents desire
nothing but their own pleasure.) The idea was memorably expressed by David Hume
in his \emph{Treatise of Human Nature}:

\begin{quote}
  'tis not contrary to reason to prefer the destruction of the whole
  world to the scratching of my finger. 'Tis not contrary to reason
  for me to chuse my total ruin, to prevent the least uneasiness of an
  Indian or person unknown to me.
\end{quote}

Hume held that our basic desires are not responsive to evidence, reason, or
argument. If your ultimate goal is to help some distant stranger, there is no
non-circular argument that could prove your goal to be wrong, nor could we fault
you for not taking into account any relevant evidence. Whatever facts you might
find out about the world, you could coherently retain your ultimate goal of
helping the stranger.

For Hume, beliefs and desires are in principle independent. What you believe is
one thing, what you desire is another. Beliefs try to answer the question: what
is the world like? Desires answer an entirely different question: what do you
want the world to be like? On the face of it, these two questions really appear
to be logically independent. Two agents could in principle give the same answer
to the first question and different answers to second, or the other way around.

What we have seen in the previous section seems to contradict these intuitions.
We have seen that an agent's utilities are thoroughly entangled with her
credences. Indeed, we can read off an agent's credence in any proposition $A$
from her utilities, assuming the utilities obey Jeffrey's axiom, the credences
obey the probability axioms, and the agent is not disinterested in $A$. Here
is how.

By Jeffrey's axiom,
\[
  \U(A \lor \neg A) = \U(A)\cdot \Cr(A) + \U(\neg A)\cdot \Cr(\neg A).
\]
By the Negation Rule, we can replace $\Cr(\neg A)$ by $1 - \Cr(A)$.
Multiplying out, we get
\[
  \U(A \lor \neg A) = \U(A)\cdot \Cr(A) + \U(\neg A) - \U(\neg A)\cdot \Cr(A).
\]
Now we solve for $\Cr(A)$:
\[
  \Cr(A) = \frac{\U(A \lor \neg A) - \U(\neg A)}{\U(A) - \U(\neg A)}.
\]
The ratio on the right-hand side is defined whenever $\U(A) \not= \U(\neg A)$, which I meant when I said that the agent is ``not disinterested'' in $A$.

What is going on here? Have we refuted Hume? Have we shown that an
agent's beliefs are \emph{part of her desires}? 

Of course not -- or not in any interesting sense. We need to distinguish
\textbf{basic desires} from \textbf{derived desires}. If you are looking for a
hammer to hang up a picture, your desire to find the hammer is not a basic
desire. It is derived from your desire to hang up the picture and your
belief that you need a hammer to achieve that goal. By contrast, a desire to be
free from pain is typically basic. If you want a headache to go away, this is
usually not (or not only) because you think having no headache is associated
with other things you desire. You simply don't want to have a headache, and
that's the end of the story.

When Hume claimed that desires are independent of beliefs, he was
talking about basic desires.

How are basic desires related to an agent's utility function?

Let's pretend that you have only one basic desire: to be free from pain. Let's
also pretend that this is an all-or-nothing matter. By your lights, all possible
worlds in which you are free from pain are then equally good, equally desirable.
In each of them, you have everything you want. The worlds in which you are
\emph{not} free from pain are also equally good. In each of them, you do not
have what you want.

Let's say that a proposition has \emph{uniform utility} for an agent if the
agent does not care how the proposition is realized: all subsets of the
proposition (understood as a set of possible worlds) have equal utility. In our
example, being pain-free and being in pain have uniform utility.

Let's change the scenario so that you have two basic desires: being free from
pain and being admired by other people. These are logically independent, so
there are four combinations: (1) being pain-free and admired, (2) being
pain-free and not admired, (3) being in pain and admired, and (4) being in pain
and not admired. Note that these form a partition.

Being in pain no longer has uniform utility. The worlds where you are in pain
divide into (better) worlds where you are in pain and admired and (worse) worlds
where you are in pain and not admired. As a consequence, the utility of being in
pain now depends on your beliefs: the stronger you believe that you are admired
if you are in pain, the more you desire being in pain.

The four combinations of being pain-free and admired, however, have uniform
utility. All worlds in which you are, say, in pain and admired are equally
desirable (still pretending these are all-or-nothing matters). I will say that
these combinations are your \textbf{concerns}. Intuitively, a concern is a
proposition that settles everything the agent ultimately cares about. An agent's
concerns always form a partition.

Remember that an outcome in a decision matrix must settle everything the agent
cares about. Every outcome in every decision problem is therefore a concern.
Many decision theorists use the word `outcome' where I use `concern'. I prefer a
different label, if only because some of an agent's concern may never figure as
outcomes in a decision situation.

It will be useful to have a name for an agent's utility function restricted to
their concerns. I'll call it the agent's \textbf{intrinsic utility function}.
(Some people say `value function'; many just say `utility function' and never
consider the wider function we call the agent's `utility function'.)

Formally, an intrinsic utility function assigns numbers to some partition of
propositions. Intuitively, each of these propositions settles everything the
agent cares about, and the numbers tell us how strongly the agent desires any
particular way of settling the things they care about. In the above example,
your intrinsic utility function might by fully given as follows:
\begin{gather*}
  \U(\emph{Pain} \land \emph{Admired}) = 1,\\
  \U(\neg\emph{Pain} \land \emph{Admired}) = 5,\\
  \U(\emph{Pain} \land \neg \emph{Admired}) = -5,\\
  \U(\neg \emph{Pain} \land \neg \emph{Admired}) = 0.
\end{gather*}
% This implies that all the agent ultimately cares about are the four possible
% combinations of being pain-free and being admired. It also tells us the strength
% and valence of the agent's attitudes towards the four combinations.

An agent's intrinsic utility function represents the belief-independent aspect
of their goals or desires. Every possible credence function is compatible with
every possible intrinsic utility function.

Since no concern is ever a disjunction of other concerns, Jeffrey's axiom
imposes no constraint on intrinsic utility functions. It only enters the picture
when we look at the utility of propositions that aren't concerns. In effect, the
axiom tells us how to derive an agent's utility for non-concerns from the
agent's intrinsic utility function and their credence function. (The axiom's
partition formulation makes the derivation transparent.)
 
In chapter \ref{ch:separability}, we will look at how an agent's intrinsic
utility function might be determined by less specific desires -- by a desire to
be free from pain, for example, and a desire to be admired. Before we do this,
we need to say more about what the utility numbers are supposed to represent.
What, exactly, does it mean that a proposition has utility 5, as opposed to -5
or 27?

% It is sometimes convenient to assume that an agent's intrinsic utility function
% directly assigns a utility to every possible world. After all, a world settles
% everything, so it is guaranteed to settle everything the agent ultimately cares
% about. It doesn't hurt that it also settles everything the agent does not care
% about.

\begin{exercise}{1}
  There's a party, and at first you want to be invited. Then you hear that Bob
  will be there, and you no longer want to be invited. Then you hear that
  there will be free beer, and you want to be invited again. Your desire seems
  to change back and forth. Nonetheless, your basic desires may have
  remained the same throughout. Explain how your fluctuating attitude
  might have come about without any change in basic desires.
\end{exercise}

\begin{exercise}{1}
  Suppose your only basic desire is to that a certain person in India is happy.
  What does your intrinsic utility function look like?
\end{exercise}

\begin{exercise}{3}
  Assume an agent's intrinsic utility function remains the same while they
  conditionalize on some proposition $E$.
  \begin{exlist}
    \item Can you define the new (total) utility function $\U_{\text{new}}$ in
    terms of the old utility function? (That is, can you complete the equation
    $\U_{\text{new}}(A) = \ldots$ in such a way that the dots make no reference
    to the agent's credences?)
    \item How does conditionalizing on an undesirable proposition (with
    $\U_{\text{old}}(E) < \U_{\text{old}}(\neg E)$) affect the utility of a
    logically necessary proposition $A \lor \neg A$?
  \end{exlist}
  % \medskip%
  % You may assume that there are only finitely many worlds.
\end{exercise}

\cmnt{%
  Suppose you desire A more than B because A is a stronger indicator of some
  other good, G. If this is the only reason why you prefer A, then A is better
  than B, but B\&G is better than A\&G, and $B\&\neg G$ is better than
  $A\&\neg G$. Isn't that a violation of STP?

  Example: Mary faces a lottery. She can either buy a ticket for \$1 or not. If
  she buys the ticket, there's a 50\% chance that she will win \$1000. If she
  doesn't buy it, the chance of winning (due to an administrative error) is
  0.0001. She figures out that it is worth buying the ticket.

  Sometime later, Mary has forgotten what she ended up doing (or perhaps she
  never knew it). At this point, she learns whether she won the lottery or not.
  In the first case, if she won, Mary will hope she didn't buy a ticket: given
  her new information, the desirability of not having bought a ticket is \$1000,
  the desirability of having bought one is \$999. In the second case, if she
  didn't win, she will also hope she didn't buy a ticket: the desirability of
  having bought one is \$-1, the desirability of not having bought one is \$0.
  Hence in the outset, Mary preferred buying a ticket over not buying one even
  though she could be certain that afterwards she will prefer to not have bought
  a ticket. } %

\cmnt{%

\section{Indicative value and subjunctive value}

To determine the utility of a proposition $A$ on the basis of an agent's basic
values, we first conditionalize the agent's credence function on $A$, and then
compute the average of the utility of the individual worlds in $A$, weighted by
the conditionalized credence. Why the first step? Why don't we simply use the
credence-weighted average of the utility of the individual worlds? Because then
highly unlikely propositions would automatically have a utility near 0.

When we want to assess the utility of a proposition $A$, the likelihood of
various sub-possibilities \emph{within} $A$ matters, but the likelihood of $A$
itself shouldn't matter. That's why we first suppose that $A$ is true and then
compute the credence-weighted average of the value of all the $A$-worlds.

Now remember from section \ref{sec:conditional} that there are two kinds of
supposition: indicative supposition and subjunctive supposition. Supposing
indicatively that Shakespeare \emph{didn't} write Hamlet, I am confident that
someone else wrote Hamlet. Supposing subjunctively that Shakespeare
\emph{hadn't} written Hamlet, I am confident that Hamlet would not have been
written at all. Indicative supposition is captured by the Ratio Formula for
conditional credence; subjunctive supposition is not. So far, we have used
indicative supposition to determine an agent's non-basic desires. But we could
also use subjunctive supposition. The result is a different way to represent an
agent's desires. Both are useful.

Let's first try to get a little clearer about subjunctive supposition. A
simplified example may help.
%
\begin{example}
  Last autumn, you visited a friend who had found a mushroom in the forest and
  was going to cook it for dinner. You could tell that the mushroom is either a
  delicious \emph{paddy straw} or a poisonous \emph{death cap}. You advised her
  not to eat it. Since you left before dinner, you don't know if she followed
  her advise, and you never asked about it. But you've seen your friend several
  times since then, so you're sure she is alive.
\end{example}

We may ask: \emph{What if your friend ate the mushroom?} If the mushroom was a
death cap, she would no longer be alive. But your friend is alive. So if she
ignored your advise and ate the mushroom, then the mushroom was almost certainly
not a death cap. Accordingly, $\Cr(\emph{Paddy Straw}\;/\;\emph{Ate})$ is close
to 0, and $\Cr(\emph{Survived}\;/\;\emph{Ate})$ is 1.

Alternatively, we may ask: \emph{What if your friend had eaten the mushroom?}
Here the answer depends on whether mushroom was a death cap or a paddy straw. If
it was a death cap, your friend would have died. If it was a paddy straw, she
would have survived. Let's say you think it is just as likely that the mushroom
was a paddy straw as that it was a death cap. Your credence in the proposition
that your friend would have survived, if she had eaten the mushroom, is then
\nicefrac{1}{2}.

The example suggests that uncertainty about what would have happened if some
proposition $A$ were the cases generally arises from uncertainty about ordinary
matters of fact -- about whether the mushroom was in fact a paddy straw or a
death cap. If we knew enough about the world, we could tell exactly what would
have happened if your friend had eaten the mushroom.

So let's assume that for every possible world $w$ and proposition $A$, the facts
at $w$ somehow settle what would have happened if $A$ were the case. That is,
for every proposition $B$, the facts at $w$ settle whether of not $B$ would be
the case if $A$ were the case. The conjunction of all the propositions $B$ that
would have been true if $A$ were the case is another possible world (a maximally
specific proposition). Let's denote this world by `$w^A$'. Informally, $w^A$ is
what $w$ would have been like if $A$ had been the case.

Given $A$ and $B$, we can divide the space of worlds into two: there are worlds
where $B$ would have been the case if $A$ had been the case, and there are
worlds where $\neg B$ would have been the case. A set of worlds is a
proposition. Let's write $A \boxright B$. And now we can define subjunctive
supposition:
\[
  \Cr(A/\!/B) = \Cr(A \boxright B).
\]

Notice that in contrast to the ratio formula, this account doesn't allow us to
find the probability of $A$ given $B$ in terms of the probability of $A$, $B$
and their Boolean combinations. The truth-value of $A \boxright B$ is not fixed
by the truth-value of $A$ and $B$. Whether or not you would have died if you had
eaten the mushroom depends on whether the mushroom is poisonous, an entirely
different proposition.

What, in general, has to be the case for $A \boxright B$ to be true at a world
$w$? If $A$ is a concrete event, then intuitively, we are considering worlds
that are just like $w$ up until the point of $A$, then minimally diverge to
allow for $A$, and then continue by the general laws of nature in $w$.

I will write $\Cr(A/B)$ for the \textbf{indicative conditional credence} in $A$
given $B$, and $\Cr(A/\!/B)$ (with two slashes) for the \textbf{subjunctive
  conditional credence} in $A$ given $B$. Thus News value vs instrumental value.

Prisoner Dilemma against twin. Cooperating is good news. But what would be the
case if you did cooperate?

--

Two things might be worth clarifying here. The first is that basic desires
plausibly pertain to properties like being rich or being in pain (but also being
such that one's offspring does well etc.). These features determine the overall
desirability of a state of affairs.

Second, however, you can't just add features to a possible world and leave the
rest the same. Some kind of minimal revision is required. Moreover, we generally
don't know the exact present state of the world. We don't know whether we'd lose
our friends in the minimally revised world where we're rich.

This very naturally leads to an imaging model. We get subjunctive desirability.

Indicative desirability is non-interventionist. Here we don't want something to
be added or changed in the world. Rather, we want the world to be one of the
ways it might be, for all we know.

--

\begin{genericthm}{The Imaging Axiom}
  If $A$ and $B$ are logically incompatible, then 
  \[ \U(A \lor B) = \U(A)\cdot \Cr(A/\!/(A\lor B)) + \U(B)\cdot \Cr(B/\!/(A \lor B)). \]
\end{genericthm}

But there is another one. Consider the proposition that Oswald didn't kill
Kennedy. Suppose you think that Oswald acted on his own, and that if he hadn't
killed Kennedy, chances are that Kennedy would have survived for many years. You
might say you wish that Oswald hadn't killed Kennedy. But conditional on Oswald
not having killed Kennedy, it is almost certain that someone else did. And you
don't wish that. Now we can say that what you really desire is that nobody
killed Kennedy, which is a more specific proposition than that Oswald didn't
kill him. But then we might also say that you don't really desire not breaking
your leg, but only not breaking your leg without benefitial side-effects. There
is another way to represent the desirability that gets the Oswald case right:
subjunctive desirability. Imaging.

Which of them is the right measure? We don't have to choose. Note that both of
them agree on the value of elementary possibilities. The question is only how to
extend those values to non-elementary propositions. It is not clear how we
should test which of them is right.

} %

\begin{essay}
  Do you agree with Hume that there are no rational constraints on
  basic desires? If so, try to defend this view. If not, try to argue
  against it.
\end{essay}

\begin{sources}

  Chapter 6 (``Game Theory and Rational Choice'') of Simon Blackburn,
  \emph{Ruling Passions} (1998) eloquently defends the idea that one shouldn't
  constrain what rational agents may care about in the theory of practical
  rationality. John Broome, ``\,`Utility'\,'' (1991) provides some more
  background and details on the two conceptions of utility.

  On possible explanations for the endowment effect, see Carey K.\ Morewedge and
  Colleen E.\ Giblin, ``Explanations of the endowment effect: an integrative
  review'' (2015). The cake slice example is from Amartya Sen, ``Internal
  Consistency of Choice'' (1993, p.501).

  The formal theory of utility in section \ref{sec:structure-of-utility} comes
  from chapter 5 of Richard Jeffrey, \emph{The Logic of Decision} (1965/1983).
  The assumption that the objects of utility are the same kinds of things
  (propositions) as the objects of credence is common in philosophy, but not in
  other disciplines.
  
  My distinction between intrinsic and non-intrinsic utility resembles a common
  distinction in economics between ``direct utility'' and ``indirect utility''.
  It also resembles the popular distinction between ``intrinsic'' and
  ``instrumental'' desire. But note that if $A$ and $B$ are concerns, then a
  desire for their disjunction $A \lor B$ is derived, although a disjunction is
  not intuitively instrumental to its disjuncts.

\end{sources}

  

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "bdrc.tex"
%%% End:
